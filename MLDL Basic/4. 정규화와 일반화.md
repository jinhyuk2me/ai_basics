## TL;DR
- 정규화(Regularization)는 모델이 훈련 데이터에 과적합(Overfitting)되는 것을 방지하고 테스트 데이터에 대한 일반화 성능을 높이는 기법이다.
- 가중치 정규화, 드롭아웃, 배치 정규화, 데이터 증강 등 다양한 정규화 기법이 존재하며, 각각 다른 메커니즘으로 일반화를 돕는다.
- 과적합과 과소적합의 균형을 맞추기 위해 모델 복잡도, 데이터 크기, 정규화 강도를 함께 조절해야 한다.

## 핵심 개념

### 과적합과 일반화

**과적합 (Overfitting)**: 모델이 훈련 데이터에는 매우 잘 맞지만 새로운 데이터에는 성능이 떨어지는 현상

**일반화 (Generalization)**: 모델이 학습하지 않은 새로운 데이터에 대해서도 좋은 성능을 내는 능력

**일반화 오차 (Generalization Error)**:

$$
\text{Generalization Error} = \text{Test Error} - \text{Train Error}
$$

일반화 오차가 크면 과적합, 작으면 일반화가 잘 되고 있음을 의미한다.

---

## 정규화 기법

### 가중치 정규화 (Weight Regularization)

손실 함수에 가중치의 크기를 제한하는 항을 추가한다.

**L2 정규화 (Ridge, Weight Decay)**

$$
L_{\text{total}} = L_{\text{data}} + \lambda \sum_{i} \theta_i^2
$$

- $L_{\text{data}}$: 데이터에 대한 손실 (Cross Entropy, MSE 등)
- $\lambda$: 정규화 강도 (하이퍼파라미터)
- $\theta_i$: 모델 파라미터

**효과**: 가중치를 0에 가깝게 유지하여 모델 복잡도를 낮춤

**L1 정규화 (Lasso)**

$$
L_{\text{total}} = L_{\text{data}} + \lambda \sum_{i} |\theta_i|
$$

**효과**: 일부 가중치를 정확히 0으로 만들어 희소성(Sparsity)을 유도. 특징 선택(Feature Selection) 효과

**Elastic Net**

L1과 L2를 결합:

$$
L_{\text{total}} = L_{\text{data}} + \lambda_1 \sum_{i} |\theta_i| + \lambda_2 \sum_{i} \theta_i^2
$$

---

### 드롭아웃 (Dropout)

훈련 중 무작위로 뉴런을 제거하여 네트워크가 특정 뉴런에 과도하게 의존하지 않도록 한다.

**수식**:

$$
\begin{aligned}
r_i &\sim \text{Bernoulli}(p) \\
\tilde{h}_i &= r_i \cdot h_i \\
y &= f(\tilde{h})
\end{aligned}
$$

- $p$: 뉴런을 유지할 확률 (보통 0.5)
- $r_i$: 드롭 마스크 (0 또는 1)
- $h_i$: 원래 뉴런 출력
- $\tilde{h}_i$: 드롭아웃 적용 후 출력

**추론 시**: 모든 뉴런을 사용하되 출력에 $p$를 곱함 (또는 훈련 시 $1/p$로 스케일링)

**효과**: 앙상블 효과를 얻어 일반화 성능 향상

**변형**:
- **DropConnect**: 가중치를 무작위로 드롭
- **Spatial Dropout**: CNN에서 전체 채널을 드롭

---

### 배치 정규화 (Batch Normalization)

미니배치 단위로 활성화 값을 정규화하여 학습을 안정화한다.

**수식**:

$$
\begin{aligned}
\mu_B &= \frac{1}{m} \sum_{i=1}^{m} x_i \\
\sigma_B^2 &= \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2 \\
\hat{x}_i &= \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \\
y_i &= \gamma \hat{x}_i + \beta
\end{aligned}
$$

- $m$: 배치 크기
- $\mu_B$: 배치 평균
- $\sigma_B^2$: 배치 분산
- $\epsilon$: 수치 안정성을 위한 작은 값 (보통 $10^{-5}$)
- $\gamma, \beta$: 학습 가능한 스케일/시프트 파라미터

**효과**:
- 내부 공변량 변화(Internal Covariate Shift) 감소
- 더 큰 학습률 사용 가능
- 정규화 효과 (Dropout 없이도 일반화)

**변형**:
- **Layer Normalization**: 배치 대신 층 단위로 정규화 (RNN/Transformer에서 사용)
- **Instance Normalization**: 각 샘플마다 정규화 (스타일 전이에서 사용)
- **Group Normalization**: 채널을 그룹으로 나누어 정규화

---

### 조기 종료 (Early Stopping)

검증 손실이 증가하기 시작하면 학습을 중단한다.

**알고리즘**:
1. 매 epoch마다 검증 손실 계산
2. 검증 손실이 최소일 때 모델 저장
3. $n$ epoch 동안 개선이 없으면 학습 중단 (patience)

**효과**: 과적합이 시작되기 전에 학습을 멈춰 일반화 성능 유지

---

### 데이터 증강 (Data Augmentation)

훈련 데이터를 인위적으로 변형하여 데이터 다양성을 높인다.

**이미지 (Computer Vision)**:
- 회전, 뒤집기, 크롭, 색상 변환
- Mixup, CutMix, AutoAugment

**텍스트 (NLP)**:
- 역번역 (Back Translation)
- 동의어 치환
- 무작위 삽입/삭제

**시계열**:
- 노이즈 추가
- 시간 왜곡 (Time Warping)
- 윈도우 슬라이싱

**효과**: 실질적으로 훈련 데이터 크기를 늘려 일반화 성능 향상

---

### 라벨 스무딩 (Label Smoothing)

정답 레이블을 완전한 1이 아닌 약간 부드럽게 만든다.

**수식**:

$$
y_{\text{smooth}} = (1 - \alpha) y + \frac{\alpha}{K}
$$

- $y$: 원래 원-핫 인코딩 레이블
- $\alpha$: 스무딩 강도 (보통 0.1)
- $K$: 클래스 개수

**예시**: 3-클래스 분류에서 정답이 클래스 0인 경우
- 원래: $[1, 0, 0]$
- 스무딩 후 ($\alpha=0.1$): $[0.9, 0.05, 0.05]$

**효과**: 모델이 너무 확신하지 않도록 하여 일반화 향상

---

### 앙상블 (Ensemble)

여러 모델의 예측을 결합한다.

**방법**:
- **Bagging**: Bootstrap 샘플로 여러 모델 학습 (Random Forest)
- **Boosting**: 순차적으로 약한 모델을 학습 (AdaBoost, XGBoost)
- **Stacking**: 여러 모델의 출력을 메타 모델이 학습
- **Snapshot Ensemble**: 하나의 학습 과정에서 여러 체크포인트 저장

**효과**: 개별 모델의 편향을 줄이고 분산을 낮춰 일반화 성능 향상

---

## 일반화 전략

### 모델 복잡도 조절

**파라미터 수 감소**:
- 은닉층 개수 줄이기
- 은닉 유닛 개수 줄이기
- 저랭크 근사 (Low-rank Approximation)

**파라미터 공유**:
- CNN의 합성곱 필터
- RNN의 시간 단계별 가중치 공유

---

### 정규화 강도 선택

**일반 가이드라인**:
- 작은 데이터셋: 강한 정규화 ($\lambda$ 크게)
- 큰 데이터셋: 약한 정규화 ($\lambda$ 작게)
- 복잡한 모델: 강한 정규화
- 단순한 모델: 약한 정규화

**튜닝 방법**:
- Grid Search: $\lambda \in \{0.001, 0.01, 0.1, 1.0\}$
- Random Search
- 베이지안 최적화

---

## 실습 예제

### L2 정규화 적용

```python
import torch
import torch.nn as nn
import torch.optim as optim

model = nn.Sequential(
    nn.Linear(100, 50),
    nn.ReLU(),
    nn.Linear(50, 10)
)

# L2 정규화는 weight_decay로 지정
optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)

# 학습 루프
for epoch in range(100):
    for x, y in train_loader:
        optimizer.zero_grad()
        logits = model(x)
        loss = nn.CrossEntropyLoss()(logits, y)
        loss.backward()
        optimizer.step()
```

---

### 드롭아웃 적용

```python
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.ReLU(),
    nn.Dropout(p=0.5),  # 50% 드롭아웃
    nn.Linear(50, 10)
)

# 훈련 모드
model.train()
output = model(x)  # 드롭아웃 적용됨

# 평가 모드
model.eval()
output = model(x)  # 드롭아웃 적용 안 됨
```

---

### 배치 정규화 적용

```python
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.BatchNorm1d(50),  # 배치 정규화
    nn.ReLU(),
    nn.Linear(50, 10)
)
```

---

### 조기 종료 구현

```python
best_val_loss = float('inf')
patience = 10
counter = 0

for epoch in range(1000):
    # 훈련
    train_loss = train_one_epoch(model, train_loader)

    # 검증
    val_loss = validate(model, val_loader)

    # 조기 종료 체크
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pt')
        counter = 0
    else:
        counter += 1

    if counter >= patience:
        print(f"Early stopping at epoch {epoch}")
        break

# 최적 모델 로드
model.load_state_dict(torch.load('best_model.pt'))
```

---

## 점검 포인트

### 과적합 진단

**훈련/검증 곡선 분석**:
- 훈련 손실은 감소하지만 검증 손실이 증가 → 과적합
- 훈련과 검증 손실 모두 높음 → 과소적합
- 훈련과 검증 손실이 비슷하게 감소 → 적절한 학습

**체크리스트**:
- [ ] 훈련/검증 정확도 차이가 5% 이상인가?
- [ ] 검증 손실이 증가 추세인가?
- [ ] 모델이 훈련 데이터를 암기하고 있는가?

---

### 정규화 기법 선택 가이드

| 상황 | 추천 기법 |
|------|----------|
| 작은 데이터셋 | L2 정규화 + Dropout + 데이터 증강 |
| 큰 데이터셋 | Batch Normalization + 약한 L2 |
| CNN | Batch Normalization + 데이터 증강 |
| RNN/Transformer | Layer Normalization + Dropout |
| 희소 특징 | L1 정규화 |
| 클래스 불균형 | 라벨 스무딩 + Focal Loss |

---

### 실험 기록 필수 메타데이터

```python
experiment_config = {
    'model': 'ResNet18',
    'regularization': {
        'l2': 1e-4,
        'dropout': 0.5,
        'batch_norm': True,
        'label_smoothing': 0.1
    },
    'initialization': 'kaiming_normal',
    'seed': 42,
    'data_augmentation': ['horizontal_flip', 'random_crop'],
    'early_stopping_patience': 10
}
```

---

## 연습 문제

### 기초
1. L1 정규화와 L2 정규화의 차이를 설명하고, 각각 언제 사용하는지 서술하시오.
2. 드롭아웃의 $p=0.5$는 무엇을 의미하며, 왜 추론 시에는 드롭아웃을 적용하지 않는지 설명하시오.
3. 배치 정규화에서 $\gamma$와 $\beta$ 파라미터는 왜 필요한가?

### 중급
4. MNIST 데이터셋에서 MLP 모델에 L2 정규화, 드롭아웃, 배치 정규화를 각각 적용하고 검증 정확도를 비교하시오.
5. 이미지 분류 모델에서 데이터 증강 전후의 학습 곡선을 시각화하고 일반화 성능 변화를 분석하시오.
6. 조기 종료의 patience 값을 5, 10, 20으로 바꾸어가며 최종 모델 성능과 학습 시간을 비교하시오.

### 고급
7. Weight Decay와 L2 정규화의 차이를 수식으로 설명하고, AdamW가 왜 필요한지 서술하시오.
8. Mixup과 CutMix 데이터 증강을 구현하고 CIFAR-10에서 성능을 비교하시오.
9. Batch Normalization과 Layer Normalization의 차이를 수식과 코드로 설명하고, 각각 어떤 상황에서 유리한지 논하시오.

---

## 참고 자료

**서적**
- Goodfellow et al., *Deep Learning*, Chapter 7: Regularization
- Bishop, *Pattern Recognition and Machine Learning*, Chapter 5.5

**논문**
- Srivastava et al., "Dropout: A Simple Way to Prevent Neural Networks from Overfitting" (2014)
- Ioffe & Szegedy, "Batch Normalization: Accelerating Deep Network Training" (2015)
- Zhang et al., "mixup: Beyond Empirical Risk Minimization" (2018)

**온라인 강의**
- Stanford CS231n: Lecture 7 - Training Neural Networks II
- Coursera Deep Learning Specialization: Regularization

---

## 다음 학습
- [[Foundations/5. 신경망 기본 구조]]
- [[Foundations/3. 최적화]]
- [[Foundations/7. 역전파와 최적화 전략]]
