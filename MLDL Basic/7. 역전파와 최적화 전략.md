## TL;DR
- 역전파(Backpropagation)는 연쇄 법칙(Chain Rule)을 사용하여 손실 함수의 그래디언트를 효율적으로 계산하는 알고리즘이다.
- 그래디언트를 계산한 후 옵티마이저가 파라미터를 업데이트하여 손실을 최소화한다.
- 기울기 소실/폭발, 학습 불안정성 등의 문제를 진단하고 해결하는 전략이 필요하다.

## 핵심 개념

### 역전파 알고리즘

역전파는 출력층에서 시작하여 입력층 방향으로 그래디언트를 계산한다.

**연쇄 법칙 (Chain Rule)**:

$$
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w}
$$

- $L$: 손실 함수
- $y$: 출력
- $z$: 사전 활성화 (pre-activation)
- $w$: 가중치

---

### 2-층 신경망의 역전파

$$
\begin{aligned}
\mathbf{z}^{[1]} &= \mathbf{W}^{[1]} \mathbf{x} + \mathbf{b}^{[1]} \\
\mathbf{a}^{[1]} &= \sigma(\mathbf{z}^{[1]}) \\
\mathbf{z}^{[2]} &= \mathbf{W}^{[2]} \mathbf{a}^{[1]} + \mathbf{b}^{[2]} \\
\mathbf{y} &= \mathbf{a}^{[2]} = \sigma(\mathbf{z}^{[2]}) \\
L &= \text{Loss}(\mathbf{y}, \mathbf{t})
\end{aligned}
$$

**역전파 단계**:

1. 출력층 그래디언트:
$$
\delta^{[2]} = \frac{\partial L}{\partial \mathbf{z}^{[2]}} = \frac{\partial L}{\partial \mathbf{y}} \odot \sigma'(\mathbf{z}^{[2]})
$$

2. 출력층 파라미터 그래디언트:
$$
\begin{aligned}
\frac{\partial L}{\partial \mathbf{W}^{[2]}} &= \delta^{[2]} (\mathbf{a}^{[1]})^\top \\
\frac{\partial L}{\partial \mathbf{b}^{[2]}} &= \delta^{[2]}
\end{aligned}
$$

3. 은닉층 그래디언트:
$$
\delta^{[1]} = \frac{\partial L}{\partial \mathbf{z}^{[1]}} = (\mathbf{W}^{[2]})^\top \delta^{[2]} \odot \sigma'(\mathbf{z}^{[1]})
$$

4. 은닉층 파라미터 그래디언트:
$$
\begin{aligned}
\frac{\partial L}{\partial \mathbf{W}^{[1]}} &= \delta^{[1]} \mathbf{x}^\top \\
\frac{\partial L}{\partial \mathbf{b}^{[1]}} &= \delta^{[1]}
\end{aligned}
$$

- $\odot$: 원소별 곱 (element-wise product)
- $\delta^{[l]}$: 층 $l$의 오차 항 (error term)

---

## 역전파 알고리즘 (일반화)

**Forward Pass**: 입력부터 출력까지

$$
\begin{aligned}
\mathbf{a}^{[0]} &= \mathbf{x} \\
\mathbf{z}^{[l]} &= \mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]} \\
\mathbf{a}^{[l]} &= \sigma^{[l]}(\mathbf{z}^{[l]})
\end{aligned}
$$

**Backward Pass**: 출력부터 입력까지

$$
\begin{aligned}
\delta^{[L]} &= \frac{\partial L}{\partial \mathbf{a}^{[L]}} \odot \sigma'^{[L]}(\mathbf{z}^{[L]}) \\
\delta^{[l]} &= (\mathbf{W}^{[l+1]})^\top \delta^{[l+1]} \odot \sigma'^{[l]}(\mathbf{z}^{[l]}) \\
\frac{\partial L}{\partial \mathbf{W}^{[l]}} &= \delta^{[l]} (\mathbf{a}^{[l-1]})^\top \\
\frac{\partial L}{\partial \mathbf{b}^{[l]}} &= \delta^{[l]}
\end{aligned}
$$

---

## 자동 미분 (Automatic Differentiation)

PyTorch와 같은 프레임워크는 자동으로 그래디언트를 계산한다.

**계산 그래프 (Computational Graph)**:
- 순전파 시 모든 연산을 그래프로 기록
- 역전파 시 그래프를 역순으로 순회하며 그래디언트 계산

```python
import torch

# 자동 미분 활성화
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
w = torch.tensor([0.5, -0.3, 0.2], requires_grad=True)

# 순전파
y = (w * x).sum()  # 스칼라 출력
print(f"y = {y.item()}")  # y = 0.4

# 역전파
y.backward()

# 그래디언트 확인
print(f"dy/dw = {w.grad}")  # tensor([1., 2., 3.])
print(f"dy/dx = {x.grad}")  # tensor([0.5, -0.3, 0.2])
```

---

## 기울기 소실과 폭발

### 기울기 소실 (Gradient Vanishing)

깊은 네트워크에서 그래디언트가 0으로 수렴하는 문제

**원인**: 역전파 시 작은 값의 반복적인 곱셈

$$
\frac{\partial L}{\partial \mathbf{W}^{[1]}} = \delta^{[L]} \prod_{l=2}^{L} \mathbf{W}^{[l]} \odot \prod_{l=1}^{L-1} \sigma'(\mathbf{z}^{[l]})
$$

Sigmoid/Tanh의 경우 $\sigma'(x) < 1$이므로, 층이 깊어질수록 기울기가 지수적으로 감소한다.

**해결책**:
- ReLU 계열 활성화 함수 사용
- Batch Normalization
- Residual Connection (Skip Connection)
- LSTM/GRU (RNN의 경우)

---

### 기울기 폭발 (Gradient Explosion)

그래디언트가 무한대로 발산하는 문제

**원인**: 가중치가 크거나 활성화 함수의 기울기가 클 때

**해결책**:
- Gradient Clipping
- 적절한 가중치 초기화
- 작은 학습률
- Batch Normalization

```python
# Gradient Clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

## 그래디언트 체크 (Gradient Checking)

역전파 구현이 올바른지 확인하는 방법

**수치적 그래디언트 (Numerical Gradient)**:

$$
\frac{\partial L}{\partial w} \approx \frac{L(w + \epsilon) - L(w - \epsilon)}{2\epsilon}
$$

$\epsilon$는 작은 값 (예: $10^{-7}$)

```python
def numerical_gradient(f, x, epsilon=1e-7):
    """수치적 그래디언트 계산"""
    grad = torch.zeros_like(x)

    for i in range(x.numel()):
        # f(x + epsilon)
        x_plus = x.clone()
        x_plus.view(-1)[i] += epsilon
        f_plus = f(x_plus)

        # f(x - epsilon)
        x_minus = x.clone()
        x_minus.view(-1)[i] -= epsilon
        f_minus = f(x_minus)

        # 그래디언트 근사
        grad.view(-1)[i] = (f_plus - f_minus) / (2 * epsilon)

    return grad

# 역전파와 수치적 그래디언트 비교
analytical_grad = x.grad
numerical_grad = numerical_gradient(lambda x: loss_fn(model(x), y), x)

diff = (analytical_grad - numerical_grad).abs().max()
print(f"Gradient difference: {diff.item()}")  # 매우 작아야 함 (< 1e-5)
```

---

## 학습 안정성 전략

### 배치 크기 선택

**작은 배치 (32-128)**:
- 장점: 정규화 효과, 메모리 효율적
- 단점: 노이즈가 많아 학습 불안정

**큰 배치 (256-1024)**:
- 장점: 안정적인 그래디언트, 빠른 학습
- 단점: 일반화 성능 저하 가능

---

### 학습률 선택

**학습률이 너무 클 때**:
- 손실이 발산 (NaN)
- 학습이 불안정

**학습률이 너무 작을 때**:
- 학습이 매우 느림
- 지역 최솟값에 갇힘

**Learning Rate Finder**:
```python
from torch.optim.lr_scheduler import ExponentialLR

lrs = []
losses = []

for lr in torch.logspace(-5, 0, 100):
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)

    # 한 배치 학습
    loss = train_one_batch(model, data, optimizer)

    lrs.append(lr)
    losses.append(loss)

# 손실이 감소하기 시작하는 학습률 선택
plt.plot(lrs, losses)
plt.xscale('log')
```

---

### 그래디언트 모니터링

```python
def monitor_gradients(model):
    """그래디언트 통계 출력"""
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            print(f"{name}: grad_norm = {grad_norm:.4f}")

            # 경고: 기울기 소실
            if grad_norm < 1e-6:
                print(f"  WARNING: Vanishing gradient!")

            # 경고: 기울기 폭발
            if grad_norm > 10.0:
                print(f"  WARNING: Exploding gradient!")
```

---

## 최적화 전략 비교

### SGD vs Adam

**SGD**:
- 단순하고 안정적
- 모멘텀 추가 시 성능 향상
- 적절한 학습률 튜닝 필요

**Adam**:
- 학습률 자동 조절
- 빠른 수렴
- 일반화 성능이 SGD보다 약간 떨어질 수 있음

```python
# SGD with Momentum
optimizer_sgd = torch.optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9,
    weight_decay=1e-4
)

# Adam
optimizer_adam = torch.optim.Adam(
    model.parameters(),
    lr=1e-3,
    betas=(0.9, 0.999),
    weight_decay=1e-4
)
```

---

## 실습 예제

### 수동 역전파 구현

```python
import torch

def manual_backward():
    """2-층 신경망 역전파 수동 구현"""
    # 데이터
    x = torch.randn(4, 10)  # 배치 4, 입력 10
    y = torch.randint(0, 3, (4,))  # 3-클래스 분류

    # 파라미터
    W1 = torch.randn(20, 10, requires_grad=False) * 0.01
    b1 = torch.zeros(20, requires_grad=False)
    W2 = torch.randn(3, 20, requires_grad=False) * 0.01
    b2 = torch.zeros(3, requires_grad=False)

    # Forward pass
    z1 = x @ W1.t() + b1  # (4, 20)
    a1 = torch.relu(z1)   # (4, 20)
    z2 = a1 @ W2.t() + b2 # (4, 3)

    # Softmax + Cross Entropy
    exp_z2 = torch.exp(z2 - z2.max(dim=1, keepdim=True)[0])
    probs = exp_z2 / exp_z2.sum(dim=1, keepdim=True)
    loss = -torch.log(probs[range(4), y]).mean()

    print(f"Loss: {loss.item():.4f}")

    # Backward pass
    # 출력층 그래디언트
    dz2 = probs.clone()
    dz2[range(4), y] -= 1  # (4, 3)
    dz2 /= 4  # 평균

    dW2 = dz2.t() @ a1  # (3, 20)
    db2 = dz2.sum(dim=0)  # (3,)

    # 은닉층 그래디언트
    da1 = dz2 @ W2  # (4, 20)
    dz1 = da1 * (z1 > 0).float()  # ReLU 미분

    dW1 = dz1.t() @ x  # (20, 10)
    db1 = dz1.sum(dim=0)  # (20,)

    return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}

grads = manual_backward()
```

---

### 학습 루프 예제

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 모델, 데이터, 옵티마이저 정의
model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# 학습 루프
for epoch in range(10):
    for batch_idx, (data, target) in enumerate(train_loader):
        # 1. 순전파
        output = model(data)
        loss = criterion(output, target)

        # 2. 그래디언트 초기화
        optimizer.zero_grad()

        # 3. 역전파
        loss.backward()

        # 4. 그래디언트 클리핑 (선택)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # 5. 파라미터 업데이트
        optimizer.step()

        # 6. 모니터링
        if batch_idx % 100 == 0:
            print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}")
```

---

## 디버깅 체크리스트

### 학습이 안 될 때

- [ ] 그래디언트가 0인가? → 기울기 소실
- [ ] 그래디언트가 너무 큰가? → 기울기 폭발
- [ ] 손실이 NaN인가? → 학습률이 너무 크거나 수치 불안정
- [ ] 손실이 감소하지 않는가? → 학습률이 너무 작음
- [ ] 훈련 정확도가 랜덤보다 낮은가? → 버그 (레이블, 전처리)

### 진단 도구

```python
def diagnose_training(model, data_loader, optimizer):
    """학습 문제 진단"""
    model.train()

    # 한 배치 학습
    data, target = next(iter(data_loader))
    output = model(data)
    loss = nn.CrossEntropyLoss()(output, target)

    optimizer.zero_grad()
    loss.backward()

    # 1. 손실 값 체크
    if torch.isnan(loss) or torch.isinf(loss):
        print("ERROR: Loss is NaN or Inf!")
        return

    # 2. 그래디언트 체크
    grad_norms = []
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            grad_norms.append(grad_norm)

            if grad_norm < 1e-7:
                print(f"WARNING: {name} has vanishing gradient")
            elif grad_norm > 100:
                print(f"WARNING: {name} has exploding gradient")

    print(f"Mean gradient norm: {sum(grad_norms)/len(grad_norms):.6f}")
    print(f"Max gradient norm: {max(grad_norms):.6f}")
    print(f"Min gradient norm: {min(grad_norms):.6f}")
```

---

## 연습 문제

### 기초
1. 2-층 신경망에서 역전파 과정을 단계별로 수식으로 유도하시오.
2. 기울기 소실이 발생하는 이유를 Sigmoid 함수의 미분값으로 설명하시오.
3. Gradient Clipping이 기울기 폭발을 어떻게 방지하는지 설명하시오.

### 중급
4. PyTorch를 사용하지 않고 NumPy로 역전파를 구현하시오.
5. 학습 중 그래디언트 노름을 기록하고 시각화하여 안정성을 분석하시오.
6. 수치적 그래디언트와 역전파 그래디언트를 비교하여 구현의 정확성을 검증하시오.

### 고급
7. 깊은 네트워크(10층 이상)에서 기울기 소실 문제를 재현하고, Residual Connection으로 해결하시오.
8. 다양한 학습률로 실험하여 최적의 학습률을 찾는 Learning Rate Finder를 구현하시오.
9. 커스텀 역전파 함수를 작성하고 `torch.autograd.Function`으로 구현하시오.

---

## 참고 자료

**논문**
- Rumelhart et al., "Learning representations by back-propagating errors" (1986) - 역전파
- Pascanu et al., "On the difficulty of training RNNs" (2013) - 기울기 문제

**서적**
- Goodfellow et al., *Deep Learning*, Chapter 6.5: Back-Propagation
- Nielsen, *Neural Networks and Deep Learning*, Chapter 2

**온라인 강의**
- Stanford CS231n: Lecture 4 - Backpropagation
- 3Blue1Brown: Backpropagation calculus

---

## 다음 학습
- [[Foundations/3. 최적화]]
- [[Foundations/6. 활성화 함수와 출력층 설계]]
- [[Foundations/4. 정규화와 일반화]]
- [[Math/Analysis/1. 라플라스 변환과 미분방정식]]
- [[Math/Analysis/3. 측도론과 확률론 기초]]
