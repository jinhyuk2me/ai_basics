## TL;DR
- 활성화 함수는 신경망에 비선형성을 부여하여 복잡한 함수를 학습할 수 있게 한다.
- ReLU는 가장 널리 사용되는 활성화 함수로, 기울기 소실 문제를 완화하고 계산이 빠르다.
- 출력층의 활성화 함수는 문제 유형에 따라 선택한다: Softmax (다중 분류), Sigmoid (이진 분류), Linear (회귀).

## 핵심 개념

### 활성화 함수의 필요성

활성화 함수가 없으면 다층 신경망도 단순한 선형 변환의 조합이 되어 표현력이 제한된다.

**증명**:
$$
\begin{aligned}
\mathbf{h} &= \mathbf{W}_1 \mathbf{x} \\
\mathbf{y} &= \mathbf{W}_2 \mathbf{h} = \mathbf{W}_2 (\mathbf{W}_1 \mathbf{x}) = (\mathbf{W}_2 \mathbf{W}_1) \mathbf{x} = \mathbf{W} \mathbf{x}
\end{aligned}
$$

비선형 활성화 함수가 있어야 복잡한 패턴을 학습할 수 있다.

---

## 주요 활성화 함수

### Sigmoid

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

**도함수**:
$$
\sigma'(x) = \sigma(x)(1 - \sigma(x))
$$

**특징**:
- 출력 범위: $(0, 1)$
- 확률로 해석 가능
- **단점**: 기울기 소실 (양 극단에서 기울기가 0에 가까움)

**사용처**: 이진 분류 출력층

```python
import torch
import torch.nn as nn

sigmoid = nn.Sigmoid()
x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
y = sigmoid(x)
# tensor([0.1192, 0.2689, 0.5000, 0.7311, 0.8808])
```

---

### Tanh (Hyperbolic Tangent)

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{e^{2x} - 1}{e^{2x} + 1}
$$

**도함수**:
$$
\tanh'(x) = 1 - \tanh^2(x)
$$

**특징**:
- 출력 범위: $(-1, 1)$
- 0을 중심으로 대칭 (zero-centered)
- Sigmoid보다 기울기 소실이 덜하지만 여전히 존재

**사용처**: RNN 은닉층 (LSTM의 일부)

```python
tanh = nn.Tanh()
y = tanh(x)
# tensor([-0.9640, -0.7616, 0.0000, 0.7616, 0.9640])
```

---

### ReLU (Rectified Linear Unit)

$$
\text{ReLU}(x) = \max(0, x) = \begin{cases}
x & \text{if } x > 0 \\
0 & \text{otherwise}
\end{cases}
$$

**도함수**:
$$
\text{ReLU}'(x) = \begin{cases}
1 & \text{if } x > 0 \\
0 & \text{otherwise}
\end{cases}
$$

**특징**:
- 계산이 매우 빠름
- 기울기 소실 완화 (양수 영역에서 기울기가 1)
- **단점**: Dying ReLU (음수 입력에서 뉴런이 죽을 수 있음)

**사용처**: 딥러닝의 기본 선택

```python
relu = nn.ReLU()
y = relu(x)
# tensor([0.0000, 0.0000, 0.0000, 1.0000, 2.0000])
```

---

### Leaky ReLU

$$
\text{LeakyReLU}(x) = \max(\alpha x, x) = \begin{cases}
x & \text{if } x > 0 \\
\alpha x & \text{otherwise}
\end{cases}
$$

**특징**:
- $\alpha$는 작은 양수 (보통 0.01)
- Dying ReLU 문제 완화
- 음수 영역에서도 작은 기울기 유지

```python
leaky_relu = nn.LeakyReLU(negative_slope=0.01)
y = leaky_relu(x)
# tensor([-0.0200, -0.0100, 0.0000, 1.0000, 2.0000])
```

---

### PReLU (Parametric ReLU)

$$
\text{PReLU}(x) = \max(\alpha x, x)
$$

**특징**:
- $\alpha$를 학습 가능한 파라미터로 설정
- 데이터에 맞게 자동으로 조절

```python
prelu = nn.PReLU()
y = prelu(x)
```

---

### ELU (Exponential Linear Unit)

$$
\text{ELU}(x) = \begin{cases}
x & \text{if } x > 0 \\
\alpha(e^x - 1) & \text{otherwise}
\end{cases}
$$

**특징**:
- 음수 영역에서 부드러운 곡선
- 평균이 0에 가까움 (zero-centered에 가까움)
- ReLU보다 노이즈에 강함

```python
elu = nn.ELU(alpha=1.0)
y = elu(x)
# tensor([-0.8647, -0.6321, 0.0000, 1.0000, 2.0000])
```

---

### GELU (Gaussian Error Linear Unit)

$$
\text{GELU}(x) = x \cdot \Phi(x)
$$

- $\Phi(x)$: 표준 정규분포의 누적분포함수 (CDF)

**근사**:
$$
\text{GELU}(x) \approx 0.5x\left(1 + \tanh\left[\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right]\right)
$$

**특징**:
- 부드러운 비선형성
- Transformer에서 널리 사용 (BERT, GPT)

```python
gelu = nn.GELU()
y = gelu(x)
```

---

### Swish / SiLU (Sigmoid Linear Unit)

$$
\text{Swish}(x) = x \cdot \sigma(\beta x)
$$

$\beta = 1$일 때 SiLU라고 부름:
$$
\text{SiLU}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

**특징**:
- 부드럽고 미분 가능
- EfficientNet 등에서 사용

```python
silu = nn.SiLU()
y = silu(x)
```

---

## 활성화 함수 비교

| 함수 | 출력 범위 | 기울기 소실 | 계산 속도 | Zero-centered | 주요 사용처 |
|------|-----------|------------|-----------|---------------|------------|
| Sigmoid | $(0, 1)$ | 심함 | 느림 | X | 출력층 (이진) |
| Tanh | $(-1, 1)$ | 있음 | 느림 | O | RNN |
| ReLU | $[0, \infty)$ | 없음 | 빠름 | X | 기본 선택 |
| Leaky ReLU | $(-\infty, \infty)$ | 없음 | 빠름 | X | Dying ReLU 방지 |
| ELU | $(-\alpha, \infty)$ | 완화 | 중간 | 거의 | 노이즈 환경 |
| GELU | $(-\infty, \infty)$ | 없음 | 중간 | X | Transformer |
| Swish/SiLU | $(-\infty, \infty)$ | 없음 | 중간 | X | EfficientNet |

---

## 출력층 설계

### 다중 클래스 분류: Softmax

$$
\text{Softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

- $\mathbf{z} \in \mathbb{R}^K$: 로짓 (logits)
- 출력: 확률 분포 ($\sum_i p_i = 1$, $p_i \in [0, 1]$)

**손실 함수**: Cross Entropy Loss

```python
import torch.nn.functional as F

logits = torch.randn(4, 10)  # 배치 4, 클래스 10
probs = F.softmax(logits, dim=1)
print(probs.sum(dim=1))  # tensor([1., 1., 1., 1.])
```

---

### 이진 분류: Sigmoid

$$
p = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

**손실 함수**: Binary Cross Entropy (BCE)

```python
logit = torch.randn(4, 1)
prob = torch.sigmoid(logit)  # (4, 1)
```

---

### 회귀: Linear (활성화 없음)

$$
y = z
$$

출력에 제한 없음: $y \in (-\infty, \infty)$

**손실 함수**: MSE (Mean Squared Error)

```python
output = logits  # 활성화 함수 없음
```

---

### 다중 레이블 분류: Sigmoid (각 클래스마다)

각 클래스가 독립적으로 0 또는 1

$$
p_i = \sigma(z_i) \quad \text{for each } i
$$

**손실 함수**: BCE with Logits

```python
logits = torch.randn(4, 10)
probs = torch.sigmoid(logits)  # 각 클래스가 독립적
```

---

## 포화 문제 (Saturation)

**포화 (Saturation)**: 입력이 극단값일 때 기울기가 0에 가까워지는 현상

**Sigmoid/Tanh의 포화**:
- $x \to \infty$: $\sigma(x) \to 1$, $\sigma'(x) \to 0$
- $x \to -\infty$: $\sigma(x) \to 0$, $\sigma'(x) \to 0$

**결과**: 역전파 시 기울기가 소실되어 학습이 느려짐

**해결책**:
- ReLU 계열 사용
- Batch Normalization
- 적절한 가중치 초기화

---

## 실습 예제

### 활성화 함수 시각화

```python
import torch
import matplotlib.pyplot as plt

x = torch.linspace(-5, 5, 100)

# 여러 활성화 함수 비교
activations = {
    'Sigmoid': torch.sigmoid(x),
    'Tanh': torch.tanh(x),
    'ReLU': torch.relu(x),
    'LeakyReLU': torch.nn.functional.leaky_relu(x, negative_slope=0.1),
    'ELU': torch.nn.functional.elu(x),
}

plt.figure(figsize=(12, 6))
for name, y in activations.items():
    plt.plot(x.numpy(), y.numpy(), label=name, linewidth=2)

plt.xlabel('Input')
plt.ylabel('Output')
plt.title('Activation Functions Comparison')
plt.legend()
plt.grid(True)
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)
plt.show()
```

---

### 출력층 설계 예제

```python
import torch.nn as nn

class MultiClassClassifier(nn.Module):
    """다중 클래스 분류"""
    def __init__(self, input_dim, num_classes):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        logits = self.fc2(x)  # 활성화 없음
        return logits  # Softmax는 손실 함수에서 처리

class BinaryClassifier(nn.Module):
    """이진 분류"""
    def __init__(self, input_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        logit = self.fc2(x)  # 활성화 없음
        return logit  # Sigmoid는 손실 함수에서 처리

class Regressor(nn.Module):
    """회귀"""
    def __init__(self, input_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        output = self.fc2(x)  # 활성화 없음
        return output
```

---

## 선택 가이드

### 은닉층 활성화 함수

**기본 선택**: ReLU
- 대부분의 경우 잘 작동
- 빠르고 효율적

**ReLU 변형**:
- Dying ReLU 문제 발생 시: Leaky ReLU 또는 PReLU
- 더 부드러운 비선형성: ELU 또는 GELU

**특수 상황**:
- Transformer: GELU
- GAN: Leaky ReLU
- RNN: Tanh

---

### 출력층 활성화 함수

| 문제 유형 | 활성화 함수 | 손실 함수 |
|----------|------------|----------|
| 다중 클래스 분류 | Softmax | CrossEntropyLoss |
| 이진 분류 | Sigmoid | BCEWithLogitsLoss |
| 다중 레이블 분류 | Sigmoid | BCEWithLogitsLoss |
| 회귀 | Linear (없음) | MSELoss |
| 확률 예측 | Sigmoid | BCELoss |

---

## 연습 문제

### 기초
1. ReLU가 Sigmoid보다 기울기 소실 문제에 강한 이유를 수식으로 설명하시오.
2. Softmax 출력의 합이 항상 1인 이유를 증명하시오.
3. 활성화 함수가 없는 신경망이 선형 변환으로 축약되는 과정을 보이시오.

### 중급
4. 여러 활성화 함수(ReLU, Leaky ReLU, ELU, GELU)를 MNIST에 적용하고 성능을 비교하시오.
5. Dying ReLU 문제가 발생하는 경우를 실험으로 재현하고, Leaky ReLU로 해결하시오.
6. Softmax의 수치 안정성 문제와 해결 방법을 구현하시오.

### 고급
7. GELU의 수식 유도 과정을 설명하고, ReLU와의 차이점을 시각화하시오.
8. 활성화 함수의 선택이 학습 속도와 최종 성능에 미치는 영향을 실험으로 분석하시오.
9. 커스텀 활성화 함수를 설계하고 PyTorch에서 구현하여 성능을 평가하시오.

---

## 참고 자료

**논문**
- Nair & Hinton, "Rectified Linear Units Improve Restricted Boltzmann Machines" (2010) - ReLU
- Clevert et al., "Fast and Accurate Deep Network Learning by ELU" (2015) - ELU
- Hendrycks & Gimpel, "Gaussian Error Linear Units (GELUs)" (2016) - GELU
- Ramachandran et al., "Searching for Activation Functions" (2017) - Swish

**온라인 자료**
- Stanford CS231n: Lecture 6 - Training Neural Networks I
- Activation Functions Explained: https://mlfromscratch.com/activation-functions-explained/

---

## 다음 학습
- [[Foundations/7. 역전파와 최적화 전략]]
- [[Foundations/5. 신경망 기본 구조]]
- [[Foundations/4. 정규화와 일반화]]
