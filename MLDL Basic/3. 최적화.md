## TL;DR
- 머신러닝 최적화는 미분 가능한 손실 함수를 최소화하기 위해 파라미터를 업데이트하는 과정이며, 최적화 알고리즘과 하이퍼파라미터 선택이 모델 성능을 크게 좌우한다.
- 확률적 gradient, 모멘텀, 적응형 learning rate, 학습률 스케줄, gradient clipping 등 실전에서 자주 쓰이는 기법을 이해하면 트러블슈팅이 쉬워진다.
- 일반화와 수렴 속도 사이의 균형을 맞추기 위해 배치 크기, 정규화, 옵티마이저 설정을 함께 고려해야 한다.

## 핵심 요소

### 배치 크기 (Batch Size)

배치 크기에 따른 일반화와 수렴 속도의 차이는 **통계적 추정의 노이즈**와 **최적화 동역학**의 차이로 설명된다.

#### 작은 배치 (Small Batch)

**Gradient의 통계적 노이즈**

배치가 작으면, 전체 데이터의 일부만 보고 gradient를 추정한다:

$$
\hat{g} = \frac{1}{B} \sum_{i=1}^{B} \nabla_{\theta} L(x_i, y_i)
$$

- $\hat{g}$: 추정된 그래디언트
- $B$: 배치 크기
- $L(x_i, y_i)$: 개별 샘플의 손실

$B$가 작으면 표본 수가 적으므로 gradient의 **분산(variance)** 이 커진다:

$$
\text{Var}[\hat{g}] \propto \frac{1}{B}
$$

즉, 매 step마다 방향이 흔들린다. 완벽하게 최적 방향은 아니지만, 이 "노이즈"가 오히려 **local minimum이나 sharp minimum을 피하게 도와준다.**

**일반화 성능**: 작은 배치는 모델이 특정 훈련 데이터에 과하게 맞춰지지 않고, 더 넓은 지역(flat minimum)을 탐색하게 되어 **일반화(generalization)** 가 잘 된다.

**수렴 속도**: gradient의 분산이 크므로 매 step마다 조금씩 엇나가서 최적점 근처에서도 오락가락하며 **step 수가 많아진다.**

---

#### 큰 배치 (Large Batch)

**안정적 Gradient 추정**

배치가 크면, 많은 표본의 평균으로 gradient를 추정하므로 분산이 작다:

$$
\text{Var}[\hat{g}] \propto \frac{1}{B}
$$

즉, 방향이 매우 "정확"하고 **빠르게 loss가 줄어드는 방향**으로 움직인다. 따라서 **학습 속도(수렴)** 는 빠르다.

**일반화 성능 저하**

하지만 너무 정확해서 문제가 된다. 큰 배치는 **sharp minimum**(좁고 깊은 지역)에 빠지기 쉽다. 이런 minimum은 학습 데이터에서는 loss가 낮지만, 새로운 데이터에서는 오히려 성능이 떨어진다. 즉, **overfitting**이나 **일반화 성능 저하**로 이어진다.

---

#### Linear Scaling Rule

큰 배치를 쓰면 gradient가 평균되면서 magnitude가 작아지므로, learning rate도 배치 크기에 비례해서 키워야 한다:

$$
\eta_B = \eta_0 \times \frac{B}{B_0}
$$

- $\eta_B$: 배치 크기 $B$에 대한 학습률
- $\eta_0$: 기준 배치 크기 $B_0$에 대한 학습률

이것이 **Linear Scaling Rule**이다. "배치가 2배면, 학습률도 2배"로 맞춰야 같은 step당 업데이트 크기가 유지된다.

---

#### 배치 크기 비교

| 배치 크기  | Gradient 특징 | 수렴 속도 | 일반화 성능  | 특징 요약                      |
| ------ | ----------- | ----- | ------- | -------------------------- |
| **작음** | 노이즈 많음      | 느림    | 좋음      | 다양한 방향 탐색, flat minimum 유도 |
| **큼**  | 안정적         | 빠름    | 나쁠 수 있음 | sharp minimum 유도, 빠른 수렴    |

원리적으로 보면, **작은 배치는 exploration**, **큰 배치는 exploitation**에 가깝다고 볼 수 있다.

**현대적 접근**:
- **Gradient Accumulation**: 작은 배치를 여러 번 누적해 큰 배치 효과
- **Adaptive Batch Size**: 작은 배치로 시작해 점진적으로 증가
- **Noise Scheduling**: 학습 초기에는 노이즈를 유지하고 후반에 안정화

---

### Momentum 계열

#### SGD with Momentum (Heavy-Ball)

**핵심 아이디어**: 최근 그래디언트를 **지수이동평균(EMA)** 으로 누적해 "속도(velocity)"를 만든 뒤 그 방향으로 더 멀리 굴린다. 노이즈를 **저역통과 필터링**하고, 협곡(ravine) 같은 지형에서 **지그재그를 줄여** 빠르게 전진한다.

$$
\begin{aligned}
v_t &= \beta v_{t-1} + (1-\beta)\nabla f(\theta_t) \\
\theta_{t+1} &= \theta_t - \eta v_t
\end{aligned}
$$


- $v_t$: 속도 벡터 (velocity)
- $\beta$: 모멘텀 계수 (보통 0.9)
- $\eta$: 학습률 (learning rate)
- $\nabla f(\theta_t)$: 파라미터 $\theta_t$에서의 그래디언트

**분산 축소 효과**:

$v_t$는 $\nabla f$의 EMA이므로, 기댓값 주변에서 **분산 축소** 효과가 있다:

$$
\text{Var}[v_t] \approx \frac{1-\beta}{1+\beta}\text{Var}[\nabla f(\theta_t)]
$$

$\beta=0.9$이면 대략 $\frac{0.1}{1.9}\approx 0.053$: 노이즈가 크게 줄어든다.

**유효 스텝 크기**:

기울기가 거의 일정할 때, $v_t \to \nabla f$의 평균에 수렴하며, 누적 관성 때문에 체감상 **유효 학습률이 커진 것과 유사**하다:

$$
\text{유효 업데이트 크기} \sim \frac{\eta}{1-\beta}
$$

그래서 모멘텀을 키우면 $\eta$는 과감히 키우되 **안정성 한계**를 넘지 않게 조절한다.

**왜 "진동 억제 + 가속"이 되나**:
- **가파른 축**(큰 곡률, Hessian 고유값 큼)에서는 그래디언트 방향이 자주 **부호 반전** → 단순 SGD는 지그재그
- 모멘텀은 반대 방향으로 튕겨도 이전 속도를 일부 유지 → **횡축 진동 감소**, **완만한 축 전진 가속**

---

#### Nesterov Momentum (NAG)

**핵심 아이디어**: "현재 위치"가 아니라, **모멘텀으로 한 번 내딛은 예상 위치**에서 기울기를 본다. 그래서 **오버슈트(overshoot)** 를 줄이고, **정확히 더 '앞선' 정보**로 업데이트한다.

**Look-ahead 형태 (직관적 표현)**:

$$
\begin{aligned}
\tilde{\theta}_t &= \theta_t - \eta\beta v_{t-1} \quad\text{(모멘텀만큼 내다본 위치)} \\
g_t^{\text{NAG}} &= \nabla f(\tilde{\theta}_t) \\
v_t &= \beta v_{t-1} + (1-\beta)g_t^{\text{NAG}} \\
\theta_{t+1} &= \theta_t - \eta v_t
\end{aligned}
$$


**라이브러리 구현에서 자주 보는 등가형** (보정항이 드러남):

$$
\theta_{t+1} = \theta_t - \eta\Big(\beta v_t + \nabla f(\theta_t)\Big)
$$

이 형태로 보면, **표준 모멘텀 업데이트**에 $\beta v_t$가 **미리 당겨진 보정항**처럼 붙어 **오버슈트를 줄이는 효과**가 직관적으로 보인다. 즉, "갈 곳을 먼저 살짝 가본 뒤, 그 위치의 기울기 기준으로 브레이크/가속을 조절"하는 셈.

**언제 어떤 게 더 유리?**:
- **Momentum(Heavy-Ball)**: 구현 단순, 대부분의 비선형/확률적 설정에서 기본값처럼 잘 작동
- **Nesterov**: 곡률 변동이 큰 지형(깊고 좁은 계곡)에서 **오버슈트 감소**로 더 안정·빠른 수렴을 보이는 경우가 많음. 큰 배치/큰 $\eta$에서 특히 이점

**하이퍼파라미터 실전 팁**:
- $\beta$: 보통 **0.9** 시작. 더 **노이즈 많으면 0.8-0.9**, 더 **안정/대배치면 0.95-0.99**까지도
- $\eta$: 모멘텀을 키우면 $\frac{\eta}{1-\beta}$가 커지는 효과가 있으니 **과대 증가 주의**
- **Warmup**: 큰 배치·큰 모멘텀일수록 **LR 워밍업**이 유리
- **Weight Decay**: AdamW처럼 **decoupled** 방식을 권장(모멘텀과 뒤엉키지 않도록)

---

### Adaptive Optimizers

Adaptive optimizer는 **파라미터별로 학습률을 자동 조정**하여 각 차원의 기울기 크기에 맞춰 최적화한다. 수동 튜닝 부담을 줄이고, 다양한 스케일을 가진 파라미터에 효과적이다.

---

#### Adagrad (Adaptive Gradient)

**핵심 아이디어**: 과거 gradient가 크게 변했던 파라미터는 학습률을 줄이고, 작게 변했던 파라미터는 학습률을 유지한다. **희소(sparse) 데이터**나 **빈도가 낮은 특징**에 특히 유리하다.

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla f(\theta_t)
$$

$$
G_t = G_{t-1} + (\nabla f(\theta_t))^2
$$

- $G_t \in \mathbb{R}^d$: 과거 gradient 제곱의 **누적합** (각 파라미터별)
- $\epsilon$: 수치 안정성을 위한 작은 값 (보통 $10^{-8}$)
- $\nabla f(\theta_t)^2$: 요소별 제곱

**장점**:
- 파라미터별 자동 학습률 조정
- 희소 특징(sparse features)에서 효과적
- 학습률 수동 튜닝 부담 감소

**단점**:
- $G_t$가 **계속 누적**되므로 학습률이 **단조 감소**
- 장기 학습 시 학습률이 너무 작아져 학습이 **조기 정지**될 수 있음

**사용처**: 자연어 처리(NLP)의 단어 임베딩, 추천 시스템의 희소 특징

---

#### RMSProp (Root Mean Square Propagation)

**핵심 아이디어**: Adagrad의 학습률 급감 문제를 해결하기 위해 **지수이동평균(EMA)** 을 사용한다. 최근 gradient에 더 많은 가중치를 두어 오래된 gradient의 영향을 점차 줄인다.

$$
\begin{aligned}
v_t &= \beta v_{t-1} + (1-\beta) (\nabla f(\theta_t))^2 \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} \nabla f(\theta_t)
\end{aligned}
$$

- $v_t \in \mathbb{R}^d$: gradient 제곱의 **지수이동평균**
- $\beta$: 감쇠율 (보통 0.9 또는 0.99)
- Adagrad와 달리 $v_t$가 무한히 커지지 않음

**장점**:
- Adagrad의 학습률 급감 문제 해결
- 비정상적(non-stationary) 문제에 효과적
- RNN 학습에서 널리 사용됨

**단점**:
- Momentum이 없어 진동하는 지형에서는 여전히 느릴 수 있음

**하이퍼파라미터**:
- $\eta$: 보통 0.001
- $\beta$: 0.9 (표준), 0.99 (더 안정적)

---

#### Adam (Adaptive Moment Estimation)

**핵심 아이디어**: **Momentum(1차 모멘트)** 과 **RMSProp(2차 모멘트)** 을 결합한다. Gradient의 평균(방향)과 분산(크기)을 동시에 추적하며, **편향 보정(bias correction)** 으로 초기 학습 안정성을 높인다.

$$
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) \nabla f(\theta_t) \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) (\nabla f(\theta_t))^2 \\
\hat{m}_t &= \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{aligned}
$$

- $m_t$: **1차 모멘트** (gradient의 지수이동평균, 방향)
- $v_t$: **2차 모멘트** (gradient 제곱의 지수이동평균, 크기)
- $\beta_1$: 1차 모멘트 감쇠율 (보통 **0.9**)
- $\beta_2$: 2차 모멘트 감쇠율 (보통 **0.999**)
- $\hat{m}_t, \hat{v}_t$: **편향 보정**된 추정치 (초기 0에서 시작하는 편향 제거)

**편향 보정의 필요성**:

$m_0 = 0, v_0 = 0$으로 초기화하면 초기 몇 step 동안 $m_t, v_t$가 0에 가깝게 편향된다. $(1-\beta^t)$로 나누면 이 편향을 보정한다:

$$
\mathbb{E}[\hat{m}_t] = \mathbb{E}[\nabla f(\theta_t)]
$$

**장점**:
- 거의 모든 문제에서 **잘 작동** (범용성)
- 하이퍼파라미터 튜닝 부담이 적음
- 수렴 속도가 빠름

**단점**:
- **일반화 성능**이 SGD+Momentum보다 떨어질 수 있음 (sharp minimum 경향)
- Weight decay와 L2 regularization이 다르게 작동 (AdamW로 해결)

**기본 하이퍼파라미터**:
- $\eta = 0.001$ (또는 $3 \times 10^{-4}$)
- $\beta_1 = 0.9$
- $\beta_2 = 0.999$
- $\epsilon = 10^{-8}$

---

#### AdamW (Adam with Decoupled Weight Decay)

**핵심 아이디어**: Adam에서 **L2 regularization**과 **weight decay**가 다르게 작동한다는 문제를 해결한다. Weight decay를 **gradient에서 분리(decouple)** 하여 일반화 성능을 크게 개선한다.

**전통적 Adam의 문제**:

L2 regularization:

$$
L_{\text{total}} = L_{\text{data}} + \frac{\lambda}{2}\|\theta\|^2
$$

이를 gradient에 반영하면:

$$
\nabla f(\theta) + \lambda\theta
$$

하지만 Adam에서는 이 gradient가 **adaptive scaling**을 거치므로, 실제 weight decay 효과가 왜곡된다.

**AdamW 해결책**:

Weight decay를 **optimizer step에서 직접** 적용:

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t - \eta \lambda \theta_t
$$

- $\lambda$: weight decay 계수 (보통 0.01)
- 마지막 항 $-\eta \lambda \theta_t$가 **decoupled weight decay**

**장점**:
- Adam의 수렴 속도 + SGD의 일반화 성능
- Transformer, BERT, GPT 등 **현대 딥러닝의 표준**
- Weight decay가 의도대로 작동

**PyTorch 사용**:

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)
```

---

#### Adaptive Optimizer 비교

| Optimizer | 1차 모멘트 | 2차 모멘트 | 편향 보정 | Weight Decay | 주요 사용처 |
|-----------|---------|---------|---------|--------------|-----------|
| **Adagrad** | X | 누적 | X | Coupled | 희소 데이터, NLP 임베딩 |
| **RMSProp** | X | EMA | X | Coupled | RNN, 비정상 문제 |
| **Adam** | EMA | EMA | O | Coupled | 범용 (대부분의 문제) |
| **AdamW** | EMA | EMA | O | **Decoupled** | **Transformer, LLM** |

---

#### 언제 어떤 Optimizer를 쓸까?

**AdamW (기본 선택)**:
- 대부분의 딥러닝 문제
- Transformer 계열 (BERT, GPT, ViT)
- 빠른 프로토타이핑

**SGD + Momentum + Nesterov**:
- **최고의 일반화 성능**이 필요할 때
- CNN (ResNet, EfficientNet)
- 충분한 튜닝 시간이 있을 때

**RMSProp**:
- RNN, LSTM
- 강화학습 (DQN, A3C)

**Adagrad**:
- 희소 데이터 (추천 시스템)
- 단어 임베딩 학습

---

#### 실전 팁

**Learning Rate 선택**:
- Adam/AdamW: $10^{-3}$ 또는 $3 \times 10^{-4}$ 시작
- SGD+Momentum: $10^{-1}$ ~ $10^{-2}$ 시작 (더 큼)
- Learning rate finder 사용 권장

**Weight Decay**:
- AdamW: 0.01 ~ 0.1
- SGD: 0.0001 ~ 0.001
- 작은 모델일수록 더 큰 값

**Warmup**:
- Transformer는 **필수** (처음 몇천 step 동안 학습률을 0에서 천천히 증가)
- Adam도 큰 배치에서는 warmup 권장

**Gradient Accumulation**:
- 작은 GPU 메모리에서 큰 배치 효과를 내려면 여러 step gradient를 누적

```python
optimizer.zero_grad()
for i, (x, y) in enumerate(loader):
    loss = model(x, y) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

### 학습률 스케줄

**Step Decay**: 일정 epoch마다 학습률 감소

**Exponential Decay**: 지수적으로 감소

**Cosine Annealing**:

$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t}{T}\pi\right)\right)
$$

- $T$: 총 epoch 수
- $t$: 현재 epoch

**Warmup**: 초기에 작은 학습률로 안정화 후 점차 증가

---

### Gradient Clipping & Scaling

**Gradient Clipping**

Gradient norm이 임계값 이상이면 비율 조정:

$$
\tilde{g} = \begin{cases}
g & \text{if } \|g\| \le \tau \\
\frac{\tau}{\|g\|} g & \text{otherwise}
\end{cases}
$$

- $g$: 원래 그래디언트
- $\tau$: 임계값 (max norm)
- $\tilde{g}$: 클리핑된 그래디언트

폭주 방지에 효과적

**AMP (Automatic Mixed Precision)**: Gradient scaling으로 underflow 방지

---

### 2차 정보 활용

**Newton Method**, **Natural Gradient**, **Fisher 정보 행렬**

대규모 모델에서는 근사 방법 사용 (Adafactor, Shampoo 등)

## 실습 예제

### Momentum/Nesterov 직접 구현

**Momentum (Heavy-Ball)**:

```python
# Momentum 직접 구현
beta = 0.9
lr = 0.01

for x, y in loader:
    optimizer.zero_grad()
    loss = loss_fn(model(x), y)
    loss.backward()

    with torch.no_grad():
        for p in model.parameters():
            if not hasattr(p, 'v'):
                p.v = torch.zeros_like(p.grad)
            p.v = beta * p.v + (1 - beta) * p.grad
            p -= lr * p.v
```

**Nesterov (Look-ahead 버전)**:

```python
# Nesterov 직접 구현 (look-ahead 방식)
beta = 0.9
lr = 0.01

for x, y in loader:
    # 1) look-ahead 위치로 임시 이동
    with torch.no_grad():
        for p in model.parameters():
            if not hasattr(p, 'v'):
                p.v = torch.zeros_like(p.grad if p.grad is not None else p)
            p._backup = p.data.clone()
            p.data.add_(-lr * beta, p.v)

    # 2) 그 위치에서 gradient 계산
    optimizer.zero_grad()
    loss = loss_fn(model(x), y)
    loss.backward()

    # 3) velocity 갱신 + 실제 업데이트
    with torch.no_grad():
        for p in model.parameters():
            p.v = beta * p.v + (1 - beta) * p.grad
            p.data = p._backup - lr * p.v
            del p._backup
```

**PyTorch 내장 사용**:

```python
# 실무에서는 내장 함수 사용
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # Momentum
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)  # NAG
```

---

### AdamW + 스케줄러 조합

```python
import torch
from torch.optim import SGD, AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR

model = torch.nn.Linear(100, 10)
criterion = torch.nn.CrossEntropyLoss()

optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)
scheduler = CosineAnnealingLR(optimizer, T_max=50)

for epoch in range(50):
    for x, y in dataloader:
        optimizer.zero_grad()
        logits = model(x)
        loss = criterion(logits, y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
    scheduler.step()
```

AdamW + cosine annealing + gradient clipping 조합은 Transformer류 모델에서 널리 사용되는 패턴이다.

## 실험 포인트
- 동일 모델에 대해 SGD, SGD+Momentum, AdamW를 적용해 학습 곡선과 최종 정확도 비교.
- Learning rate finder(주석 달린 lr range test)로 적절한 초기 학습률을 찾기.
- 배치 크기를 32/128/512로 바꾸어 loss 변동성과 generalization 측정.
- Gradient clipping 적용 시/미적용 시 gradient norm의 변화를 로깅.

### 배치 크기 실험 예제

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# 동일 모델, 다른 배치 크기로 학습
batch_sizes = [32, 128, 512]
results = {}

for batch_size in batch_sizes:
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    model = SimpleNet()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01 * (batch_size / 32))  # Linear scaling

    train_losses = []
    test_accs = []

    for epoch in range(50):
        # Training
        model.train()
        epoch_loss = 0
        for x, y in train_loader:
            optimizer.zero_grad()
            loss = criterion(model(x), y)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        train_losses.append(epoch_loss / len(train_loader))

        # Evaluation
        model.eval()
        correct = 0
        with torch.no_grad():
            for x, y in test_loader:
                correct += (model(x).argmax(1) == y).sum().item()
        test_accs.append(correct / len(test_dataset))

    results[batch_size] = {'train_loss': train_losses, 'test_acc': test_accs}

# 시각화
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

for bs in batch_sizes:
    ax1.plot(results[bs]['train_loss'], label=f'Batch {bs}')
    ax2.plot(results[bs]['test_acc'], label=f'Batch {bs}')

ax1.set_xlabel('Epoch')
ax1.set_ylabel('Train Loss')
ax1.set_title('Train Loss: 작은 배치가 노이즈가 많음')
ax1.legend()

ax2.set_xlabel('Epoch')
ax2.set_ylabel('Test Accuracy')
ax2.set_title('Test Accuracy: 작은 배치가 일반화 좋음')
ax2.legend()

plt.tight_layout()
plt.show()
```

**예상 결과**:
- 작은 배치(32): Train loss가 흔들리지만 test accuracy가 높음
- 큰 배치(512): Train loss가 부드럽지만 test accuracy가 낮을 수 있음

## 일반화 관점에서의 최적화
- 큰 학습률과 노이즈는 flat minimum(평평한 지역)에 도달하도록 도와 일반화에 유리할 수 있다.
- Adaptive optimizer는 빠르게 수렴하지만 overfitting 위험이 있으므로 weight decay나 learning rate decay로 보완.
- Sharpness-Aware Minimization(SAM), stochastic weight averaging(SWA) 등 최근 기법은 손실 landscape의 평평함을 직접 제어한다.

## 추천 참고 자료
- Sebastian Ruder, *An overview of gradient descent optimization algorithms*
- Loshchilov & Hutter, *Decoupled Weight Decay Regularization (AdamW)*
- Smith, *Cyclical Learning Rates for Training Neural Networks*

## 다음 학습
- [[Foundations/4. 정규화와 일반화]]
- [[Foundations/5. 신경망 기본 구조]]
