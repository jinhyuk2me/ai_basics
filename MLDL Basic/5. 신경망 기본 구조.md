## TL;DR
- 신경망은 입력층, 은닉층, 출력층으로 구성된 층별 구조로, 각 층은 선형 변환과 비선형 활성화 함수의 조합이다.
- 다층 퍼셉트론(MLP)은 가장 기본적인 피드포워드 신경망으로, 모든 뉴런이 다음 층의 모든 뉴런과 연결된다(Fully Connected).
- 파라미터 수는 모델 용량을 결정하며, 너무 적으면 과소적합, 너무 많으면 과적합의 위험이 있다.

## 핵심 개념

### 신경망의 기본 구조

신경망은 생물학적 뉴런에서 영감을 받은 수학적 모델로, **층(Layer)**으로 구성된다.

**구성 요소**:
1. **입력층 (Input Layer)**: 데이터를 받아들이는 층
2. **은닉층 (Hidden Layer)**: 특징을 추출하는 중간 층 (1개 이상)
3. **출력층 (Output Layer)**: 최종 예측을 생성하는 층

---

### 뉴런 (Neuron)

개별 뉴런은 다음과 같이 동작한다:

$$
y = \sigma\left(\sum_{i=1}^{n} w_i x_i + b\right) = \sigma(w^\top x + b)
$$

- $x = [x_1, x_2, \ldots, x_n]^\top$: 입력 벡터
- $w = [w_1, w_2, \ldots, w_n]^\top$: 가중치 벡터
- $b$: 편향 (bias)
- $\sigma$: 활성화 함수 (Activation Function)
- $y$: 뉴런의 출력

**해석**: 입력의 가중합을 계산하고, 비선형 활성화 함수를 적용한다.

---

### 다층 퍼셉트론 (Multi-Layer Perceptron, MLP)

MLP는 여러 층을 쌓은 피드포워드 신경망이다.

**2-층 신경망 (1개 은닉층) 예시**:

$$
\begin{aligned}
\mathbf{h} &= \sigma_1(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) \\
\mathbf{y} &= \sigma_2(\mathbf{W}_2 \mathbf{h} + \mathbf{b}_2)
\end{aligned}
$$

- $\mathbf{x} \in \mathbb{R}^{d_{\text{in}}}$: 입력 벡터
- $\mathbf{W}_1 \in \mathbb{R}^{d_h \times d_{\text{in}}}$: 첫 번째 층 가중치 행렬
- $\mathbf{b}_1 \in \mathbb{R}^{d_h}$: 첫 번째 층 편향
- $\mathbf{h} \in \mathbb{R}^{d_h}$: 은닉층 출력 (은닉 상태)
- $\mathbf{W}_2 \in \mathbb{R}^{d_{\text{out}} \times d_h}$: 두 번째 층 가중치 행렬
- $\mathbf{b}_2 \in \mathbb{R}^{d_{\text{out}}}$: 두 번째 층 편향
- $\mathbf{y} \in \mathbb{R}^{d_{\text{out}}}$: 출력 벡터
- $\sigma_1, \sigma_2$: 활성화 함수

---

### L-층 신경망

일반적으로 $L$개의 층을 가진 신경망은 다음과 같이 표현된다:

$$
\begin{aligned}
\mathbf{a}^{[0]} &= \mathbf{x} \\
\mathbf{z}^{[l]} &= \mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]} \quad \text{for } l = 1, 2, \ldots, L \\
\mathbf{a}^{[l]} &= \sigma^{[l]}(\mathbf{z}^{[l]}) \\
\mathbf{y} &= \mathbf{a}^{[L]}
\end{aligned}
$$

- $\mathbf{a}^{[l]}$: 층 $l$의 활성화 (activation)
- $\mathbf{z}^{[l]}$: 층 $l$의 사전 활성화 (pre-activation, logits)
- $\mathbf{W}^{[l]}, \mathbf{b}^{[l]}$: 층 $l$의 파라미터

---

## 순전파 (Forward Propagation)

순전파는 입력에서 출력까지 데이터를 전방으로 흘려보내는 과정이다.

**알고리즘**:

1. 입력 $\mathbf{x}$를 받음
2. 각 층 $l = 1, 2, \ldots, L$에 대해:
   - 선형 변환: $\mathbf{z}^{[l]} = \mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]}$
   - 활성화 함수 적용: $\mathbf{a}^{[l]} = \sigma^{[l]}(\mathbf{z}^{[l]})$
3. 최종 출력 $\mathbf{y} = \mathbf{a}^{[L]}$ 반환

**벡터화 (배치 처리)**:

배치 크기 $m$인 경우, 행렬 형태로 처리:

$$
\mathbf{Z}^{[l]} = \mathbf{W}^{[l]} \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]}
$$

- $\mathbf{A}^{[l]} \in \mathbb{R}^{n^{[l]} \times m}$: 배치 활성화 행렬 (각 열이 샘플)
- $\mathbf{Z}^{[l]} \in \mathbb{R}^{n^{[l]} \times m}$: 배치 사전 활성화 행렬

---

## 파라미터 수 계산

모델의 용량은 학습 가능한 파라미터 수로 측정된다.

**Fully Connected Layer의 파라미터 수**:

$$
\text{Params} = d_{\text{in}} \times d_{\text{out}} + d_{\text{out}}
$$

- $d_{\text{in}} \times d_{\text{out}}$: 가중치 행렬 크기
- $d_{\text{out}}$: 편향 벡터 크기

**예시**: 입력 784, 은닉 128, 출력 10인 2-층 MLP

$$
\begin{aligned}
\text{Layer 1} &: 784 \times 128 + 128 = 100{,}480 \\
\text{Layer 2} &: 128 \times 10 + 10 = 1{,}290 \\
\text{Total} &: 101{,}770 \text{ parameters}
\end{aligned}
$$

---

## Universal Approximation Theorem (보편 근사 정리)

충분히 넓은 은닉층을 가진 단일 은닉층 신경망은 임의의 연속 함수를 임의의 정확도로 근사할 수 있다.

**정리**:

$$
\forall \epsilon > 0, \exists N, \{w_i, b_i\}_{i=1}^N \text{ such that } |f(x) - \sum_{i=1}^{N} w_i \sigma(v_i^\top x + b_i)| < \epsilon
$$

**실무적 의미**:
- 이론적으로는 단일 은닉층으로 충분하지만, 실제로는 깊은 네트워크가 더 효율적
- 깊은 네트워크는 더 적은 뉴런으로 복잡한 함수를 표현 가능

---

## 모델 복잡도와 표현력

### 깊이 vs 너비

**깊이 (Depth)**: 층의 개수
- 장점: 계층적 특징 학습, 파라미터 효율적
- 단점: 학습 어려움 (기울기 소실/폭발)

**너비 (Width)**: 각 층의 뉴런 개수
- 장점: 학습 안정적
- 단점: 파라미터 많음, 과적합 위험

**경험적 가이드라인**:
- 작은 데이터셋: 얕고 좁은 네트워크
- 큰 데이터셋: 깊고 넓은 네트워크

---

## 실습 예제

### 기본 MLP 구현

```python
import torch
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)  # 첫 번째 층
        self.relu = nn.ReLU()                         # 활성화 함수
        self.fc2 = nn.Linear(hidden_dim, output_dim)  # 두 번째 층

    def forward(self, x):
        """순전파"""
        # x: (batch_size, input_dim)
        h = self.fc1(x)       # (batch_size, hidden_dim)
        h = self.relu(h)      # (batch_size, hidden_dim)
        y = self.fc2(h)       # (batch_size, output_dim)
        return y

# 모델 생성
model = MLP(input_dim=784, hidden_dim=128, output_dim=10)

# 파라미터 수 확인
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params}")  # 101,770

# 순전파 테스트
x = torch.randn(32, 784)  # 배치 크기 32
y = model(x)              # (32, 10)
print(f"Output shape: {y.shape}")
```

---

### 다층 MLP 구현

```python
class DeepMLP(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim):
        """
        hidden_dims: 은닉층 크기 리스트 (예: [256, 128, 64])
        """
        super(DeepMLP, self).__init__()

        layers = []
        prev_dim = input_dim

        # 은닉층 생성
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            prev_dim = hidden_dim

        # 출력층
        layers.append(nn.Linear(prev_dim, output_dim))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# 3-은닉층 MLP
model = DeepMLP(
    input_dim=784,
    hidden_dims=[256, 128, 64],
    output_dim=10
)

# 각 층의 파라미터 수 확인
for name, param in model.named_parameters():
    print(f"{name}: {param.shape} ({param.numel()} params)")
```

---

### 순전파 수동 구현

```python
import torch

def manual_forward(x, W1, b1, W2, b2):
    """
    2-층 MLP 순전파 수동 구현
    """
    # 첫 번째 층
    z1 = torch.matmul(x, W1.t()) + b1  # (batch, hidden)
    a1 = torch.relu(z1)                 # ReLU 활성화

    # 두 번째 층
    z2 = torch.matmul(a1, W2.t()) + b2  # (batch, output)

    return z2

# 파라미터 초기화
input_dim, hidden_dim, output_dim = 10, 20, 5
W1 = torch.randn(hidden_dim, input_dim) * 0.01
b1 = torch.zeros(hidden_dim)
W2 = torch.randn(output_dim, hidden_dim) * 0.01
b2 = torch.zeros(output_dim)

# 순전파
x = torch.randn(8, input_dim)  # 배치 크기 8
y = manual_forward(x, W1, b1, W2, b2)
print(f"Output: {y.shape}")  # (8, 5)
```

---

## 설계 시 고려사항

### 하이퍼파라미터 선택

**은닉층 개수**:
- 시작: 1-3개 은닉층
- 복잡한 문제: 3개 이상

**은닉 유닛 개수**:
- 입력 차원보다 크게: 특징 확장
- 입력 차원보다 작게: 차원 축소 (오토인코더)
- 경험적 규칙: 입력과 출력 차원의 기하평균

$$
d_h \approx \sqrt{d_{\text{in}} \times d_{\text{out}}}
$$

**활성화 함수**:
- 은닉층: ReLU (기본 선택)
- 출력층: 문제에 따라 다름
  - 분류: Softmax
  - 이진 분류: Sigmoid
  - 회귀: Linear (활성화 없음)

---

### 초기화 전략

파라미터를 어떻게 초기화하느냐가 학습 성공 여부를 좌우한다.

**Xavier 초기화 (Tanh, Sigmoid용)**:

$$
W \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}\right)
$$

**He 초기화 (ReLU용)**:

$$
W \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n_{\text{in}}}}\right)
$$

```python
# PyTorch 구현
import torch.nn.init as init

# He 초기화 (ReLU용)
init.kaiming_normal_(model.fc1.weight, mode='fan_in', nonlinearity='relu')
init.zeros_(model.fc1.bias)

# Xavier 초기화 (Tanh용)
init.xavier_normal_(model.fc1.weight)
```

---

## 스킵 연결 (Skip Connection)

깊은 네트워크에서 기울기 소실 문제를 완화하기 위해 층을 건너뛰는 연결을 추가한다.

**Residual Connection (ResNet)**:

$$
\mathbf{a}^{[l]} = \mathbf{a}^{[l-1]} + F(\mathbf{a}^{[l-1]})
$$

```python
class ResidualBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.fc1 = nn.Linear(dim, dim)
        self.fc2 = nn.Linear(dim, dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        residual = x  # 스킵 연결
        out = self.relu(self.fc1(x))
        out = self.fc2(out)
        out = out + residual  # 잔차 더하기
        out = self.relu(out)
        return out
```

---

## 연습 문제

### 기초
1. 입력 100, 은닉 [50, 30], 출력 10인 3-층 MLP의 총 파라미터 수를 계산하시오.
2. 활성화 함수가 없는 다층 신경망은 왜 단일 선형 변환과 동등한지 수식으로 설명하시오.
3. 순전파와 역전파의 차이를 설명하시오.

### 중급
4. MNIST 데이터셋에 대해 1-은닉층, 2-은닉층, 3-은닉층 MLP를 구현하고 테스트 정확도를 비교하시오.
5. 은닉 유닛 개수를 [32, 64, 128, 256]으로 바꾸어가며 모델 성능과 학습 시간을 측정하시오.
6. PyTorch의 `nn.Sequential`을 사용하여 5-층 MLP를 구현하시오.

### 고급
7. Universal Approximation Theorem이 왜 실무에서 깊은 네트워크를 사용하는 것과 모순되지 않는지 논하시오.
8. Residual Connection이 있는 MLP와 없는 MLP를 구현하고, 매우 깊은 네트워크(10층 이상)에서 성능을 비교하시오.
9. 가중치 초기화 방법(Xavier vs He)에 따른 학습 곡선 차이를 시각화하고 분석하시오.

---

## 참고 자료

**서적**
- Goodfellow et al., *Deep Learning*, Chapter 6: Deep Feedforward Networks
- Bishop, *Pattern Recognition and Machine Learning*, Chapter 5

**논문**
- Cybenko, "Approximation by Superpositions of a Sigmoidal Function" (1989) - Universal Approximation
- He et al., "Deep Residual Learning for Image Recognition" (2015) - ResNet

**온라인 강의**
- Stanford CS231n: Lecture 4 - Neural Networks Part 1
- 3Blue1Brown: Neural Networks Series

---

## 다음 학습
- [[Foundations/6. 활성화 함수와 출력층 설계]]
- [[Foundations/7. 역전파와 최적화 전략]]
- [[Foundations/PyTorch Tensor]]
