## TL;DR
- 시퀀스 모델은 순서가 있는 데이터(텍스트, 시계열, 음성 등)를 처리하기 위해 설계된 신경망이다.
- RNN은 기본적인 시퀀스 모델이지만 장기 의존성 문제를 겪으며, LSTM과 GRU가 이를 해결한다.
- Transformer는 self-attention 메커니즘을 통해 병렬 처리가 가능하고 장거리 의존성을 효과적으로 학습한다.
- 현대 NLP와 시계열 분석의 핵심은 Transformer 기반 모델로 이동하고 있다.

## 핵심 개념

### 시퀀스 데이터 (Sequential Data)

순서가 중요한 데이터. 순서를 바꾸면 의미가 달라진다.

**예시**:
- 자연어: "나는 밥을 먹었다" ≠ "먹었다 밥을 나는"
- 시계열: 주가 데이터, 센서 측정값
- 음성: 음소의 순서
- 비디오: 프레임의 연속

**표기법**:

$$
\mathbf{x} = (x_1, x_2, \ldots, x_T)
$$

- $x_t \in \mathbb{R}^d$: 시간 $t$에서의 입력
- $T$: 시퀀스 길이

---

## 순환 신경망 (RNN)

### 기본 RNN

**구조**:

$$
h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
$$

$$
y_t = W_{hy} h_t + b_y
$$

- $h_t \in \mathbb{R}^h$: 시간 $t$에서의 은닉 상태 (hidden state)
- $h_{t-1}$: 이전 시간 단계의 은닉 상태
- $x_t \in \mathbb{R}^d$: 현재 입력
- $W_{hh} \in \mathbb{R}^{h \times h}$: 은닉-은닉 가중치
- $W_{xh} \in \mathbb{R}^{h \times d}$: 입력-은닉 가중치
- $W_{hy} \in \mathbb{R}^{o \times h}$: 은닉-출력 가중치
- $y_t \in \mathbb{R}^o$: 출력

**특징**:
- 가중치 공유: 모든 시간 단계에서 같은 $W$를 사용
- 순차 처리: $h_t$는 $h_{t-1}$에 의존하므로 병렬화 불가능
- 이론적으로 무한한 과거 정보를 기억 가능

---

### RNN의 문제점: 기울기 소실/폭발

**역전파 시간 (BPTT, Backpropagation Through Time)**:

시간을 거슬러 올라가며 기울기를 계산할 때:

$$
\frac{\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}} = \prod_{i=k+1}^{t} W_{hh}^T \cdot \text{diag}(\tanh'(z_i))
$$

- $z_i$: 시간 $i$에서의 활성화 함수 입력

**문제**:
- $\tanh'(x) \in [0, 1]$ → 곱이 누적되면 기울기가 0으로 수렴 (기울기 소실)
- $\|W_{hh}\| > 1$이면 기울기가 기하급수적으로 증가 (기울기 폭발)

**결과**: 장기 의존성(Long-term dependency)을 학습하기 어려움

---

## LSTM (Long Short-Term Memory)

### 구조

LSTM은 셀 상태 (cell state) $c_t$를 추가하고, 3개의 게이트로 정보 흐름을 제어한다.

**게이트 (Gates)**:

1. **Forget Gate**: 과거 정보를 얼마나 잊을지 결정

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

2. **Input Gate**: 새 정보를 얼마나 받을지 결정

$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$

$$
\tilde{c}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)
$$

3. **Output Gate**: 출력을 얼마나 낼지 결정

$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$

**셀 상태 업데이트**:

$$
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
$$

**은닉 상태 업데이트**:

$$
h_t = o_t \odot \tanh(c_t)
$$

- $\sigma$: Sigmoid 활성화 함수 (0~1 범위로 게이트 역할)
- $\odot$: 요소별 곱셈 (element-wise multiplication)
- $[h_{t-1}, x_t]$: 벡터 연결 (concatenation)
- $f_t, i_t, o_t \in \mathbb{R}^h$: 각 게이트 값
- $c_t \in \mathbb{R}^h$: 셀 상태
- $\tilde{c}_t \in \mathbb{R}^h$: 후보 셀 상태

**장점**:
- 셀 상태 $c_t$는 덧셈으로 업데이트되어 기울기 소실 완화
- 장기 의존성을 효과적으로 학습

---

## GRU (Gated Recurrent Unit)

LSTM을 단순화한 변형. 게이트를 2개로 줄이고 셀 상태와 은닉 상태를 통합.

**수식**:

1. **Update Gate**:

$$
z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
$$

2. **Reset Gate**:

$$
r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
$$

3. **후보 은닉 상태**:

$$
\tilde{h}_t = \tanh(W \cdot [r_t \odot h_{t-1}, x_t] + b)
$$

4. **은닉 상태 업데이트**:

$$
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
$$

- $z_t$: 과거와 현재 정보의 비율 조절 (forget + input gate 역할)
- $r_t$: 과거 정보를 얼마나 사용할지 결정

**장점**:
- LSTM보다 파라미터가 적어 학습 속도가 빠름
- 성능은 LSTM과 비슷한 수준

---

## Attention 메커니즘

### Seq2Seq의 한계

**Encoder-Decoder 구조**:
- Encoder: 입력 시퀀스를 고정 길이 벡터 $c$로 압축
- Decoder: $c$를 바탕으로 출력 시퀀스 생성

**문제**: 긴 시퀀스의 모든 정보를 하나의 벡터 $c$에 담기 어려움 (정보 병목)

---

### Attention의 아이디어

**핵심**: 각 출력 단계마다 입력의 다른 부분에 집중 (attend)

**수식**:

1. **Attention Score** (유사도 계산):

$$
e_{t,i} = \text{score}(h_t, s_i)
$$

- $h_t$: 디코더의 $t$번째 은닉 상태
- $s_i$: 인코더의 $i$번째 은닉 상태

2. **Attention Weight** (softmax로 정규화):

$$
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^{T} \exp(e_{t,j})}
$$

3. **Context Vector** (가중합):

$$
c_t = \sum_{i=1}^{T} \alpha_{t,i} s_i
$$

4. **출력**:

$$
y_t = f(h_t, c_t)
$$

- $\alpha_{t,i}$: $t$번째 출력이 $i$번째 입력에 주는 가중치
- $c_t$: $t$번째 출력을 위한 문맥 벡터

**효과**: 긴 시퀀스에서도 관련된 정보에 직접 접근 가능

---

## Transformer

### Self-Attention

**아이디어**: 시퀀스 내 모든 위치 간의 관계를 한 번에 계산

**수식**:

1. **Query, Key, Value 생성**:

$$
Q = XW_Q, \quad K = XW_K, \quad V = XW_V
$$

- $X \in \mathbb{R}^{T \times d}$: 입력 시퀀스 ($T$개 토큰, 각 $d$차원)
- $W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}$: 학습 가능한 가중치
- $Q, K, V \in \mathbb{R}^{T \times d_k}$: Query, Key, Value 행렬

2. **Scaled Dot-Product Attention**:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

- $QK^T \in \mathbb{R}^{T \times T}$: 모든 위치 쌍의 유사도
- $\sqrt{d_k}$: 스케일링 (큰 값으로 인한 softmax 포화 방지)
- $\text{softmax}(...)$: 각 행을 확률 분포로 변환
- 출력: $\mathbb{R}^{T \times d_k}$

**특징**:
- 모든 위치 간의 관계를 병렬로 계산 (RNN처럼 순차 처리 불필요)
- 장거리 의존성을 직접 모델링

---

### Multi-Head Attention

여러 개의 attention을 병렬로 수행하여 다양한 관점에서 정보 추출

**수식**:

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
$$

- $h$: head 개수 (보통 8 또는 16)
- $W_i^Q, W_i^K, W_i^V$: 각 head의 가중치
- $W^O \in \mathbb{R}^{hd_k \times d}$: 출력 투영 행렬

**장점**: 서로 다른 표현 부분공간(subspace)에서 정보를 학습

---

### Positional Encoding

**문제**: Self-Attention은 위치 정보를 고려하지 않음 (순서 무관)

**해결**: 각 위치에 고유한 벡터를 추가

**수식** (Sinusoidal Encoding):

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$

- $pos$: 위치 (0, 1, 2, ...)
- $i$: 차원 인덱스
- $d$: 임베딩 차원

**대안**: 학습 가능한 위치 임베딩 (Learned Positional Embedding)

---

### Transformer Block

**Encoder Block**:

```
Input
  ↓
Multi-Head Self-Attention
  ↓
Add & LayerNorm
  ↓
Feed-Forward Network (2-layer MLP)
  ↓
Add & LayerNorm
  ↓
Output
```

**Decoder Block**:

```
Input
  ↓
Masked Multi-Head Self-Attention
  ↓
Add & LayerNorm
  ↓
Multi-Head Cross-Attention (with Encoder output)
  ↓
Add & LayerNorm
  ↓
Feed-Forward Network
  ↓
Add & LayerNorm
  ↓
Output
```

**Feed-Forward Network**:

$$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$

- 각 위치에 독립적으로 적용되는 2-layer MLP

---

## 실습 예제

### RNN 구현

```python
import torch
import torch.nn as nn

class VanillaRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.hidden_size = hidden_size

        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.i2o = nn.Linear(input_size + hidden_size, output_size)
        self.tanh = nn.Tanh()

    def forward(self, x, hidden):
        """
        x: (batch, input_size)
        hidden: (batch, hidden_size)
        """
        combined = torch.cat([x, hidden], dim=1)  # (batch, input_size + hidden_size)
        hidden = self.tanh(self.i2h(combined))
        output = self.i2o(combined)
        return output, hidden

    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_size)

# 사용 예제
rnn = VanillaRNN(input_size=10, hidden_size=20, output_size=5)
x = torch.randn(4, 10)  # 배치 4
hidden = rnn.init_hidden(batch_size=4)

output, hidden = rnn(x, hidden)
print(f"Output: {output.shape}")  # (4, 5)
print(f"Hidden: {hidden.shape}")  # (4, 20)
```

---

### LSTM 구현 (PyTorch 내장)

```python
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        """
        x: (batch, seq_len) - 토큰 인덱스
        """
        # 임베딩: (batch, seq_len, embedding_dim)
        embedded = self.embedding(x)

        # LSTM: output (batch, seq_len, hidden_dim)
        #       hidden, cell (1, batch, hidden_dim)
        lstm_out, (hidden, cell) = self.lstm(embedded)

        # 마지막 은닉 상태 사용
        last_hidden = hidden.squeeze(0)  # (batch, hidden_dim)

        # 분류
        logits = self.fc(last_hidden)  # (batch, num_classes)
        return logits

# 예제: 감성 분석
model = LSTMClassifier(vocab_size=10000, embedding_dim=128,
                       hidden_dim=256, num_classes=2)

# 샘플 입력 (배치 크기 8, 시퀀스 길이 50)
x = torch.randint(0, 10000, (8, 50))
output = model(x)
print(output.shape)  # (8, 2)
```

---

### GRU 구현 (PyTorch 내장)

```python
class GRUSeq2Seq(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.encoder = nn.GRU(input_dim, hidden_dim, batch_first=True)
        self.decoder = nn.GRU(output_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, src, tgt):
        """
        src: (batch, src_len, input_dim)
        tgt: (batch, tgt_len, output_dim)
        """
        # Encoding
        _, hidden = self.encoder(src)  # hidden: (1, batch, hidden_dim)

        # Decoding
        decoder_out, _ = self.decoder(tgt, hidden)

        # 출력
        output = self.fc(decoder_out)  # (batch, tgt_len, output_dim)
        return output
```

---

### Transformer 구현 (간단한 Self-Attention)

```python
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        assert self.head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads"

        self.qkv = nn.Linear(embed_dim, embed_dim * 3)
        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        """
        x: (batch, seq_len, embed_dim)
        """
        batch_size, seq_len, embed_dim = x.shape

        # QKV 계산: (batch, seq_len, 3 * embed_dim)
        qkv = self.qkv(x)

        # reshape: (batch, seq_len, 3, num_heads, head_dim)
        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)

        # permute: (3, batch, num_heads, seq_len, head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)

        q, k, v = qkv[0], qkv[1], qkv[2]

        # Attention scores: (batch, num_heads, seq_len, seq_len)
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)

        # Attention weights
        attn_weights = F.softmax(scores, dim=-1)

        # Weighted sum: (batch, num_heads, seq_len, head_dim)
        attn_output = torch.matmul(attn_weights, v)

        # Concatenate heads: (batch, seq_len, embed_dim)
        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)

        # Output projection
        output = self.out_proj(attn_output)
        return output

# 사용 예제
attn = SelfAttention(embed_dim=512, num_heads=8)
x = torch.randn(4, 20, 512)  # (batch, seq_len, embed_dim)
output = attn(x)
print(output.shape)  # (4, 20, 512)
```

---

### PyTorch Transformer (내장)

```python
class TransformerClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_encoder = PositionalEncoding(embed_dim)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=2048,
            dropout=0.1
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.fc = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        """
        x: (batch, seq_len)
        """
        # 임베딩 + 위치 인코딩
        embedded = self.embedding(x) * (self.embedding.embedding_dim ** 0.5)
        embedded = self.pos_encoder(embedded)

        # Transformer expects (seq_len, batch, embed_dim)
        embedded = embedded.transpose(0, 1)

        # Transformer encoding
        encoded = self.transformer(embedded)  # (seq_len, batch, embed_dim)

        # Global average pooling
        pooled = encoded.mean(dim=0)  # (batch, embed_dim)

        # 분류
        logits = self.fc(pooled)  # (batch, num_classes)
        return logits

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()

        # 위치 인코딩 계산
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-torch.log(torch.tensor(10000.0)) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        x: (batch, seq_len, d_model)
        """
        x = x + self.pe[:, :x.size(1), :]
        return x
```

---

## 시퀀스 모델 비교

| 모델 | 장기 의존성 | 병렬화 | 파라미터 | 추론 속도 | 주요 사용처 |
|------|------------|--------|----------|-----------|------------|
| RNN | 약함 | 불가능 | 적음 | 느림 | 간단한 시계열 |
| LSTM | 강함 | 불가능 | 많음 | 느림 | 음성, 시계열 |
| GRU | 강함 | 불가능 | 중간 | 중간 | LSTM 대체 |
| Transformer | 매우 강함 | 가능 | 매우 많음 | 빠름 (학습), 느림 (긴 시퀀스) | NLP, Vision |

---

## 응용 분야

### 1. 자연어 처리 (NLP)
- **언어 모델**: GPT, BERT
- **기계 번역**: Seq2Seq + Attention, Transformer
- **감성 분석**: LSTM, Transformer
- **질의응답**: BERT, T5

### 2. 시계열 예측
- **주가 예측**: LSTM, GRU
- **날씨 예측**: RNN, Transformer
- **이상 탐지**: Autoencoder + LSTM

### 3. 음성 처리
- **음성 인식**: LSTM + CTC, Transformer (Wav2Vec)
- **음성 합성**: Tacotron (Seq2Seq + Attention)

### 4. 비디오 이해
- **행동 인식**: 3D CNN + LSTM
- **비디오 캡셔닝**: CNN (프레임) + LSTM (캡션)

---

## 학습 및 최적화 팁

### 1. 기울기 폭발 방지
```python
# 기울기 클리핑
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### 2. 시퀀스 패딩
```python
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence

# 가변 길이 시퀀스 처리
sequences = [torch.randn(10, 5), torch.randn(7, 5), torch.randn(15, 5)]
padded = pad_sequence(sequences, batch_first=True)  # (3, 15, 5)

# 패딩 무시하고 RNN 처리
lengths = torch.tensor([10, 7, 15])
packed = pack_padded_sequence(padded, lengths, batch_first=True, enforce_sorted=False)
output, hidden = rnn(packed)
output, _ = pad_packed_sequence(output, batch_first=True)
```

### 3. Teacher Forcing (Seq2Seq)
```python
# 학습 시 이전 출력 대신 정답(target)을 다음 입력으로 사용
for t in range(target_len):
    decoder_output, decoder_hidden = decoder(target[:, t], decoder_hidden)
    # vs. decoder_output, decoder_hidden = decoder(decoder_output, decoder_hidden)
```

### 4. Warmup + Cosine Annealing (Transformer)
```python
from torch.optim.lr_scheduler import CosineAnnealingLR

# Transformer는 warmup이 중요
def get_lr(step, d_model, warmup_steps=4000):
    step = max(step, 1)
    return d_model ** (-0.5) * min(step ** (-0.5), step * warmup_steps ** (-1.5))
```

---

## 연습 문제

### 기초
1. RNN의 은닉 상태 $h_t$가 과거의 모든 입력 정보를 담는 이유를 수식으로 설명하시오.
2. LSTM의 Forget Gate와 Input Gate가 각각 어떤 역할을 하는지 서술하시오.
3. Self-Attention에서 Query, Key, Value의 개념을 직관적으로 설명하시오.

### 중급
4. 간단한 RNN을 PyTorch로 구현하고, MNIST 숫자 인식 문제에 적용하시오. (이미지를 행 단위 시퀀스로 처리)
5. LSTM과 GRU를 감성 분석 태스크에 적용하고 성능과 학습 속도를 비교하시오.
6. Scaled Dot-Product Attention에서 $\sqrt{d_k}$로 나누는 이유를 수식으로 설명하시오.

### 고급
7. Seq2Seq 모델에 Attention을 추가하고, 기계 번역 문제에서 Attention Weight를 시각화하시오.
8. Transformer의 Positional Encoding을 학습 가능한 파라미터로 바꾸어 성능 차이를 비교하시오.
9. Bidirectional LSTM을 구현하고, 양방향성이 Named Entity Recognition (NER) 태스크에 미치는 영향을 분석하시오.

---

## 참고 자료

**논문**
- Hochreiter & Schmidhuber, "Long Short-Term Memory" (1997) - LSTM
- Cho et al., "Learning Phrase Representations using RNN Encoder-Decoder" (2014) - GRU
- Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate" (2014) - Attention
- Vaswani et al., "Attention Is All You Need" (2017) - Transformer
- Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers" (2018)

**서적**
- Goodfellow et al., *Deep Learning*, Chapter 10: Sequence Modeling
- Jurafsky & Martin, *Speech and Language Processing* (3rd ed.)

**온라인 강의**
- Stanford CS224n: Natural Language Processing with Deep Learning
- The Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/
- Andrej Karpathy, "The Unreasonable Effectiveness of Recurrent Neural Networks"

---

## 다음 학습
- [[Foundations/8. 합성곱 신경망 기초]]
- [[Foundations/7. 역전파와 최적화 전략]]
- [[Foundations/6. 활성화 함수와 출력층 설계]]
