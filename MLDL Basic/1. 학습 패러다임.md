## TL;DR
- 머신러닝 패러다임은 데이터가 어떻게 주어지고 어떤 피드백을 받는지에 따라 구분된다.
- 지도, 비지도, 자기지도, 반지도, 강화 학습 각각의 목적과 주요 알고리즘, 실제 사례를 이해하면 문제 정의 단계에서 올바른 접근을 선택할 수 있다.
- 데이터 수집 비용, 라벨 품질, 환경 상호작용 가능 여부 등을 고려해 패러다임을 결정한다.

## 주요 패러다임 정리

### 지도 학습 (Supervised Learning)

입력-정답 쌍 $(x, y)$를 사용하여 함수 $f: x \to y$를 학습한다.

- $x$: 입력 데이터 (features)
- $y$: 정답 레이블 (target)
- $f$: 학습하려는 예측 함수

**과제**: 분류, 회귀, 시계열 예측, 구조적 예측(시퀀스 라벨링 등)

**대표 모델**: Logistic Regression, SVM, Random Forest, CNN, Transformer

**응용**: 이미지 분류, 스팸 필터링, 수요 예측

---

### 비지도 학습 (Unsupervised Learning)

라벨 없이 데이터 $\{x_i\}_{i=1}^N$의 구조를 파악한다. 패턴 발견, 군집화, 차원 축소를 수행한다.

- $x_i$: 레이블이 없는 입력 데이터
- $N$: 전체 데이터 개수

**대표 기법**: K-means, GMM, PCA, Autoencoder, Density Estimation

**응용**: 고객 세분화, 이상 탐지, 데이터 시각화

---

### 자기지도 학습 (Self-supervised Learning)

라벨 없이도 프리텍스트 태스크(pretext task)를 정의해 표현 벡터를 학습한다.

데이터 자체에서 자동으로 생성한 레이블을 사용:
$$
\hat{y} = g(x)
$$

- $g$: 프리텍스트 태스크 함수 (예: 마스킹, 회전 예측)
- $\hat{y}$: 자동 생성된 레이블

**예시**: 마스크 언어 모델(BERT), 이미지 색 복원, contrastive learning(SimCLR)

라벨이 적더라도 사전학습 후 파인튜닝으로 좋은 성능 달성

---

### 반지도 학습 (Semi-supervised Learning)

소량의 라벨 데이터 $\{(x_i, y_i)\}_{i=1}^{N_L}$과 대량의 비라벨 데이터 $\{x_j\}_{j=1}^{N_U}$를 동시에 활용한다.

일반적으로 $N_U \gg N_L$ (비라벨 데이터가 훨씬 많음)

**대표 기법**: Consistency regularization, pseudo-labeling, graph-based methods

**응용**: 의료 영상, 라벨링 비용이 큰 도메인

---

### 강화 학습 (Reinforcement Learning)

에이전트가 환경과 상호작용하며 누적 보상을 최대화하는 정책을 학습한다.

$$
\max_{\pi} \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R_t \mid \pi\right]
$$

- $\pi$: 정책 (policy) - 상태에서 행동을 선택하는 함수
- $R_t$: 시간 $t$에서의 보상 (reward)
- $\gamma \in [0, 1]$: 할인율 (discount factor)
- $S$: 상태 공간 (state space)
- $A$: 행동 공간 (action space)
- $V$: 가치 함수 (value function)

**대표 알고리즘**: Q-learning, Policy Gradient, Actor-Critic, PPO

**응용**: 게임, 로보틱스, 추천 시스템

---

### 온라인/배치 학습

**온라인 학습 (Online Learning)**: 데이터 스트림을 순차적으로 처리, 모델이 지속적으로 업데이트

**배치 학습 (Batch Learning)**: 고정된 데이터셋을 한 번에 학습, 모델 업데이트 주기가 길다

## 선택을 위한 체크리스트
- 라벨 수집 비용과 가능 여부는?
- 데이터가 정적/스트리밍/상호작용 환경 중 어느 형태인가?
- 예측해야 할 값이 범주형인지, 연속값인지, 시퀀스인지?
- 모델이 실시간으로 환경과 상호작용해 피드백을 받을 수 있는가?
- 측정 가능한 보상/평가 함수가 존재하는가?

## 실습 아이디어
- 동일한 데이터셋(예: Iris)을 사용해 지도 학습(SVM) vs 비지도 학습(K-means) 결과 비교.
- 자기지도 프리텍스트: 이미지 패치 shuffle 복원, 문장 일부 마스킹 예측 실험.
- Semi-supervised 실험: 라벨 10%만 사용하고 pseudo-labeling으로 정확도 향상 테스트.
- OpenAI Gym CartPole 환경에서 Q-learning으로 정책 학습.

## 실제 사례 매핑
- 지도: “이 고객이 이탈할 가능성은?” → 이진 분류
- 비지도: “고객을 성향에 따라 그룹화?” → 클러스터링
- 자기지도: “라벨 없이 대규모 텍스트/이미지에서 표현 벡터를 학습” → 사전학습
- 반지도: “라벨 없는 데이터가 10배 많은 상황” → consistency regularization
- 강화: “에이전트가 광고 노출 순서를 결정” → RL 정책 학습