## TL;DR
- 특이값 분해(SVD, Singular Value Decomposition)는 모든 실수 행렬을 $A = U\Sigma V^\top$ 형태로 분해하는 강력한 도구이다.
- 고유값 분해와 달리 정사각 행렬이 아니어도 적용 가능하며, 행렬의 본질적 구조(랭크, 주요 방향)를 드러낸다.
- 차원 축소(PCA), 이미지 압축, 추천 시스템(matrix factorization), 노이즈 제거, 역행렬 근사(pseudo-inverse) 등 딥러닝과 데이터 과학 전반에서 핵심 기법이다.

---

## 1. SVD의 정의

### 1.1 기본 정리

**정리 (Singular Value Decomposition)**: 모든 $m \times n$ 실수 행렬 $A$에 대해 다음이 성립한다:
$$
A = U \Sigma V^\top
$$
여기서:
- $U \in \mathbb{R}^{m \times m}$: 직교 행렬 ($U^\top U = I$), **좌특이벡터(left singular vectors)**
- $\Sigma \in \mathbb{R}^{m \times n}$: 대각 행렬 (비음수 성분), **특이값(singular values)**
- $V \in \mathbb{R}^{n \times n}$: 직교 행렬 ($V^\top V = I$), **우특이벡터(right singular vectors)**

**특이값 순서**: $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$ (단, $r = \operatorname{rank}(A)$)

### 1.2 형상 이해

**직사각 행렬의 경우** ($m \neq n$):
$$
\Sigma = \begin{bmatrix}
\sigma_1 & 0 & \cdots & 0 \\
0 & \sigma_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_{\min(m,n)} \\
0 & 0 & \cdots & 0 \\
\vdots & \vdots & & \vdots
\end{bmatrix}
$$

실제로는 0이 아닌 $r$개 특이값만 유의미.

### 1.3 기하학적 해석

행렬 $A$에 의한 선형 변환은 3단계로 분해된다:
1. $V^\top$: 입력 공간의 회전/반사
2. $\Sigma$: 각 축 방향으로 스케일링 ($\sigma_i$배)
3. $U$: 출력 공간의 회전/반사

**핵심**: 모든 선형 변환은 "회전 → 늘리기 → 회전"으로 분해 가능!

---

## 2. SVD 유도와 구성

### 2.1 고유값 분해와의 관계

$A^\top A$와 $AA^\top$는 대칭 행렬이므로 직교 대각화 가능:
$$
A^\top A = V D_V V^\top, \quad AA^\top = U D_U U^\top
$$

**핵심 관찰**:
$$
A^\top A = (U\Sigma V^\top)^\top (U\Sigma V^\top) = V\Sigma^\top U^\top U\Sigma V^\top = V(\Sigma^\top \Sigma)V^\top
$$

따라서:
- $V$는 $A^\top A$의 고유벡터
- $\sigma_i^2$는 $A^\top A$의 고유값
- $U$는 $AA^\top$의 고유벡터

**우특이벡터 구하기**:
$$
A^\top A \mathbf{v}_i = \sigma_i^2 \mathbf{v}_i
$$

**좌특이벡터 구하기**:
$$
\mathbf{u}_i = \frac{1}{\sigma_i} A\mathbf{v}_i
$$

### 2.2 손으로 계산하는 예제

$$
A = \begin{bmatrix} 3 & 1 \\ 1 & 3 \\ 1 & 1 \end{bmatrix}
$$

**Step 1**: $A^\top A$ 계산
$$
A^\top A = \begin{bmatrix} 3 & 1 & 1 \\ 1 & 3 & 1 \end{bmatrix} \begin{bmatrix} 3 & 1 \\ 1 & 3 \\ 1 & 1 \end{bmatrix} = \begin{bmatrix} 11 & 7 \\ 7 & 11 \end{bmatrix}
$$

**Step 2**: 고유값
$$
\det(A^\top A - \lambda I) = (11-\lambda)^2 - 49 = \lambda^2 - 22\lambda + 72 = 0
$$
$$
\lambda_1 = 18, \quad \lambda_2 = 4
$$

특이값: $\sigma_1 = \sqrt{18} = 3\sqrt{2}$, $\sigma_2 = 2$

**Step 3**: $A^\top A$의 고유벡터 (우특이벡터)
$$
\mathbf{v}_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad
\mathbf{v}_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}
$$

**Step 4**: 좌특이벡터
$$
\mathbf{u}_1 = \frac{1}{\sigma_1}A\mathbf{v}_1 = \frac{1}{3\sqrt{2}}\begin{bmatrix} 4 \\ 4 \\ 2 \end{bmatrix} = \frac{1}{\sqrt{6}}\begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}
$$
$$
\mathbf{u}_2 = \frac{1}{\sigma_2}A\mathbf{v}_2 = \frac{1}{2}\begin{bmatrix} 2 \\ -2 \\ 0 \end{bmatrix} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix}
$$

$\mathbf{u}_3$는 $\mathbf{u}_1, \mathbf{u}_2$에 직교하도록 선택 (Gram-Schmidt).

---

## 3. Reduced SVD와 Compact SVD

### 3.1 Full SVD

$$
A_{m \times n} = U_{m \times m} \Sigma_{m \times n} V_{n \times n}^\top
$$

모든 좌특이벡터와 우특이벡터를 포함하지만, 많은 부분이 0.

### 3.2 Reduced (Thin) SVD

$\min(m, n) = r$인 경우:
$$
A = U_r \Sigma_r V_r^\top
$$
- $U_r \in \mathbb{R}^{m \times r}$: 처음 $r$개 좌특이벡터
- $\Sigma_r \in \mathbb{R}^{r \times r}$: 대각 행렬
- $V_r \in \mathbb{R}^{n \times r}$: 처음 $r$개 우특이벡터

**장점**: 메모리 효율적, 계산 빠름.

### 3.3 Truncated SVD (Rank-k 근사)

상위 $k < r$개 특이값만 사용:
$$
A \approx A_k = \sum_{i=1}^k \sigma_i \mathbf{u}_i \mathbf{v}_i^\top = U_k \Sigma_k V_k^\top
$$

**Eckart-Young 정리**: $A_k$는 랭크가 $k$ 이하인 행렬 중 $A$에 가장 가까운 행렬이다 (Frobenius norm 또는 spectral norm 기준).

$$
\|A - A_k\|_F = \sqrt{\sigma_{k+1}^2 + \cdots + \sigma_r^2}
$$

---

## 4. SVD의 주요 응용

### 4.1 랭크와 영공간

**랭크**:
$$
\operatorname{rank}(A) = \#\{\sigma_i > 0\}
$$

**Four Fundamental Subspaces**:
1. **열공간(Column space)**: $\operatorname{col}(A) = \operatorname{span}\{\mathbf{u}_1, \ldots, \mathbf{u}_r\}$
2. **행공간(Row space)**: $\operatorname{row}(A) = \operatorname{span}\{\mathbf{v}_1, \ldots, \mathbf{v}_r\}$
3. **영공간(Null space)**: $\ker(A) = \operatorname{span}\{\mathbf{v}_{r+1}, \ldots, \mathbf{v}_n\}$
4. **좌영공간(Left null space)**: $\ker(A^\top) = \operatorname{span}\{\mathbf{u}_{r+1}, \ldots, \mathbf{u}_m\}$

### 4.2 Pseudo-Inverse (Moore-Penrose Inverse)

정사각이 아니거나 특이(singular)한 행렬의 "역행렬" 개념:
$$
A^+ = V \Sigma^+ U^\top
$$
여기서 $\Sigma^+ = \operatorname{diag}(1/\sigma_1, \ldots, 1/\sigma_r, 0, \ldots, 0)$

**성질**:
- $AA^+A = A$
- $A^+AA^+ = A^+$
- 최소제곱 해: $\mathbf{x} = A^+ \mathbf{b}$는 $\|A\mathbf{x} - \mathbf{b}\|_2$를 최소화

### 4.3 이미지 압축

이미지를 $m \times n$ 행렬 $A$로 간주하고 SVD 적용:
$$
A_k = \sum_{i=1}^k \sigma_i \mathbf{u}_i \mathbf{v}_i^\top
$$

**저장 공간**:
- 원본: $mn$
- SVD: $k(m + n + 1)$
- 압축률: $\frac{k(m+n+1)}{mn}$

$k \ll \min(m, n)$이면 큰 압축률 달성.

**예제**: 100×100 이미지, $k=10$
- 원본: 10,000 값
- SVD: 10(100+100+1) = 2,010 값
- 압축률: 20%

### 4.4 추천 시스템 (Matrix Factorization)

사용자-아이템 행렬 $R \in \mathbb{R}^{m \times n}$ (m명, n개 아이템):
$$
R \approx U_k \Sigma_k V_k^\top
$$

- $U_k$: 사용자 잠재 특징 (latent features)
- $V_k$: 아이템 잠재 특징
- 누락된 평점 예측: $\hat{r}_{ij} = (\mathbf{u}_i)^\top \Sigma_k \mathbf{v}_j$

Netflix Prize에서 사용된 핵심 기법.

### 4.5 PCA와의 관계

데이터 행렬 $X \in \mathbb{R}^{n \times d}$ (중심화됨)의 SVD:
$$
X = U\Sigma V^\top
$$

공분산 행렬:
$$
C = \frac{1}{n}X^\top X = \frac{1}{n}V\Sigma^\top U^\top U\Sigma V^\top = V\left(\frac{\Sigma^2}{n}\right)V^\top
$$

따라서:
- $V$의 열: 주성분 방향
- $\sigma_i^2 / n$: 각 주성분의 분산

**차원 축소**:
$$
X_{\text{reduced}} = X V_k = U_k \Sigma_k
$$

---

## 5. 조건수와 수치 안정성

### 5.1 조건수

$$
\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}} = \frac{\sigma_1}{\sigma_r}
$$

**의미**:
- $\kappa \approx 1$: well-conditioned (수치적으로 안정)
- $\kappa \gg 1$: ill-conditioned (작은 오차가 큰 영향)

**예제**: Hessian의 조건수가 크면 gradient descent가 느림 → preconditioner 필요.

### 5.2 Frobenius Norm과 Spectral Norm

**Frobenius norm**:
$$
\|A\|_F = \sqrt{\sum_{i,j} a_{ij}^2} = \sqrt{\sigma_1^2 + \cdots + \sigma_r^2}
$$

**Spectral norm (2-norm)**:
$$
\|A\|_2 = \sigma_{\max} = \sigma_1
$$

**Low-rank approximation error**:
$$
\min_{\operatorname{rank}(B) \leq k} \|A - B\|_2 = \sigma_{k+1}
$$

---

## 6. PyTorch 실습

```python
import torch
import matplotlib.pyplot as plt

# 1. 기본 SVD
A = torch.tensor([
    [3., 1.],
    [1., 3.],
    [1., 1.]
], dtype=torch.float64)

U, S, Vh = torch.linalg.svd(A, full_matrices=True)
V = Vh.T

print("U shape:", U.shape)  # (3, 3)
print("S:", S)               # (2,)
print("V shape:", V.shape)   # (2, 2)

# 재구성
Sigma_full = torch.zeros(3, 2)
Sigma_full[:2, :2] = torch.diag(S)
A_reconstructed = U @ Sigma_full @ V.T
print("재구성 오차:", torch.norm(A - A_reconstructed).item())

# 2. Reduced SVD
U_thin, S_thin, Vh_thin = torch.linalg.svd(A, full_matrices=False)
print("Reduced U shape:", U_thin.shape)  # (3, 2)

# 3. Rank-k 근사
k = 1
A_k = U_thin[:, :k] @ torch.diag(S_thin[:k]) @ Vh_thin[:k, :]
print(f"Rank-{k} 근사:\n", A_k)
print(f"근사 오차: {torch.norm(A - A_k).item():.4f}")
print(f"이론적 오차: {S_thin[k].item():.4f}")

# 4. 이미지 압축 예제
from PIL import Image
import numpy as np

# 흑백 이미지 생성 (또는 로드)
img = torch.randn(100, 100)  # 또는 실제 이미지

U, S, Vh = torch.linalg.svd(img, full_matrices=False)

# 다양한 k 값으로 압축
for k in [5, 10, 20, 50]:
    img_k = U[:, :k] @ torch.diag(S[:k]) @ Vh[:k, :]
    compression_ratio = k * (img.shape[0] + img.shape[1] + 1) / img.numel()
    error = torch.norm(img - img_k).item() / torch.norm(img).item()
    print(f"k={k}: 압축률={compression_ratio:.2%}, 상대오차={error:.4f}")

# 5. Pseudo-inverse
A = torch.tensor([[1., 2.], [3., 4.], [5., 6.]])
A_pinv = torch.linalg.pinv(A)
print("Pseudo-inverse:\n", A_pinv)

# 최소제곱 해
b = torch.tensor([1., 2., 3.])
x = A_pinv @ b
print("최소제곱 해:", x)
print("잔차:", torch.norm(A @ x - b).item())

# 6. 조건수
A_square = torch.tensor([[1., 2.], [2., 4.001]])
U, S, Vh = torch.linalg.svd(A_square)
cond = (S.max() / S.min()).item()
print(f"조건수: {cond:.2e}")
```

---

## 7. 연습 문제

1. **SVD 손계산**
   $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}$의 SVD를 손으로 계산하라. $A^\top A$의 고유값과 고유벡터를 먼저 구하라.

2. **Rank-k 근사**
   $5 \times 5$ 랜덤 행렬을 생성하고, $k=1, 2, 3$에 대한 근사 오차를 계산해 Eckart-Young 정리를 검증하라.

3. **이미지 압축**
   실제 흑백 이미지를 로드하고, 상위 10%, 25%, 50% 특이값만 사용해 재구성한 이미지를 시각화하라.

4. **Pseudo-inverse 응용**
   과결정 시스템(overdetermined) $A\mathbf{x} = \mathbf{b}$ (방정식이 미지수보다 많음)를 생성하고, pseudo-inverse로 최소제곱 해를 구하라.

5. **조건수 실험**
   $A = \begin{bmatrix} 1 & 1 \\ 1 & 1+\epsilon \end{bmatrix}$에서 $\epsilon$을 $10^{-2}, 10^{-4}, 10^{-6}$으로 바꿔가며 조건수와 역행렬의 수치 오차를 관찰하라.

6. **PCA vs SVD**
   다변량 정규분포에서 샘플링한 데이터에 대해, (a) 공분산 행렬 고유분해, (b) 데이터 행렬 SVD 두 방법으로 주성분을 구하고 결과가 일치함을 확인하라.
