## TL;DR
- ì„ í˜•ëŒ€ìˆ˜ëŠ” **ë²¡í„° ê³µê°„ê³¼ ì„ í˜• ë³€í™˜**ì„ ë‹¤ë£¨ëŠ” í•™ë¬¸ìœ¼ë¡œ, ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬, ì–´í…ì…˜ ì ìˆ˜ ê³„ì‚°, ì°¨ì› ì¶•ì†Œ(PCA), SVD ê¸°ë°˜ ì••ì¶• ë“± ê±°ì˜ ëª¨ë“  ë”¥ëŸ¬ë‹ ê¸°ë²•ì˜ ìˆ˜í•™ì  í† ëŒ€ë¥¼ ì œê³µí•œë‹¤.
- ì´ ë…¸íŠ¸ëŠ” ì„ í˜•ëŒ€ìˆ˜ ì „ì²´ ë‚´ìš©ì˜ í—ˆë¸Œë¡œ, ê° ì£¼ì œë³„ ìƒì„¸ ë…¸íŠ¸ë¡œ ì—°ê²°í•˜ëŠ” ë¡œë“œë§µ ì—­í• ì„ í•œë‹¤.
- ë²¡í„° ê³µê°„ â†’ í–‰ë ¬ ì—°ì‚° â†’ ê³ ìœ ê°’/ê³ ìœ ë²¡í„° â†’ SVD ìˆœì„œë¡œ í•™ìŠµí•˜ë©´ ì²´ê³„ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.

---

## í•™ìŠµ ë¡œë“œë§µ

| ìˆœì„œ | ì£¼ì œ | í•µì‹¬ ì§ˆë¬¸ | ë°”ë¡œ ê°€ê¸° |
|------|------|-----------|-----------|
| 1 | ë²¡í„° ê³µê°„ê³¼ ê¸°ì € | ë²¡í„° ê³µê°„ì´ë€ ë¬´ì—‡ì´ë©°, ê¸°ì €ëŠ” ì™œ ì¤‘ìš”í•œê°€? | [[ë²¡í„° ê³µê°„ê³¼ ê¸°ì €]] |
| 2 | í–‰ë ¬ê³¼ ì„ í˜• ë³€í™˜ | í–‰ë ¬ ê³±ì…ˆì˜ ê¸°í•˜í•™ì  ì˜ë¯¸ëŠ”? ì—­í–‰ë ¬ì€ ì–¸ì œ ì¡´ì¬í•˜ëŠ”ê°€? | [[í–‰ë ¬ê³¼ ì„ í˜• ë³€í™˜]] |
| 3 | ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„° | ë³€í™˜ í›„ì—ë„ ë°©í–¥ì´ ìœ ì§€ë˜ëŠ” ë²¡í„°ëŠ”? ëŒ€ê°í™”ëŠ” ì™œ ìœ ìš©í•œê°€? | [[ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°]] |
| 4 | íŠ¹ì´ê°’ ë¶„í•´ | ëª¨ë“  í–‰ë ¬ì„ ì–´ë–»ê²Œ ë¶„í•´í•˜ëŠ”ê°€? SVDì˜ ì‘ìš©ì€? | [[íŠ¹ì‡ê°’ ë¶„í•´]] |

---

## 1. ë²¡í„° ê³µê°„ê³¼ ê¸°ì €

### í•µì‹¬ ê°œë…
- **ë²¡í„° ê³µê°„**: ë²¡í„° ë§ì…ˆê³¼ ìŠ¤ì¹¼ë¼ ê³±ì´ 8ê°€ì§€ ê³µë¦¬ë¥¼ ë§Œì¡±í•˜ëŠ” ëŒ€ìˆ˜ êµ¬ì¡°
- **ì„ í˜• ê²°í•©**: ë²¡í„°ë“¤ì˜ ê°€ì¤‘í•© $c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k$
- **ì„ í˜• ë…ë¦½**: ì–´ë–¤ ë²¡í„°ë„ ë‹¤ë¥¸ ë²¡í„°ë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„ ë¶ˆê°€
- **ê¸°ì €**: ê³µê°„ì„ ìƒì„±í•˜ë©´ì„œ ì„ í˜• ë…ë¦½ì¸ ë²¡í„°ë“¤ì˜ ì§‘í•©
- **ì°¨ì›**: ê¸°ì € ë²¡í„°ì˜ ê°œìˆ˜

### ì™œ ì¤‘ìš”í•œê°€?
- ë”¥ëŸ¬ë‹ì˜ íŠ¹ì§• ë²¡í„°, ì„ë² ë”©, íŒŒë¼ë¯¸í„° ê³µê°„ ëª¨ë‘ ë²¡í„° ê³µê°„
- ê¸°ì € ë³€í™˜ì€ ì¢Œí‘œê³„ ë³€ê²½ (RGB â†” YUV, PCA ë³€í™˜ ë“±)
- ì°¨ì› ê°œë…ì€ ëª¨ë¸ ë³µì¡ë„ì™€ ì§ê²°

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[ë²¡í„° ê³µê°„ê³¼ ê¸°ì €|ë²¡í„° ê³µê°„ê³¼ ê¸°ì € ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- ë²¡í„° ê³µê°„ì˜ ëª¨ë“  ê¸°ì €ëŠ” ê°™ì€ ê°œìˆ˜ì˜ ë²¡í„°ë¥¼ ê°–ëŠ”ë‹¤
- Rank-Nullity ì •ë¦¬: $\operatorname{rank}(A) + \operatorname{nullity}(A) = n$

---

## 2. í–‰ë ¬ê³¼ ì„ í˜• ë³€í™˜

### í•µì‹¬ ê°œë…
- **ì„ í˜• ë³€í™˜**: $T(c_1\mathbf{v}_1 + c_2\mathbf{v}_2) = c_1 T(\mathbf{v}_1) + c_2 T(\mathbf{v}_2)$
- **í–‰ë ¬ í‘œí˜„**: ëª¨ë“  ì„ í˜• ë³€í™˜ì€ í–‰ë ¬ë¡œ í‘œí˜„ ê°€ëŠ¥
- **í–‰ë ¬ ê³±ì…ˆ**: ë³€í™˜ì˜ í•©ì„±
- **ì—­í–‰ë ¬**: ë³€í™˜ì„ ë˜ëŒë¦¬ëŠ” ì—°ì‚°
- **í–‰ë ¬ì‹**: ë¶€í”¼ ë³€í™˜ ê³„ìˆ˜, ê°€ì—­ì„± íŒë³„

### ì™œ ì¤‘ìš”í•œê°€?
- ì‹ ê²½ë§ì˜ ì™„ì „ì—°ê²°ì¸µ $y = Wx + b$ì—ì„œ $W$ê°€ ì„ í˜• ë³€í™˜
- í–‰ë ¬ ê³±ì…ˆ ì—°ì‚°ì´ ë”¥ëŸ¬ë‹ ê³„ì‚°ì˜ ëŒ€ë¶€ë¶„
- ì§êµ í–‰ë ¬ì€ BatchNorm, Spectral Normalizationì—ì„œ í™œìš©

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[í–‰ë ¬ê³¼ ì„ í˜• ë³€í™˜|í–‰ë ¬ê³¼ ì„ í˜• ë³€í™˜ ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- $\det(AB) = \det(A)\det(B)$
- $A$ê°€ ê°€ì—­ $\Leftrightarrow$ $\det(A) \neq 0$ $\Leftrightarrow$ $\operatorname{rank}(A) = n$
- ì§êµ í–‰ë ¬: $Q^\top Q = I$, ê¸¸ì´ì™€ ê°ë„ ë³´ì¡´

---

## 3. ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°

### í•µì‹¬ ê°œë…
- **ê³ ìœ ë²¡í„°**: $A\mathbf{v} = \lambda\mathbf{v}$ (ë³€í™˜ í›„ ë°©í–¥ ìœ ì§€)
- **ê³ ìœ ê°’**: ê³ ìœ ë²¡í„°ì˜ ìŠ¤ì¼€ì¼ ë°°ìœ¨
- **íŠ¹ì„± ë°©ì •ì‹**: $\det(A - \lambda I) = 0$
- **ëŒ€ê°í™”**: $A = PDP^{-1}$ (ë‹¨, $P$ëŠ” ê³ ìœ ë²¡í„° í–‰ë ¬)
- **ìŠ¤í™íŠ¸ëŸ´ ì •ë¦¬**: ëŒ€ì¹­ í–‰ë ¬ì€ ì§êµ ëŒ€ê°í™” ê°€ëŠ¥

### ì™œ ì¤‘ìš”í•œê°€?
- PCAëŠ” ê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ ë²¡í„°ë¥¼ ì£¼ì„±ë¶„ìœ¼ë¡œ ì‚¬ìš©
- Hessian ê³ ìœ ê°’ìœ¼ë¡œ ê·¹ì†Œ/ê·¹ëŒ€/ì•ˆì¥ì  íŒë³„
- ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆì˜ ê³ ìœ ê°’ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ (Spectral clustering)
- ì‹œìŠ¤í…œ ì•ˆì •ì„± ë¶„ì„ (RNNì˜ gradient explosion)

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°|ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„° ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- ëŒ€ì¹­ í–‰ë ¬ì˜ ê³ ìœ ê°’ì€ ëª¨ë‘ ì‹¤ìˆ˜
- $n \times n$ í–‰ë ¬ì´ ëŒ€ê°í™” ê°€ëŠ¥ $\Leftrightarrow$ $n$ê°œì˜ ì„ í˜• ë…ë¦½ ê³ ìœ ë²¡í„° ì¡´ì¬
- ì¡°ê±´ìˆ˜ $\kappa(H) = \lambda_{\max}/\lambda_{\min}$ì´ í¬ë©´ ìµœì í™” ì–´ë ¤ì›€

---

## 4. íŠ¹ì´ê°’ ë¶„í•´ (SVD)

### í•µì‹¬ ê°œë…
- **SVD**: $A = U\Sigma V^\top$ (ëª¨ë“  í–‰ë ¬ì— ì ìš© ê°€ëŠ¥)
- **íŠ¹ì´ê°’**: $\Sigma$ì˜ ëŒ€ê° ì„±ë¶„ $\sigma_1 \geq \sigma_2 \geq \cdots$
- **Rank-k ê·¼ì‚¬**: ìƒìœ„ $k$ê°œ íŠ¹ì´ê°’ë§Œ ì‚¬ìš©
- **Pseudo-inverse**: $A^+ = V\Sigma^+ U^\top$
- **Eckart-Young ì •ë¦¬**: $A_k$ëŠ” ìµœì  ì €ë­í¬ ê·¼ì‚¬

### ì™œ ì¤‘ìš”í•œê°€?
- PCAì˜ ìˆ˜í•™ì  í† ëŒ€ (ë°ì´í„° í–‰ë ¬ì˜ SVD = ì£¼ì„±ë¶„)
- ì´ë¯¸ì§€ ì••ì¶•, ë…¸ì´ì¦ˆ ì œê±°
- ì¶”ì²œ ì‹œìŠ¤í…œì˜ matrix factorization (Netflix Prize)
- ì¡°ê±´ìˆ˜ ê³„ì‚° ($\kappa = \sigma_{\max}/\sigma_{\min}$)
- Over/underdetermined ì‹œìŠ¤í…œì˜ ìµœì†Œì œê³± í•´

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[íŠ¹ì‡ê°’ ë¶„í•´|íŠ¹ì´ê°’ ë¶„í•´ ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- $A^\top A$ì˜ ê³ ìœ ë²¡í„° = $V$ (ìš°íŠ¹ì´ë²¡í„°)
- $A^\top A$ì˜ ê³ ìœ ê°’ = $\sigma_i^2$
- Rank-k ê·¼ì‚¬ ì˜¤ì°¨: $\|A - A_k\|_F = \sqrt{\sigma_{k+1}^2 + \cdots + \sigma_r^2}$

---

## ì£¼ì œë³„ ì—°ê²°

### ê°œë… ê°„ ê´€ê³„ë„

```
ë²¡í„° ê³µê°„ â”€â”€â”€â”€> ì„ í˜• ë³€í™˜ â”€â”€â”€â”€> í–‰ë ¬ í‘œí˜„
    â”‚                           â”‚
    â”‚                           â–¼
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> ê¸°ì € ë³€í™˜ í–‰ë ¬
                                â”‚
                                â–¼
        ê³ ìœ ê°’ ë¶„í•´ â—„â”€â”€â”€â”€ ëŒ€ì¹­ í–‰ë ¬ (A = A^T)
            â”‚                   â”‚
            â–¼                   â–¼
        ëŒ€ê°í™”            ìŠ¤í™íŠ¸ëŸ´ ì •ë¦¬
            â”‚                   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
              íŠ¹ì´ê°’ ë¶„í•´ (SVD)
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼           â–¼           â–¼
       PCA    ì´ë¯¸ì§€ ì••ì¶•    ì¶”ì²œ ì‹œìŠ¤í…œ
```

### í•µì‹¬ ë¶„í•´ ë¹„êµ

| ë¶„í•´ ë°©ë²• | í˜•íƒœ | ì¡°ê±´ | ì£¼ìš” ì‘ìš© |
|----------|------|------|-----------|
| **ê³ ìœ ê°’ ë¶„í•´** | $A = PDP^{-1}$ | ì •ì‚¬ê°, ëŒ€ê°í™” ê°€ëŠ¥ | ê±°ë“­ì œê³±, ë¯¸ë¶„ë°©ì •ì‹ |
| **ìŠ¤í™íŠ¸ëŸ´ ë¶„í•´** | $A = QDQ^\top$ | ëŒ€ì¹­ í–‰ë ¬ | PCA, Hessian ë¶„ì„ |
| **SVD** | $A = U\Sigma V^\top$ | ëª¨ë“  í–‰ë ¬ | ì°¨ì› ì¶•ì†Œ, ì••ì¶•, pseudo-inverse |
| **LU ë¶„í•´** | $A = LU$ | ì •ì‚¬ê° (í”¼ë²— í•„ìš”) | ì„ í˜• ì‹œìŠ¤í…œ |
| **QR ë¶„í•´** | $A = QR$ | ëª¨ë“  í–‰ë ¬ | ìµœì†Œì œê³±, Gram-Schmidt |

---

## ë¹ ë¥¸ ì°¸ì¡°

### ìì£¼ ì“°ëŠ” ê³µì‹

**í–‰ë ¬ ì—°ì‚°**:
- $(AB)^\top = B^\top A^\top$
- $(AB)^{-1} = B^{-1}A^{-1}$
- $\det(AB) = \det(A)\det(B)$
- $\operatorname{trace}(AB) = \operatorname{trace}(BA)$

**ì§êµ í–‰ë ¬**:
- $Q^{-1} = Q^\top$
- $\det(Q) = \pm 1$
- $\|Q\mathbf{x}\| = \|\mathbf{x}\|$

**ëŒ€ì¹­ í–‰ë ¬**:
- ëª¨ë“  ê³ ìœ ê°’ì´ ì‹¤ìˆ˜
- ì§êµ ëŒ€ê°í™” ê°€ëŠ¥
- $A = QDQ^\top$

**SVD ê´€ê³„**:
- $A^\top A = V\Sigma^2 V^\top$ (ìš°íŠ¹ì´ë²¡í„° = $A^\top A$ì˜ ê³ ìœ ë²¡í„°)
- $AA^\top = U\Sigma^2 U^\top$ (ì¢ŒíŠ¹ì´ë²¡í„° = $AA^\top$ì˜ ê³ ìœ ë²¡í„°)
- $\operatorname{rank}(A) = \#\{\sigma_i > 0\}$

---

## ì‹¤ìŠµ ê°€ì´ë“œ

### PyTorch ì£¼ìš” í•¨ìˆ˜

```python
import torch

# ë²¡í„°/í–‰ë ¬ ìƒì„±
A = torch.randn(3, 3)
I = torch.eye(3)

# ê¸°ë³¸ ì—°ì‚°
A_T = A.T                           # ì „ì¹˜
A_inv = torch.linalg.inv(A)         # ì—­í–‰ë ¬
det_A = torch.linalg.det(A)         # í–‰ë ¬ì‹
rank_A = torch.linalg.matrix_rank(A) # ë­í¬

# ê³ ìœ ê°’ ë¶„í•´
eigenvalues, eigenvectors = torch.linalg.eig(A)
eigenvalues, eigenvectors = torch.linalg.eigh(A)  # ëŒ€ì¹­ìš©

# SVD
U, S, Vh = torch.linalg.svd(A)
A_pinv = torch.linalg.pinv(A)      # Pseudo-inverse

# ì„ í˜• ì‹œìŠ¤í…œ
x = torch.linalg.solve(A, b)       # Ax = b
x = torch.linalg.lstsq(A, b)       # ìµœì†Œì œê³± í•´
```

### í•™ìŠµ ì „ëµ

1. **ì´ë¡  â†’ ì˜ˆì œ â†’ ì½”ë“œ ìˆœì„œë¡œ**
   - ê° ë…¸íŠ¸ì˜ ì •ì˜ì™€ ì •ë¦¬ ì´í•´
   - ì†ìœ¼ë¡œ ì‘ì€ ì˜ˆì œ ê³„ì‚°
   - PyTorchë¡œ ê²€ì¦

2. **ì‹œê°í™” í™œìš©**
   - 2D ë²¡í„° ë³€í™˜ì„ ê·¸ë ¤ë³´ê¸°
   - ê³ ìœ ë²¡í„° ë°©í–¥ í‘œì‹œ
   - PCA ì£¼ì„±ë¶„ ì‹œê°í™”

3. **ì—°ìŠµ ë¬¸ì œ í•„ìˆ˜**
   - ê° ë…¸íŠ¸ì˜ ì—°ìŠµ ë¬¸ì œ í’€ì´
   - ì¦ëª…ì„ ë”°ë¼ê°€ë©° ì´í•´ ì‹¬í™”

---

## ë”¥ëŸ¬ë‹ ì—°ê²° í¬ì¸íŠ¸

### ì–´ë””ì— ì“°ì´ë‚˜?

| ë”¥ëŸ¬ë‹ ê¸°ë²• | ì„ í˜•ëŒ€ìˆ˜ ê°œë… | ê´€ë ¨ ë…¸íŠ¸ |
|-----------|--------------|----------|
| ì™„ì „ì—°ê²°ì¸µ | í–‰ë ¬ ê³±ì…ˆ, ì„ í˜• ë³€í™˜ | [[í–‰ë ¬ê³¼ ì„ í˜• ë³€í™˜]] |
| Attention | ë‚´ì , í–‰ë ¬ ê³± | [[ë²¡í„° ê³µê°„ê³¼ ê¸°ì €]] |
| BatchNorm | ì •ê·œì§êµ ê¸°ì € | [[í–‰ë ¬ê³¼ ì„ í˜• ë³€í™˜]] |
| PCA, Autoencoder | SVD, ê³ ìœ ê°’ ë¶„í•´ | [[íŠ¹ì‡ê°’ ë¶„í•´]] |
| Gradient Descent | Hessian ê³ ìœ ê°’ | [[ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°]] |
| Weight Initialization | ì§êµ í–‰ë ¬, ì¡°ê±´ìˆ˜ | [[í–‰ë ¬ê³¼ ì„ í˜• ë³€í™˜]] |
| ì¶”ì²œ ì‹œìŠ¤í…œ | Matrix Factorization | [[íŠ¹ì‡ê°’ ë¶„í•´]] |
| Spectral Norm | ìµœëŒ€ íŠ¹ì´ê°’ | [[íŠ¹ì‡ê°’ ë¶„í•´]] |

---

## ì°¸ê³  ìë£Œ

### êµê³¼ì„œ
- Gilbert Strang, *Introduction to Linear Algebra*, 5th ed.
- David C. Lay, *Linear Algebra and Its Applications*
- Sheldon Axler, *Linear Algebra Done Right*
- Carl D. Meyer, *Matrix Analysis and Applied Linear Algebra*

### ì˜¨ë¼ì¸ ê°•ì˜
- MIT OCW 18.06 Linear Algebra (Gilbert Strang)
- 3Blue1Brown, *Essence of Linear Algebra* (YouTube)
- Khan Academy Linear Algebra

### ë”¥ëŸ¬ë‹ ê´€ì 
- Ian Goodfellow et al., *Deep Learning*, Chapter 2
- "Matrix Calculus for Deep Learning" (Stanford CS231n)

---

## ë‹¤ìŒ í•™ìŠµ

ì„ í˜•ëŒ€ìˆ˜ ê¸°ì´ˆë¥¼ ë§ˆì³¤ë‹¤ë©´:

1. **ë¯¸ì ë¶„ìœ¼ë¡œ í™•ì¥**
   - [[Math/Calculus/ë¯¸ë¶„ê³¼ ë¯¸ë¶„ë²•|ë¯¸ë¶„ê³¼ ë¯¸ì ë¶„]]
   - Jacobian, Hessianê³¼ ì„ í˜•ëŒ€ìˆ˜ì˜ ì—°ê²°

2. **ìµœì í™”ë¡œ ì—°ê²°**
   - [[Math/Optimization/ìµœì í™” ê¸°ì´ˆ|ìµœì í™” ê¸°ì´ˆ]]
   - Gradient descentì™€ Hessian

3. **ì‹¤ì „ ì‘ìš©**
   - [[Operators/ì„ í˜• ì—°ì‚°ì|ì„ í˜• ì—°ì‚°ì]]
   - [[Operators/í–‰ë ¬ ë¶„í•´|í–‰ë ¬ ë¶„í•´ ëª¨ìŒ]]

4. **ìƒìœ„ ê°œë…**
   - [[AI ê¸°ì´ˆ ê°œìš”|AI ê¸°ì´ˆ ì „ì²´ ë¡œë“œë§µ]]
