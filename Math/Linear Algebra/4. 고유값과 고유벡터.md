## TL;DR
- 행렬 $A$는 공간을 '늘리고, 줄이고, 비트는' 변환기다. 이때 고유벡터(Eigenvector)는 그 변환에도 방향이 바뀌지 않는 특별한 축, 고유값(Eigenvalue)은 그 축이 얼마나 늘어나거나 줄어드는 비율이다.
- 대각화(Diagonalization)는 행렬을 "서로 독립된 변형 축"으로 단순화하여 거듭제곱, 미분방정식 등을 쉽게 계산할 수 있게 한다.
- PCA(주성분 분석), 그래프 라플라시안, Hessian 분석 등 딥러닝 전반에서 핵심적으로 사용되는 개념이다.

---

## 1. 고유값과 고유벡터의 정의

### 1.1 정의

**정의 (고유값과 고유벡터)**: 정사각 행렬 $A \in \mathbb{R}^{n \times n}$에 대해, 영벡터가 아닌 벡터 $\mathbf{v}$와 스칼라 $\lambda$가
$$
A\mathbf{v} = \lambda \mathbf{v}
$$
를 만족하면:
- $\lambda$를 $A$의 **고유값(eigenvalue)**
- $\mathbf{v}$를 고유값 $\lambda$에 대응하는 **고유벡터(eigenvector)**

라고 한다.

**핵심 직관**: 행렬 $A$에 의한 변환 후에도 방향이 바뀌지 않고, 단지 $\lambda$배만큼 늘어나거나 줄어드는 벡터.

### 1.2 선형 변환으로 이해하기

행렬 $A$는 벡터를 다른 벡터로 바꾸는 선형 변환이다. 예를 들어:

- **회전 행렬**: "방향만 바꾸고 크기는 그대로"
- **스케일링 행렬**: "크기만 바꾸고 방향은 그대로"
- **전단(shear) 행렬**: "방향도, 크기도 다 바뀌는"

이때 일반적인 벡터들은 변환을 거치면 기울어지고 방향이 바뀌지만, 특별한 몇몇 벡터들은
$$
A\mathbf{v} = \lambda\mathbf{v}
$$
를 만족한다. 즉, 방향은 그대로인데 크기만 바뀐다.

**이 벡터가 바로 고유벡터, 그 크기 변화 비율이 고유값이다.**

### 1.3 2D 예시로 직관 잡기

$$
A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}
$$

이 행렬은 "x축으로 조금 잡아당기고, y축은 2배 늘리는 변환"이다.

여기서:
- $\mathbf{v}_1 = (1, 0)$ (x축 방향 벡터)
  - $A\mathbf{v}_1 = (3, 0) = 3\mathbf{v}_1$
  - 방향 그대로, 길이만 3배 → 고유값 3

- $\mathbf{v}_2 = (1, -1)$ (대각선 방향)
  - $A\mathbf{v}_2 = (2, -2) = 2\mathbf{v}_2$
  - 역시 방향 유지, 길이만 2배 → 고유값 2

즉, 이 변환은 "모든 방향을 섞지 않고, 특정 두 축을 각각 3배·2배로 늘리는 것"으로 이해할 수 있다.

일반 벡터는 방향이 바뀌지만, 고유벡터는 같은 직선 위에 머문다.

### 1.4 행렬이 공간을 늘리고 비트는 방식

행렬 $A$는 일반적으로 원(circle)을 타원(ellipse)으로 바꾼다.

| 변환 전 | 변환 후 | 고유벡터/고유값 |
|---------|---------|----------------|
| 단위 원(circle) | 타원(ellipse) | 타원의 축 = 고유벡터, 반지름 길이 = 고유값 |
| 2D 평면상의 벡터들 | 방향이 섞이고 비틀림 | 단, 고유벡터만 방향 유지 |
| 공간 변형의 중심축 | 변형 정도(스케일) | 바로 고유값 $\lambda$ |

**핵심**: 이 타원의 장축·단축이 바로 고유벡터, 그 축의 스케일이 바로 고유값이다.

즉:
- 고유벡터 = 변형의 주축
- 고유값 = 그 축의 변형 정도

### 1.5 회전과의 차이

**회전 행렬**: 방향만 바꾸고 크기 유지 → 고유값이 복소수 ($e^{i\theta}$)
- 실수 고유벡터가 존재하지 않음 (모든 방향이 다 회전함)

**대칭 행렬**: 공간을 비틀지 않음 → 항상 실수 고유값과 직교하는 고유벡터 존재
- 그래서 PCA, Hessian 분석 등에 대칭 행렬이 자주 등장함

### 1.6 기하학적 직관 요약표

| 개념 | 기하학적 의미 |
|------|--------------|
| 고유벡터 (Eigenvector) | 변환 후에도 방향이 바뀌지 않는 축 벡터 |
| 고유값 (Eigenvalue) | 그 방향으로의 늘림/줄임 비율 (스케일) |
| 대각화 | "서로 독립된 변형 축"으로 변환 전체를 단순화 |
| 대칭 행렬 | 서로 수직인 변형 축(직교 고유벡터)을 가짐 |
| 복소수 고유값 | 순수 회전(방향은 바뀌지만 크기 유지)을 의미 |

**한 문장 정리**: 고유벡터는 선형변환의 고유한 축, 고유값은 그 축의 변형 비율이다. 행렬은 결국 공간을 "고유벡터 방향으로 늘리고 줄이는 변환기"다.

### 1.7 특성 방정식

고유값을 구하려면 $A\mathbf{v} = \lambda \mathbf{v}$를 다시 쓴다:
$$
(A - \lambda I)\mathbf{v} = \mathbf{0}
$$

자명하지 않은 해 $\mathbf{v} \neq \mathbf{0}$가 존재하려면:
$$
\det(A - \lambda I) = 0
$$

이를 **특성 방정식(characteristic equation)**이라 하며, $\lambda$에 대한 $n$차 다항식이다.

**예제**:
$$
A = \begin{bmatrix} 4 & 1 \\ 2 & 3 \end{bmatrix}
$$
$$
\det(A - \lambda I) = \det\begin{bmatrix} 4-\lambda & 1 \\ 2 & 3-\lambda \end{bmatrix} = (4-\lambda)(3-\lambda) - 2
$$
$$
= \lambda^2 - 7\lambda + 10 = (\lambda - 5)(\lambda - 2) = 0
$$

고유값: $\lambda_1 = 5$, $\lambda_2 = 2$

---

## 2. 고유벡터 찾기

### 2.1 고유벡터 계산

고유값 $\lambda$가 주어졌을 때, $(A - \lambda I)\mathbf{v} = \mathbf{0}$의 null space를 구한다.
```
Null space = “행렬에 곱했을 때 0이 되는 모든 벡터의 집합”
```

**예제 (계속)**:
$\lambda_1 = 5$일 때:
$$
(A - 5I)\mathbf{v} = \begin{bmatrix} -1 & 1 \\ 2 & -2 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \mathbf{0}
$$
$$
-v_1 + v_2 = 0 \Rightarrow v_2 = v_1
$$
고유벡터: $\mathbf{v}_1 = t\begin{bmatrix} 1 \\ 1 \end{bmatrix}$, $t \neq 0$ (보통 $t=1$로 정규화)

$\lambda_2 = 2$일 때:
$$
(A - 2I)\mathbf{v} = \begin{bmatrix} 2 & 1 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \mathbf{0}
$$
$$
2v_1 + v_2 = 0 \Rightarrow v_2 = -2v_1
$$
고유벡터: $\mathbf{v}_2 = \begin{bmatrix} 1 \\ -2 \end{bmatrix}$

### 2.2 고유공간 (Eigenspace)

**정의**: 고유값 $\lambda$에 대응하는 모든 고유벡터와 영벡터의 집합:
$$
E_\lambda = \{\mathbf{v} : A\mathbf{v} = \lambda\mathbf{v}\} = \ker(A - \lambda I)
$$

이는 벡터 공간(부분공간)을 이룬다.

**기하 다중도(Geometric Multiplicity)**: $\dim(E_\lambda)$ (고유공간의 차원)

**대수 다중도(Algebraic Multiplicity)**: 특성 방정식에서 $\lambda$가 근으로 나타나는 횟수

**정리**: 기하 다중도 ≤ 대수 다중도

---

## 3. 대각화 (Diagonalization)

### 3.1 대각화란 무엇인가

**핵심 한 줄**: 대각화란, "복잡한 변환 $A$"를 "각 고유벡터 방향으로만 단순히 늘리거나 줄이는 변환 $D$"으로 바꾸는 것이다. 즉, 좌표계를 고유벡터 기준으로 바꿔서 행렬을 가장 단순한 형태로 보는 것이다.

### 3.2 왜 대각화가 필요한가

행렬 $A$는 일반적으로 벡터들을 비틀고(회전), 눌러서(스케일), 섞는(선형결합) 변환을 한다. 그래서 "그 안쪽에서 무슨 일이 일어나는지" 한눈에 보기 어렵다.

그런데 만약 우리가 "그 변환이 가장 단순하게 작용하는 방향(=고유벡터)"을 기준으로 좌표계를 다시 잡으면?

그 좌표계에서는 $A$가 "각 축을 $\lambda$배로 늘리거나 줄이는" 단순한 변환으로 바뀌어버린다. 그게 바로 **대각행렬 $D$** 이다.

### 3.3 기하학적 해석

**원래 좌표계 기준 (복잡한 변환)**:
- $A$는 벡터들을 섞는다
- 어떤 벡터를 넣으면 방향도 바뀌고 크기도 바뀜
- 예: 원(circle)을 타원(ellipse)으로 찌그러뜨림

**고유벡터 좌표계 기준 (단순한 변환)**:
- 각 고유벡터 방향(축)으로만 독립적으로 스케일링
- 각 축은 서로 간섭하지 않음
- 예: 타원의 축 방향이 바로 고유벡터, 축의 길이가 고유값

그래서 $A$를 "고유벡터 축으로 보면 단순한 스케일링 행렬"로 표현할 수 있다:
$$
A = PDP^{-1}
$$

### 3.4 각 항의 기하학적 의미

| 항 | 기하학적 의미 |
|----|--------------|
| $A$ | 원래 복잡한 선형변환 |
| $P$ | 좌표축을 고유벡터 방향으로 회전시키는 행렬 |
| $D$ | 그 좌표계에서 단순히 각 축을 $\lambda$배로 스케일 |
| $P^{-1}$ | 다시 원래 좌표계로 되돌림 |

즉:
1. "$P^{-1}$"로 축을 새로 잡고,
2. "$D$"로 각 축을 늘리고 줄인 뒤,
3. "$P$"로 원래 좌표계로 복귀.

### 3.5 2D로 시각화

원래 좌표계 기준에서 $A$는 평면의 벡터들을 섞고 방향도 바꿔버린다.

고유벡터 방향으로 좌표계를 바꿔보면, 이제 $A$는 단순히:
- x축 방향은 5배 늘리고,
- y축 방향은 2배 늘리는 변환이 됨.
- (이게 바로 대각행렬 $D = \operatorname{diag}(5, 2)$)

즉, "기저 벡터를 고유벡터로 교체하면" $A$는 완전히 단순한 형태로 보인다.

**비유**: 복잡한 변환 $A$는 마치 "비틀린 렌즈"처럼 보이지만, 그 안에는 사실 단순한 확대·축소 축(고유벡터)들이 숨어 있다. 대각화는 그 축들을 찾아내서 렌즈를 '올바른 각도'에서 보는 과정이다.

### 3.6 대각화 가능성

**정의**: 행렬 $A$가 대각화 가능(diagonalizable)하다 $\Leftrightarrow$ 가역 행렬 $P$와 대각 행렬 $D$가 존재하여
$$
A = PDP^{-1}
$$

**정리**: $n \times n$ 행렬 $A$가 대각화 가능 $\Leftrightarrow$ $A$가 $n$개의 선형 독립인 고유벡터를 가진다.

**구성 방법**:
- $P$: 고유벡터들을 열로 배치 $P = [\mathbf{v}_1 \mid \mathbf{v}_2 \mid \cdots \mid \mathbf{v}_n]$
- $D$: 고유값을 대각 성분으로 $D = \operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)$

### 3.7 대각화의 의미 요약표

| 개념 | 의미 |
|------|------|
| $A$ | 일반적인 선형변환 (뒤틀림, 회전, 스케일 혼합) |
| $D$ | 순수 스케일 변환 (각 축만 독립적으로 늘림) |
| $P$ | 변환의 "숨겨진 고유축(고유벡터)"을 기준으로 좌표계를 재정의 |
| 결과 | 복잡한 변환을 단순한 스케일 조합으로 분해한 것 |

### 3.8 대각화 예제

**예제**:
$$
A = \begin{bmatrix} 4 & 1 \\ 2 & 3 \end{bmatrix}, \quad
\lambda_1 = 5, \mathbf{v}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad
\lambda_2 = 2, \mathbf{v}_2 = \begin{bmatrix} 1 \\ -2 \end{bmatrix}
$$

$$
P = \begin{bmatrix} 1 & 1 \\ 1 & -2 \end{bmatrix}, \quad
D = \begin{bmatrix} 5 & 0 \\ 0 & 2 \end{bmatrix}
$$

검증:
$$
AP = \begin{bmatrix} 4 & 1 \\ 2 & 3 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & -2 \end{bmatrix} = \begin{bmatrix} 5 & 2 \\ 5 & -4 \end{bmatrix}
$$
$$
PD = \begin{bmatrix} 1 & 1 \\ 1 & -2 \end{bmatrix} \begin{bmatrix} 5 & 0 \\ 0 & 2 \end{bmatrix} = \begin{bmatrix} 5 & 2 \\ 5 & -4 \end{bmatrix} \quad
$$

### 3.9 대각화의 활용

| 활용             | 수학식                      | 기하학적 해석                                              |
| -------------- | ------------------------ | ---------------------------------------------------- |
| 거듭제곱 $A^k$     | $A^k = PD^kP^{-1}$       | 한 번의 변환이 각 축별로 $\lambda$배라면, k번 하면 $\lambda^k$배 스케일링 |
| 지수 행렬 $e^{At}$ | $e^{At} = Pe^{Dt}P^{-1}$ | 각 축이 $\lambda$에 따라 독립적으로 성장/감쇠                       |
| 안정성 분석         | 고유값의 실부가 음수 → 안정         | 시간이 지날수록 모든 방향의 크기가 줄어듦                              |

**1) 거듭제곱**:
$$
A^k = (PDP^{-1})^k = PD^kP^{-1} = P\operatorname{diag}(\lambda_1^k, \ldots, \lambda_n^k)P^{-1}
$$

대각 행렬의 거듭제곱은 각 성분을 거듭제곱하면 되므로 계산이 매우 쉽다.

**기하학적 의미**: 한 번의 변환이 각 고유벡터 방향으로 $\lambda_i$배 스케일한다면, $k$번 반복하면 $\lambda_i^k$배가 된다. 각 방향이 독립적으로 변화한다.

**2) 행렬 지수 함수**:
$$
e^{At} = Pe^{Dt}P^{-1} = P\operatorname{diag}(e^{\lambda_1 t}, \ldots, e^{\lambda_n t})P^{-1}
$$

미분방정식 $\frac{d\mathbf{x}}{dt} = A\mathbf{x}$의 해: $\mathbf{x}(t) = e^{At}\mathbf{x}(0)$

**기하학적 의미**: 각 고유벡터 방향이 $e^{\lambda_i t}$에 따라 독립적으로 성장하거나 감쇠한다. $\lambda_i > 0$이면 성장, $\lambda_i < 0$이면 감쇠.

**3) 안정성 분석**:
- 모든 고유값의 실부가 음수 → 시스템이 안정
- RNN에서 고유값 크기가 1보다 크면 gradient explosion

**기하학적 의미**: 시간이 지날수록 모든 고유벡터 방향의 크기가 줄어들어 원점으로 수렴한다.

---

## 4. 대칭 행렬의 고유값 분해

### 4.1 대칭 행렬의 특별함

대칭 행렬 $A = A^\top$는 일반 행렬과 달리 매우 특별한 성질을 갖는다:

**왜 대칭 행렬이 중요한가?**
- 공분산 행렬 (PCA에서 사용)
- Hessian 행렬 (2차 최적화)
- 그래프 라플라시안 (Graph Neural Networks)
- 물리학의 관성 텐서, 응력 텐서 등

이들은 모두 대칭 행렬이며, 그래서 "공간을 비틀지 않고" 순수하게 늘리고 줄이는 변환으로 볼 수 있다.

### 4.2 스펙트럴 정리 (Spectral Theorem)

스펙트럴 정리는 "대칭 행렬은 기하학적으로 얼마나 순수한 변환인가"를 공식적으로 증명한 정리다.

**핵심 직관**: 대칭 행렬은 공간을 비틀지 않고, 서로 수직인 축(고유벡터)들을 따라 단순히 스케일링만 하는 변환이다.

#### 4.2.1 수학적 형태

**정리**: 실수 대칭 행렬 $A = A^\top$는 다음 성질을 갖는다:
1. **모든 고유값이 실수**
2. **서로 다른 고유값에 대응하는 고유벡터는 직교**
3. **직교 대각화 가능**: $A = QDQ^\top$ (단, $Q$는 직교 행렬)

여기서:
- $Q$: 직교 행렬 (서로 직교하는 축 = 회전 또는 반사)
- $D$: 대각 행렬 (각 축 방향으로의 스케일 $\lambda_1, \lambda_2, \ldots$)

#### 4.2.2 일반 행렬 vs 대칭 행렬

**일반 행렬 $A$는 공간을 이렇게 바꾼다:**
- 회전시킴 (벡터 방향 바뀜)
- 비틀거나 늘림 (축이 기울어짐)
- 타원 축이 서로 직교하지 않을 수 있음
- 복소수 고유값을 가질 수 있음

**대칭 행렬은 비틀지 않는다:**
- 원(circle)은 타원(ellipse)으로 바뀌되,
- 타원의 축(고유벡터)은 항상 서로 90도,
- 각 축은 단지 $\lambda$배로 늘어나거나 줄어든다.

즉, "회전도, 뒤틀림도 없고, 단순히 서로 수직한 축 방향으로만 스케일하는 변환."

#### 4.2.3 $QDQ^\top$의 기하학적 단계

$A = QDQ^\top$를 단계별로 이해하면:

| 단계 | 수학적 의미 | 기하학적 의미 |
|------|------------|--------------|
| $Q^\top$ | 좌표축을 고유벡터 방향으로 회전 | "새 좌표계로 보기" |
| $D$ | 각 축마다 $\lambda$배 스케일링 | "순수하게 늘이기/줄이기" |
| $Q$ | 다시 원래 좌표계로 회전 | "다시 원래 관점으로 복귀" |

즉, **대칭 행렬의 변환 = 회전 → 스케일 → 역회전**

#### 4.2.4 2D 시각화

1. 원(circle)을 준비
2. $Q^\top$로 좌표축을 고유벡터 축 방향으로 회전
3. $D$에서 x축은 $\lambda_1$배, y축은 $\lambda_2$배 늘림 → 타원(ellipse)
4. $Q$로 원래 좌표계로 돌리면 → 회전된 타원

이 타원의 장축·단축이 고유벡터, 그 길이비가 고유값이다.

즉, 스펙트럴 정리는 "대칭 행렬이 어떤 원을 어떤 타원으로 바꾸는가"를 완벽히 설명하는 공식이다.

#### 4.2.5 왜 "Spectral(스펙트럴)"이라 부르나?

"스펙트럼(spectrum)"이란 "고유값들의 집합"을 뜻한다.

스펙트럴 정리 = 행렬을 그 '스펙트럼(고유값)'에 따라 완전히 분해할 수 있다.

그 말은 곧 **대칭 행렬은 고유값과 고유벡터만으로 100% 설명된다**는 뜻이다. 이것은 "모든 정보가 순수하게 스케일 정보로만 구성된 변환"이라는 의미다.

#### 4.2.6 물리적/실무적 응용

스펙트럴 정리는 물리적으로도 중요하다. 대칭 행렬은 물리적 "힘, 응력, 관성" 같은 회전 없이 작용하는 변형을 나타낸다.

| 분야 | 대칭 행렬 | 기하학적 의미 |
|------|-----------|--------------|
| PCA | 공분산 행렬 | 데이터의 분산 방향(주성분)은 직교, 분산량은 고유값 |
| Hessian | 2차 미분 행렬 | 표면의 곡률(방향별 휘어짐) 축은 서로 수직 |
| 물리학 | 응력·관성 텐서 | 물체가 늘어나거나 압축되는 방향은 항상 직교 |
| 그래프 라플라시안 | 네트워크 연결성 | 서로 독립적인 진동/모드(고유벡터)가 존재 |

#### 4.2.7 기하학적 요약

| 수학적 표현         | 기하학적 해석              |
| -------------- | -------------------- |
| $A = QDQ^\top$ | 회전-스케일-역회전           |
| 고유벡터           | 타원의 축 방향             |
| 고유값            | 각 축의 길이 비율 (스케일링 정도) |
| $Q$            | 회전 행렬 (좌표계 변환)       |
| $D$            | 순수 스케일 행렬            |
| 결과             | 원 → 비틀림 없는 타원        |

**한 문장 요약**: 스펙트럴 정리는 "대칭 행렬은 회전이나 비틀림 없이 서로 수직한 방향(고유벡터)으로만 공간을 스케일링한다"는 기하학적 진리를 수학적으로 공식화한 정리다.

### 4.3 증명 스케치

**1) 모든 고유값이 실수**

고유값 방정식 $A\mathbf{v} = \lambda\mathbf{v}$에서 양변에 $\mathbf{v}^*$ (켤레 전치)를 곱하면:
$$
\mathbf{v}^* A \mathbf{v} = \lambda \mathbf{v}^* \mathbf{v} = \lambda |\mathbf{v}|^2
$$

$A = A^\top$이고 $A$가 실수 행렬이므로:
$$
\mathbf{v}^* A \mathbf{v} = \mathbf{v}^* A^\top \mathbf{v} = (A\mathbf{v})^* \mathbf{v} = (\lambda\mathbf{v})^* \mathbf{v} = \lambda^* |\mathbf{v}|^2
$$

따라서 $\lambda |\mathbf{v}|^2 = \lambda^* |\mathbf{v}|^2$ → $\lambda = \lambda^*$ → $\lambda$는 실수

**2) 서로 다른 고유값의 고유벡터는 직교**

$\lambda_1 \neq \lambda_2$이고 $A\mathbf{v}_1 = \lambda_1\mathbf{v}_1$, $A\mathbf{v}_2 = \lambda_2\mathbf{v}_2$라 하자.

양변에 $\mathbf{v}_2^\top$를 곱하면:
$$
\mathbf{v}_2^\top A\mathbf{v}_1 = \lambda_1 \mathbf{v}_2^\top \mathbf{v}_1
$$

$A = A^\top$를 이용하면:
$$
\mathbf{v}_2^\top A\mathbf{v}_1 = (A\mathbf{v}_2)^\top \mathbf{v}_1 = (\lambda_2\mathbf{v}_2)^\top \mathbf{v}_1 = \lambda_2 \mathbf{v}_2^\top \mathbf{v}_1
$$

두 식을 같다고 놓으면:
$$
\lambda_1 \mathbf{v}_2^\top \mathbf{v}_1 = \lambda_2 \mathbf{v}_2^\top \mathbf{v}_1
$$
$$
(\lambda_1 - \lambda_2)\mathbf{v}_2^\top \mathbf{v}_1 = 0
$$

$\lambda_1 \neq \lambda_2$이므로:
$$
\mathbf{v}_2^\top \mathbf{v}_1 = 0 \quad \Rightarrow \quad \mathbf{v}_1 \perp \mathbf{v}_2
$$

**3) 직교 대각화**

$n$개의 선형독립인 고유벡터를 정규직교화하면 직교 행렬 $Q$를 얻을 수 있다.

### 4.4 직교 대각화 예제

대칭 행렬:
$$
A = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix}
$$

이 행렬은 대칭이므로 ($A = A^\top$), 스펙트럴 정리에 의해 직교 대각화가 가능하다.

**Step 1: 고유값 구하기**

특성 방정식:
$$
\det(A - \lambda I) = \det\begin{bmatrix} 3-\lambda & 1 \\ 1 & 3-\lambda \end{bmatrix} = (3-\lambda)^2 - 1 = 0
$$
$$
\lambda^2 - 6\lambda + 8 = 0 \quad \Rightarrow \quad (\lambda - 4)(\lambda - 2) = 0
$$

고유값: $\lambda_1 = 4$, $\lambda_2 = 2$

**Step 2: 고유벡터 구하기**

$\lambda_1 = 4$일 때:
$$
(A - 4I)\mathbf{v} = \begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix}\begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \mathbf{0}
$$
$$
-v_1 + v_2 = 0 \quad \Rightarrow \quad v_2 = v_1
$$

고유벡터: $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$

정규화: $\mathbf{q}_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ (길이가 1이 되도록)

$\lambda_2 = 2$일 때:
$$
(A - 2I)\mathbf{v} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}\begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \mathbf{0}
$$
$$
v_1 + v_2 = 0 \quad \Rightarrow \quad v_2 = -v_1
$$

고유벡터: $\mathbf{v}_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$

정규화: $\mathbf{q}_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$

**Step 3: 직교성 확인**

두 고유벡터의 내적:
$$
\mathbf{q}_1^\top \mathbf{q}_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \end{bmatrix} \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix} = \frac{1}{2}(1 \cdot 1 + 1 \cdot (-1)) = \frac{1}{2}(1 - 1) = 0
$$

따라서 $\mathbf{q}_1 \perp \mathbf{q}_2$ → 스펙트럴 정리 확인됨!

**Step 4: 직교 행렬 $Q$ 구성**

$$
Q = \begin{bmatrix} \mathbf{q}_1 & \mathbf{q}_2 \end{bmatrix} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}
$$

대각 행렬:
$$
D = \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix}
$$

**Step 5: 직교 행렬 검증**

$Q$가 직교 행렬인지 확인: $Q^\top Q = I$?
$$
Q^\top Q = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}^\top \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}
$$
$$
= \frac{1}{2}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}
$$
$$
= \frac{1}{2}\begin{bmatrix} 1+1 & 1-1 \\ 1-1 & 1+1 \end{bmatrix} = \frac{1}{2}\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I \quad \checkmark
$$

**Step 6: 대각화 검증**

$A = QDQ^\top$인지 확인:
$$
QDQ^\top = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix} \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}
$$
$$
= \frac{1}{2}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}\begin{bmatrix} 4 & 4 \\ 2 & -2 \end{bmatrix}
$$
$$
= \frac{1}{2}\begin{bmatrix} 4+2 & 4-2 \\ 4-2 & 4+2 \end{bmatrix} = \frac{1}{2}\begin{bmatrix} 6 & 2 \\ 2 & 6 \end{bmatrix} = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix} = A \quad \checkmark
$$

### 4.5 직교 대각화의 기하학적 의미

**일반 대각화** ($A = PDP^{-1}$):
- $P$는 단순히 기저 변환
- $P^{-1}$로 역변환 필요

**직교 대각화** ($A = QDQ^\top$):
- $Q$는 회전 (또는 반사)
- $Q^\top = Q^{-1}$이므로 역변환이 전치만으로 가능
- 기하학적으로: "회전 → 스케일 → 역회전"

**대칭 행렬의 변환 과정**:
1. $Q^\top$: 좌표축을 고유벡터 방향으로 회전
2. $D$: 각 축을 고유값만큼 스케일
3. $Q$: 원래 좌표축으로 회전 복귀

이 모든 과정에서 각도와 비율이 보존되므로 "순수한 변형"만 일어난다.

### 4.6 왜 직교 대각화가 중요한가

| 특징 | 일반 대각화 | 직교 대각화 |
|------|-----------|-----------|
| 조건 | $n$개의 선형독립 고유벡터 | 대칭 행렬 ($A = A^\top$) |
| 기저 벡터 관계 | 임의의 각도 | 직교 (서로 수직) |
| 역변환 | $P^{-1}$ 필요 (계산 복잡) | $Q^\top$ (전치만) |
| 고유값 | 복소수 가능 | 항상 실수 |
| 기하학적 해석 | 비틀림 + 스케일 | 순수 스케일 |
| 수치적 안정성 | 낮음 ($P$가 ill-conditioned 가능) | 높음 ($Q$는 항상 well-conditioned) |

**실무적 장점**:
- PCA: 주성분들이 서로 직교 → 독립적인 특징
- Hessian 분석: 극값의 방향이 명확
- 수치 계산: 직교 행렬은 조건수가 1로 안정적 

---

## 5. 딥러닝 응용

### 5.1 PCA (주성분 분석)

데이터 행렬 $X \in \mathbb{R}^{n \times d}$의 공분산 행렬:
$$
C = \frac{1}{n}X^\top X
$$

$C$는 대칭이므로 직교 대각화 가능:
$$
C = QDQ^\top
$$

**주성분**: $Q$의 열벡터 (고유벡터)
- 고유값이 큰 순서대로 정렬
- 상위 $k$개 고유벡터로 차원 축소

**분산 보존**: $k$개 주성분이 설명하는 분산 비율 = $\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^d \lambda_i}$

### 5.2 Hessian과 최적화

손실 함수 $L(\theta)$의 Hessian $H = \nabla^2 L$은 대칭 행렬이다.

**최적점 판별**:
- 모든 고유값 > 0 → positive definite → 극소점
- 모든 고유값 < 0 → negative definite → 극대점
- 양수/음수 혼재 → indefinite → saddle point

**조건수(Condition Number)**:
$$
\kappa(H) = \frac{\lambda_{\max}}{\lambda_{\min}}
$$
- $\kappa$가 크면 학습이 어려움 (gradient descent 느림)
- Preconditioning (Adam 등)이 필요한 이유

### 5.3 그래프 라플라시안

무향 그래프의 라플라시안 $L = D - A$ (대칭):
- 고유값 $0 \leq \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n$
- $\lambda_1 = 0$의 중복도 = 연결 요소 개수
- $\lambda_2$ (Fiedler value)가 작으면 그래프가 "거의 단절"
- Spectral clustering: 작은 고유값의 고유벡터로 클러스터링

---

## 6. 복소수 고유값과 Jordan 표준형

### 6.1 복소수 고유값

실수 행렬도 복소수 고유값을 가질 수 있다.

**예제**: 회전 행렬
$$
R = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
$$

특성 방정식: $\lambda^2 + 1 = 0$ → $\lambda = \pm i$

회전 변환은 실수 고유벡터가 없다 (어떤 벡터도 방향 유지 안 됨).

### 6.2 Jordan 표준형

대각화 불가능한 행렬 (예: 중복 고유값이지만 독립 고유벡터 부족)은 **Jordan 표준형**으로 변환:
$$
J = \begin{bmatrix}
\lambda & 1 & 0 \\
0 & \lambda & 1 \\
0 & 0 & \lambda
\end{bmatrix}
$$

딥러닝에서는 거의 사용하지 않으나, 이론적 분석에서 등장.

---

## 7. PyTorch 실습

```python
import torch

# 1. 고유값, 고유벡터 계산
A = torch.tensor([[4., 1.], [2., 3.]], dtype=torch.float64)
eigenvalues, eigenvectors = torch.linalg.eig(A)

print("고유값:", eigenvalues)
print("고유벡터:\n", eigenvectors)

# 2. 대각화 검증
lambda1, lambda2 = eigenvalues
P = eigenvectors
D = torch.diag(eigenvalues)

A_reconstructed = P @ D @ torch.linalg.inv(P)
print("재구성된 A:\n", A_reconstructed.real)
print("원래 A:\n", A)

# 3. 거듭제곱
k = 10
A_k_direct = torch.linalg.matrix_power(A, k)
D_k = torch.diag(eigenvalues ** k)
A_k_diag = (P @ D_k @ torch.linalg.inv(P)).real
print(f"A^{k} (직접):\n", A_k_direct)
print(f"A^{k} (대각화):\n", A_k_diag)

# 4. 대칭 행렬의 직교 대각화
S = torch.tensor([[3., 1.], [1., 3.]], dtype=torch.float64)
eigenvalues, eigenvectors = torch.linalg.eigh(S)  # 대칭용 함수

print("대칭 행렬 고유값:", eigenvalues)
Q = eigenvectors
print("Q^T Q (항등행렬):\n", Q.T @ Q)

# 5. PCA 예제
torch.manual_seed(42)
X = torch.randn(100, 5)  # 100개 샘플, 5차원
X = X - X.mean(dim=0)    # 중심화

C = (X.T @ X) / X.shape[0]  # 공분산 행렬
eigenvalues, eigenvectors = torch.linalg.eigh(C)

# 고유값 큰 순서로 정렬
idx = eigenvalues.argsort(descending=True)
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

print("주성분 설명 분산:", eigenvalues)
print("누적 분산 비율:", eigenvalues.cumsum(0) / eigenvalues.sum())

# 상위 2개 주성분으로 투영
k = 2
X_reduced = X @ eigenvectors[:, :k]
print("축소 데이터 shape:", X_reduced.shape)
```

---

## 8. 연습 문제

1. **고유값 계산**
   $A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$의 고유값과 고유벡터를 손으로 계산하고, PyTorch로 검증하라.

2. **대각화**
   $A = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$를 대각화하고, $A^{100}$을 효율적으로 계산하라.

3. **직교성 확인**
   대칭 행렬 $A = \begin{bmatrix} 5 & 3 \\ 3 & 5 \end{bmatrix}$의 고유벡터들이 직교함을 확인하라.

4. **안정성**
   $A = \begin{bmatrix} -1 & 1 \\ 0 & -2 \end{bmatrix}$의 고유값을 구하고, 시스템 $\frac{d\mathbf{x}}{dt} = A\mathbf{x}$가 안정한지 판별하라.

5. **PCA 구현**
   2차원 데이터를 생성하고 공분산 행렬의 고유분해로 주축을 찾아 시각화하라.

6. **Hessian 분석**
   $f(x, y) = x^2 - xy + y^2$의 Hessian을 계산하고 고유값으로 원점이 극소점임을 증명하라.
