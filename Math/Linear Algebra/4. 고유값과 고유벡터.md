## TL;DR
- 고유값(Eigenvalue)과 고유벡터(Eigenvector)는 선형 변환에서 "방향은 유지되고 크기만 바뀌는" 특별한 벡터와 그 배율을 의미한다.
- 대각화(Diagonalization)는 행렬을 간단한 형태로 분해하여 거듭제곱, 미분방정식 등을 쉽게 계산할 수 있게 한다.
- PCA(주성분 분석), 그래프 라플라시안, Hessian 분석 등 딥러닝 전반에서 핵심적으로 사용되는 개념이다.

---

## 1. 고유값과 고유벡터의 정의

### 1.1 정의

**정의 (고유값과 고유벡터)**: 정사각 행렬 $A \in \mathbb{R}^{n \times n}$에 대해, 영벡터가 아닌 벡터 $\mathbf{v}$와 스칼라 $\lambda$가
$$
A\mathbf{v} = \lambda \mathbf{v}
$$
를 만족하면:
- $\lambda$를 $A$의 **고유값(eigenvalue)**
- $\mathbf{v}$를 고유값 $\lambda$에 대응하는 **고유벡터(eigenvector)**

라고 한다.

**핵심 직관**: 행렬 $A$에 의한 변환 후에도 방향이 바뀌지 않고, 단지 $\lambda$배만큼 늘어나거나 줄어드는 벡터.

### 1.2 기하학적 의미

**2차원 예시**:
$$
A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}
$$

- $\mathbf{v}_1 = (1, 0)$: $A\mathbf{v}_1 = (3, 0) = 3\mathbf{v}_1$ → 고유값 $\lambda_1 = 3$
- $\mathbf{v}_2 = (1, -1)$: $A\mathbf{v}_2 = (2, -2) = 2\mathbf{v}_2$ → 고유값 $\lambda_2 = 2$

일반 벡터는 방향이 바뀌지만, 고유벡터는 같은 직선 위에 머문다.

### 1.3 특성 방정식

고유값을 구하려면 $A\mathbf{v} = \lambda \mathbf{v}$를 다시 쓴다:
$$
(A - \lambda I)\mathbf{v} = \mathbf{0}
$$

자명하지 않은 해 $\mathbf{v} \neq \mathbf{0}$가 존재하려면:
$$
\det(A - \lambda I) = 0
$$

이를 **특성 방정식(characteristic equation)**이라 하며, $\lambda$에 대한 $n$차 다항식이다.

**예제**:
$$
A = \begin{bmatrix} 4 & 1 \\ 2 & 3 \end{bmatrix}
$$
$$
\det(A - \lambda I) = \det\begin{bmatrix} 4-\lambda & 1 \\ 2 & 3-\lambda \end{bmatrix} = (4-\lambda)(3-\lambda) - 2
$$
$$
= \lambda^2 - 7\lambda + 10 = (\lambda - 5)(\lambda - 2) = 0
$$

고유값: $\lambda_1 = 5$, $\lambda_2 = 2$

---

## 2. 고유벡터 찾기

### 2.1 고유벡터 계산

고유값 $\lambda$가 주어졌을 때, $(A - \lambda I)\mathbf{v} = \mathbf{0}$의 null space를 구한다.

**예제 (계속)**:
$\lambda_1 = 5$일 때:
$$
(A - 5I)\mathbf{v} = \begin{bmatrix} -1 & 1 \\ 2 & -2 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \mathbf{0}
$$
$$
-v_1 + v_2 = 0 \Rightarrow v_2 = v_1
$$
고유벡터: $\mathbf{v}_1 = t\begin{bmatrix} 1 \\ 1 \end{bmatrix}$, $t \neq 0$ (보통 $t=1$로 정규화)

$\lambda_2 = 2$일 때:
$$
(A - 2I)\mathbf{v} = \begin{bmatrix} 2 & 1 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \mathbf{0}
$$
$$
2v_1 + v_2 = 0 \Rightarrow v_2 = -2v_1
$$
고유벡터: $\mathbf{v}_2 = \begin{bmatrix} 1 \\ -2 \end{bmatrix}$

### 2.2 고유공간 (Eigenspace)

**정의**: 고유값 $\lambda$에 대응하는 모든 고유벡터와 영벡터의 집합:
$$
E_\lambda = \{\mathbf{v} : A\mathbf{v} = \lambda\mathbf{v}\} = \ker(A - \lambda I)
$$

이는 벡터 공간(부분공간)을 이룬다.

**기하 다중도(Geometric Multiplicity)**: $\dim(E_\lambda)$ (고유공간의 차원)

**대수 다중도(Algebraic Multiplicity)**: 특성 방정식에서 $\lambda$가 근으로 나타나는 횟수

**정리**: 기하 다중도 ≤ 대수 다중도

---

## 3. 대각화 (Diagonalization)

### 3.1 대각화 가능성

**정의**: 행렬 $A$가 대각화 가능(diagonalizable)하다 $\Leftrightarrow$ 가역 행렬 $P$와 대각 행렬 $D$가 존재하여
$$
A = PDP^{-1}
$$

**정리**: $n \times n$ 행렬 $A$가 대각화 가능 $\Leftrightarrow$ $A$가 $n$개의 선형 독립인 고유벡터를 가진다.

**구성 방법**:
- $P$: 고유벡터들을 열로 배치 $P = [\mathbf{v}_1 \mid \mathbf{v}_2 \mid \cdots \mid \mathbf{v}_n]$
- $D$: 고유값을 대각 성분으로 $D = \operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)$

### 3.2 대각화 예제

**예제**:
$$
A = \begin{bmatrix} 4 & 1 \\ 2 & 3 \end{bmatrix}, \quad
\lambda_1 = 5, \mathbf{v}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad
\lambda_2 = 2, \mathbf{v}_2 = \begin{bmatrix} 1 \\ -2 \end{bmatrix}
$$

$$
P = \begin{bmatrix} 1 & 1 \\ 1 & -2 \end{bmatrix}, \quad
D = \begin{bmatrix} 5 & 0 \\ 0 & 2 \end{bmatrix}
$$

검증:
$$
AP = \begin{bmatrix} 4 & 1 \\ 2 & 3 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & -2 \end{bmatrix} = \begin{bmatrix} 5 & 2 \\ 5 & -4 \end{bmatrix}
$$
$$
PD = \begin{bmatrix} 1 & 1 \\ 1 & -2 \end{bmatrix} \begin{bmatrix} 5 & 0 \\ 0 & 2 \end{bmatrix} = \begin{bmatrix} 5 & 2 \\ 5 & -4 \end{bmatrix} \quad \checkmark
$$

### 3.3 대각화의 활용

**1) 거듭제곱**:
$$
A^k = (PDP^{-1})^k = PD^kP^{-1} = P\operatorname{diag}(\lambda_1^k, \ldots, \lambda_n^k)P^{-1}
$$

대각 행렬의 거듭제곱은 각 성분을 거듭제곱하면 되므로 계산이 매우 쉽다.

**2) 행렬 지수 함수**:
$$
e^{At} = Pe^{Dt}P^{-1} = P\operatorname{diag}(e^{\lambda_1 t}, \ldots, e^{\lambda_n t})P^{-1}
$$

미분방정식 $\frac{d\mathbf{x}}{dt} = A\mathbf{x}$의 해: $\mathbf{x}(t) = e^{At}\mathbf{x}(0)$

**3) 안정성 분석**:
- 모든 고유값의 실부가 음수 → 시스템이 안정
- RNN에서 고유값 크기가 1보다 크면 gradient explosion

---

## 4. 대칭 행렬의 고유값 분해

### 4.1 스펙트럴 정리 (Spectral Theorem)

**정리**: 실수 대칭 행렬 $A = A^\top$는 다음 성질을 갖는다:
1. 모든 고유값이 실수
2. 서로 다른 고유값에 대응하는 고유벡터는 직교
3. 직교 대각화 가능: $A = QDQ^\top$ (단, $Q$는 직교 행렬)

**증명 스케치**:
1. $A\mathbf{v} = \lambda\mathbf{v}$에서 $\mathbf{v}^* A \mathbf{v} = \lambda |\mathbf{v}|^2$
2. $A = A^\top$이면 좌변이 실수 → $\lambda$도 실수
3. $\lambda_1 \neq \lambda_2$일 때, $\lambda_1 \mathbf{v}_1^\top \mathbf{v}_2 = \mathbf{v}_1^\top A \mathbf{v}_2 = (A\mathbf{v}_1)^\top \mathbf{v}_2 = \lambda_2 \mathbf{v}_1^\top \mathbf{v}_2$
   따라서 $(\lambda_1 - \lambda_2)\mathbf{v}_1^\top \mathbf{v}_2 = 0$ → $\mathbf{v}_1 \perp \mathbf{v}_2$ ∎

### 4.2 직교 대각화 예제

$$
A = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix}
$$

특성 방정식: $(3-\lambda)^2 - 1 = 0$ → $\lambda_1 = 4$, $\lambda_2 = 2$

고유벡터:
- $\lambda_1 = 4$: $\mathbf{v}_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ (정규화)
- $\lambda_2 = 2$: $\mathbf{v}_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$

$$
Q = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}, \quad
D = \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix}
$$

검증: $Q^\top Q = I$ ✓

---

## 5. 딥러닝 응용

### 5.1 PCA (주성분 분석)

데이터 행렬 $X \in \mathbb{R}^{n \times d}$의 공분산 행렬:
$$
C = \frac{1}{n}X^\top X
$$

$C$는 대칭이므로 직교 대각화 가능:
$$
C = QDQ^\top
$$

**주성분**: $Q$의 열벡터 (고유벡터)
- 고유값이 큰 순서대로 정렬
- 상위 $k$개 고유벡터로 차원 축소

**분산 보존**: $k$개 주성분이 설명하는 분산 비율 = $\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^d \lambda_i}$

### 5.2 Hessian과 최적화

손실 함수 $L(\theta)$의 Hessian $H = \nabla^2 L$은 대칭 행렬이다.

**최적점 판별**:
- 모든 고유값 > 0 → positive definite → 극소점
- 모든 고유값 < 0 → negative definite → 극대점
- 양수/음수 혼재 → indefinite → saddle point

**조건수(Condition Number)**:
$$
\kappa(H) = \frac{\lambda_{\max}}{\lambda_{\min}}
$$
- $\kappa$가 크면 학습이 어려움 (gradient descent 느림)
- Preconditioning (Adam 등)이 필요한 이유

### 5.3 그래프 라플라시안

무향 그래프의 라플라시안 $L = D - A$ (대칭):
- 고유값 $0 \leq \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n$
- $\lambda_1 = 0$의 중복도 = 연결 요소 개수
- $\lambda_2$ (Fiedler value)가 작으면 그래프가 "거의 단절"
- Spectral clustering: 작은 고유값의 고유벡터로 클러스터링

---

## 6. 복소수 고유값과 Jordan 표준형

### 6.1 복소수 고유값

실수 행렬도 복소수 고유값을 가질 수 있다.

**예제**: 회전 행렬
$$
R = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
$$

특성 방정식: $\lambda^2 + 1 = 0$ → $\lambda = \pm i$

회전 변환은 실수 고유벡터가 없다 (어떤 벡터도 방향 유지 안 됨).

### 6.2 Jordan 표준형

대각화 불가능한 행렬 (예: 중복 고유값이지만 독립 고유벡터 부족)은 **Jordan 표준형**으로 변환:
$$
J = \begin{bmatrix}
\lambda & 1 & 0 \\
0 & \lambda & 1 \\
0 & 0 & \lambda
\end{bmatrix}
$$

딥러닝에서는 거의 사용하지 않으나, 이론적 분석에서 등장.

---

## 7. PyTorch 실습

```python
import torch

# 1. 고유값, 고유벡터 계산
A = torch.tensor([[4., 1.], [2., 3.]], dtype=torch.float64)
eigenvalues, eigenvectors = torch.linalg.eig(A)

print("고유값:", eigenvalues)
print("고유벡터:\n", eigenvectors)

# 2. 대각화 검증
lambda1, lambda2 = eigenvalues
P = eigenvectors
D = torch.diag(eigenvalues)

A_reconstructed = P @ D @ torch.linalg.inv(P)
print("재구성된 A:\n", A_reconstructed.real)
print("원래 A:\n", A)

# 3. 거듭제곱
k = 10
A_k_direct = torch.linalg.matrix_power(A, k)
D_k = torch.diag(eigenvalues ** k)
A_k_diag = (P @ D_k @ torch.linalg.inv(P)).real
print(f"A^{k} (직접):\n", A_k_direct)
print(f"A^{k} (대각화):\n", A_k_diag)

# 4. 대칭 행렬의 직교 대각화
S = torch.tensor([[3., 1.], [1., 3.]], dtype=torch.float64)
eigenvalues, eigenvectors = torch.linalg.eigh(S)  # 대칭용 함수

print("대칭 행렬 고유값:", eigenvalues)
Q = eigenvectors
print("Q^T Q (항등행렬):\n", Q.T @ Q)

# 5. PCA 예제
torch.manual_seed(42)
X = torch.randn(100, 5)  # 100개 샘플, 5차원
X = X - X.mean(dim=0)    # 중심화

C = (X.T @ X) / X.shape[0]  # 공분산 행렬
eigenvalues, eigenvectors = torch.linalg.eigh(C)

# 고유값 큰 순서로 정렬
idx = eigenvalues.argsort(descending=True)
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

print("주성분 설명 분산:", eigenvalues)
print("누적 분산 비율:", eigenvalues.cumsum(0) / eigenvalues.sum())

# 상위 2개 주성분으로 투영
k = 2
X_reduced = X @ eigenvectors[:, :k]
print("축소 데이터 shape:", X_reduced.shape)
```

---

## 8. 연습 문제

1. **고유값 계산**
   $A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$의 고유값과 고유벡터를 손으로 계산하고, PyTorch로 검증하라.

2. **대각화**
   $A = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$를 대각화하고, $A^{100}$을 효율적으로 계산하라.

3. **직교성 확인**
   대칭 행렬 $A = \begin{bmatrix} 5 & 3 \\ 3 & 5 \end{bmatrix}$의 고유벡터들이 직교함을 확인하라.

4. **안정성**
   $A = \begin{bmatrix} -1 & 1 \\ 0 & -2 \end{bmatrix}$의 고유값을 구하고, 시스템 $\frac{d\mathbf{x}}{dt} = A\mathbf{x}$가 안정한지 판별하라.

5. **PCA 구현**
   2차원 데이터를 생성하고 공분산 행렬의 고유분해로 주축을 찾아 시각화하라.

6. **Hessian 분석**
   $f(x, y) = x^2 - xy + y^2$의 Hessian을 계산하고 고유값으로 원점이 극소점임을 증명하라.

---

## 9. 참고 자료

- Gilbert Strang, *Introduction to Linear Algebra*, Chapter 6
- David C. Lay, *Linear Algebra and Its Applications*, Chapter 5
- MIT OCW 18.06, Lectures 21-24
- Ian Goodfellow et al., *Deep Learning*, Chapter 2.7
- 3Blue1Brown, *Essence of Linear Algebra*, Episode 13-16

---

## 10. 다음 학습

- [[Math/Linear Algebra/특잇값 분해|특이값 분해 (SVD)]]
- [[Math/Linear Algebra/행렬과 선형 변환|행렬과 선형 변환]]
- [[Math/Calculus/미분과 미분법|미분과 미적분]]
- [[ML Foundations/정규화와 일반화|정규화와 일반화]]
