## TL;DR
- 수치 선형대수는 조건수, 수치 안정성, 반복법, 사전조건자 등 대규모 행렬 계산의 실제 비용과 정확도를 다룬다.
- Deep Equilibrium Model, Implicit Layer, Hessian-Free Optimization, Krylov 기반 솔버 등에 필수적이다.
- PyTorch/NumPy로 CG/GMRES/Preconditioning을 실험해 수렴 속도와 오류를 정량화하면 대규모 모델 최적화 전략에 직접 적용할 수 있다.

## 언제 쓰나
- 거대한 선형 시스템 $Ax=b$를 직접 분해하기 어렵고 반복법으로 근사해야 할 때
- Hessian-vector product로 Newton/Quasi-Newton을 근사하거나, implicit differentiation이 필요한 모델에서
- PSD 체크, 조건수 추정, Floating point 안정성 분석이 필요한 PDE/물리 기반 모델에서
- 대규모 그래프/GNN/Recommendation에서 sparse matrix 연산을 안정화할 때

## 주요 API
| 개념 | 정의 | 실습 포인트 |
| --- | --- | --- |
| 조건수 $\kappa(A)$ | $\kappa(A)=\|A\|\|A^{-1}\|$ → 입력 오차 대비 출력 오차 | `torch.linalg.cond`(2-노름) |
| 반복법 | Jacobi, Gauss-Seidel, CG, GMRES 등 | 수렴/발산 비교 |
| Krylov Subspace | $\mathcal{K}_k(A,r_0)$ | Arnoldi/Lanczos |
| Preconditioner | $M^{-1}A x = M^{-1}b$로 변환, $M \approx A$ | Jacobi, ILU |
| Mixed Precision | FP16/FP32 혼합, loss scaling | `torch.cuda.amp.autocast` |
| Sparse Solver | CSR 기반 | `scipy.sparse.linalg.cg` 참고 |

## 실습 예제
### 1. 조건수 측정과 스케일링 효과
```python
import math

A = [[10.0, 0.5], [0.2, 1.0]]
# 2-노름 조건수 (수동 계산)
trace = A[0][0] + A[1][1]
# eigenvalues of A^T A
AtA = [
    [A[0][0]**2 + A[1][0]**2, A[0][0]*A[0][1] + A[1][0]*A[1][1]],
    [A[0][0]*A[0][1] + A[1][0]*A[1][1], A[0][1]**2 + A[1][1]**2]
]
trace_AtA = AtA[0][0] + AtA[1][1]
det_AtA = AtA[0][0]*AtA[1][1] - AtA[0][1]*AtA[1][0]
disc = math.sqrt(trace_AtA**2/4 - det_AtA)
lam1 = trace_AtA/2 + disc
lam2 = trace_AtA/2 - disc
cond = math.sqrt(lam1 / lam2)
print('condition number ≈', cond)
```
- 행렬을 스케일링(각 열을 정규화)하면 조건수가 감소해 수치 안정성이 향상된다.

### 2. Jacobi vs. Gauss-Seidel 반복법
```python
A = [[4., 1., 0.], [1., 3., 1.], [0., 1., 2.]]
b = [1., 2., 3.]

def jacobi(A, b, iters=20):
    x = [0., 0., 0.]
    for _ in range(iters):
        new_x = x.copy()
        for i in range(3):
            sigma = sum(A[i][j]*x[j] for j in range(3) if j != i)
            new_x[i] = (b[i]-sigma)/A[i][i]
        x = new_x
    return x

def gauss_seidel(A, b, iters=20):
    x = [0., 0., 0.]
    for _ in range(iters):
        for i in range(3):
            sigma = sum(A[i][j]*x[j] for j in range(3) if j != i)
            x[i] = (b[i]-sigma)/A[i][i]
    return x

print('Jacobi:', jacobi(A, b))
print('Gauss-Seidel:', gauss_seidel(A, b))
```
- 대각 우세 행렬에서는 Gauss-Seidel이 Jacobi보다 빠르게 수렴함을 확인할 수 있다.

### 3. Conjugate Gradient (CG) 구현 스케치
```python
import math

A = [[4., 1.], [1., 3.]]
b = [1., 2.]
x = [0., 0.]
r = [b[i] - sum(A[i][j]*x[j] for j in range(2)) for i in range(2)]
p = r.copy()
rs_old = sum(val*val for val in r)
for _ in range(10):
    Ap = [sum(A[i][j]*p[j] for j in range(2)) for i in range(2)]
    alpha = rs_old / sum(p[i]*Ap[i] for i in range(2))
    x = [x[i] + alpha * p[i] for i in range(2)]
    r = [r[i] - alpha * Ap[i] for i in range(2)]
    rs_new = sum(val*val for val in r)
    if math.sqrt(rs_new) < 1e-6:
        break
    beta = rs_new / rs_old
    p = [r[i] + beta * p[i] for i in range(2)]
    rs_old = rs_new
print('CG solution:', x)
```
- SPD 행렬에서 CG는 정확히 $n$번 이내에 수렴한다(수치오차 제외). 실제로는 preconditioning을 곁들여 수렴을 가속한다.

### 4. Jacobi Preconditioner 예시
```python
# M = diag(A)
M_inv = [1.0 / A[i][i] for i in range(2)]
# Preconditioned residual z = M^{-1} r
z = [M_inv[i] * r[i] for i in range(2)]
```
- Jacobi preconditioner는 계산이 단순해 Krylov 방법과 함께 자주 사용된다.

### 5. 혼합 정밀도에서 조건수 영향
```python
values = [1e-3, 1.0, 1e3]
scaled = [val / 1e3 for val in values]
print('original range:', max(values)/min(values))
print('scaled range:', max(scaled)/min(scaled))
```
- 값의 스케일을 축소하면 FP16에서도 underflow/overflow를 줄일 수 있다.
- Implicit Layer/DEQ에서 FP16을 사용할 때는 layer normalization이나 scaling 전략을 사전에 설계해야 한다.

## 실수 주의
- 반복법은 SPD/대각 우세 등 수렴 조건을 충족해야 하므로, 조건이 맞지 않는 행렬에 무작정 적용하면 발산한다.
- CG/GMRES에서 rounding error가 누적되면 정규직교성이 깨져 수렴이 느려질 수 있다.
- Preconditioner를 만들 때 역행렬을 직접 계산하면 비용이 폭증하므로 근사(예: incomplete factorization)를 사용한다.
- Mixed precision에서 scaling을 잘못하면 gradient가 NaN이 되거나 underflow로 0이 되므로 점진적 loss scaling이 필요하다.

## 관련 노트
- [[Math/Linear Algebra/5. 특잇값 분해]]
- [[Math/Linear Algebra/6. 슈타이니츠 교환 정리]]
- [[Math/Linear Algebra/7. 저랭크 근사와 행렬 분해]]
- [[Math/Calculus/6. 벡터 미적분]]
- [[Math/Math_확장_계획]]
