## TL;DR
- **스펙트럴 그래프 이론**은 그래프의 인접 행렬과 Laplacian 행렬의 **고유값과 고유벡터**로 그래프 구조를 분석한다.
- **그래프 라플라시안**의 고유값은 연결성, 클러스터 수, 그래프 컷과 깊은 관계가 있으며, 스펙트럴 클러스터링의 이론적 기반이다.
- **GNN (Graph Neural Networks)**은 메시지 패싱으로 노드 표현을 학습하며, GCN은 스펙트럴 필터링의 근사로 해석된다.

---

## 1. 핵심 개념

### 1.1 왜 스펙트럴 이론인가?

**동기**:
- 그래프 구조를 **행렬로 표현** → 선형대수 도구 활용
- 고유값/고유벡터가 전역 구조 정보 함축
- Fourier 변환의 그래프 버전

**응용**:
- 커뮤니티 탐지 (spectral clustering)
- 그래프 분할 (graph partitioning)
- 차원 축소 (Laplacian Eigenmap)
- GNN 설계 (spectral convolution)

---

## 2. 수학적 전개

### 2.1 인접 행렬의 스펙트럼

**인접 행렬** $A$:
$$
A_{ij} = \begin{cases}
1 & \text{if } (i, j) \in E \\
0 & \text{otherwise}
\end{cases}
$$

**고유값 분해**:
$$
A = Q \Lambda Q^\top
$$
- $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_n)$: 고유값
- $Q = [q_1, \ldots, q_n]$: 고유벡터

**Perron-Frobenius 정리** (연결 그래프):
- 최대 고유값 $\lambda_{\max} > 0$은 단순(simple)
- 대응 고유벡터는 모든 성분 양수

**예시** (Complete graph $K_n$):
- $\lambda_1 = n - 1$ (multiplicity 1)
- $\lambda_2 = \cdots = \lambda_n = -1$

### 2.2 그래프 Laplacian

**정의**:
$$
L = D - A
$$
- $D = \text{diag}(\deg(1), \ldots, \deg(n))$: 차수 행렬

**Quadratic form**:
$$
x^\top L x = \sum_{(i,j) \in E} (x_i - x_j)^2
$$

**증명**:
$$
\begin{align}
x^\top L x &= x^\top D x - x^\top A x \\
&= \sum_i \deg(i) x_i^2 - \sum_{(i,j) \in E} x_i x_j \\
&= \sum_{(i,j) \in E} (x_i^2 + x_j^2 - 2 x_i x_j) \\
&= \sum_{(i,j) \in E} (x_i - x_j)^2 \quad \text{∎}
\end{align}
$$

**의미**: $L$은 **smoothness**를 측정 ($x$가 엣지에서 천천히 변함)

### 2.3 Laplacian의 성질

**명제 1**: $L$은 **positive semidefinite**
- $x^\top L x \ge 0$ (위 quadratic form에서 자명)

**명제 2**: 0은 항상 고유값
- $L \mathbf{1} = (D - A) \mathbf{1} = 0$ ($\mathbf{1} = [1, \ldots, 1]^\top$)

**정리** (Laplacian 고유값과 연결성):
0의 중복도(multiplicity) = 연결 요소 개수

**증명 스케치**:
- 각 연결 요소에서 상수 벡터가 고유벡터
- $k$개 연결 요소 → 0의 multiplicity $= k$ ∎

**고유값 정렬**:
$$
0 = \lambda_1 \le \lambda_2 \le \cdots \le \lambda_n
$$

### 2.4 Fiedler Value and Vector

**Fiedler value**: $\lambda_2$ (두 번째로 작은 고유값)
- **Algebraic connectivity**라고도 부름
- $\lambda_2 > 0$ $\Leftrightarrow$ 그래프가 연결

**Fiedler vector**: $\lambda_2$에 대응하는 고유벡터
- 노드를 두 그룹으로 분할하는 자연스러운 방법
- 성분 부호로 clustering: $\{i : v_i > 0\}$, $\{i : v_i < 0\}$

---

## 3. Normalized Laplacian

### 3.1 정의

**대칭 정규화**:
$$
L_{sym} = D^{-1/2} L D^{-1/2} = I - D^{-1/2} A D^{-1/2}
$$

**Random walk 정규화**:
$$
L_{rw} = D^{-1} L = I - D^{-1} A
$$

**동기**:
차수가 큰 노드가 지배하는 것을 방지

### 3.2 성질

**Normalized Laplacian의 고유값**: $0 \le \lambda_i \le 2$

**Cheeger's Inequality**:
$$
\frac{\lambda_2}{2} \le h(G) \le \sqrt{2 \lambda_2}
$$
- $h(G)$: Cheeger constant (최소 cut ratio)
- 스펙트럴 gap과 그래프 분할 품질의 관계

---

## 4. 스펙트럴 클러스터링

### 4.1 알고리즘

**목표**: 그래프를 $k$개 클러스터로 분할

**절차**:
1. Laplacian $L$ (또는 $L_{sym}$) 계산
2. 최소 $k$개 고유값에 대응하는 고유벡터 추출: $u_1, \ldots, u_k$
3. 각 노드 $i$를 $k$차원 벡터 $[u_1(i), \ldots, u_k(i)]$로 임베딩
4. K-means로 클러스터링

**예시 코드**:
```python
import numpy as np
from scipy.linalg import eigh
from sklearn.cluster import KMeans

def spectral_clustering(adj_matrix, k):
    """
    adj_matrix: adjacency matrix (numpy array)
    k: number of clusters
    """
    # Degree matrix
    D = np.diag(adj_matrix.sum(axis=1))

    # Laplacian
    L = D - adj_matrix

    # Eigendecomposition
    eigenvalues, eigenvectors = eigh(L)

    # Take first k eigenvectors
    U = eigenvectors[:, :k]

    # K-means
    kmeans = KMeans(n_clusters=k, random_state=0)
    labels = kmeans.fit_predict(U)

    return labels
```

### 4.2 이론적 근거

**Ratio cut**:
$$
\text{RatioCut} = \sum_{i=1}^k \frac{\text{cut}(C_i, \bar{C}_i)}{|C_i|}
$$

**Normalized cut** (Shi & Malik):
$$
\text{NCut} = \sum_{i=1}^k \frac{\text{cut}(C_i, \bar{C}_i)}{\text{vol}(C_i)}
$$

**정리**: Spectral clustering은 relaxed Ncut 문제의 해

---

## 5. Graph Signal Processing

### 5.1 그래프 Fourier 변환

**신호**: $f \in \mathbb{R}^n$ (각 노드에 값)

**Fourier basis**: Laplacian의 고유벡터 $u_1, \ldots, u_n$

**Graph Fourier Transform**:
$$
\hat{f} = U^\top f
$$
($\hat{f}_i = \langle f, u_i \rangle$: 고유벡터 $u_i$로의 투영)

**Inverse**:
$$
f = U \hat{f} = \sum_{i=1}^n \hat{f}_i u_i
$$

### 5.2 Spectral Filtering

**필터** $g(\lambda)$를 신호에 적용:
$$
g(L) f = U g(\Lambda) U^\top f
$$

**Low-pass filter** ($\lambda_2, \lambda_3, \ldots$ 제거):
$$
f_{\text{smooth}} = u_1 \hat{f}_1 = u_1 u_1^\top f
$$
(고주파 성분 제거 → smooth signal)

**응용**: Denoising, smoothing

---

## 6. Graph Neural Networks (GNN)

### 6.1 메시지 패싱 프레임워크

**핵심 아이디어**:
노드는 이웃으로부터 정보를 받아 자신의 표현을 업데이트

**일반 형태**:
$$
h_v^{(k+1)} = \text{UPDATE}^{(k)} \left( h_v^{(k)}, \text{AGGREGATE}^{(k)} \left( \{h_u^{(k)} : u \in \mathcal{N}(v)\} \right) \right)
$$
- $h_v^{(k)}$: 노드 $v$의 $k$ 번째 레이어 표현
- $\mathcal{N}(v)$: $v$의 이웃

**예시** (Simple GNN):
$$
h_v^{(k+1)} = \sigma \left( W^{(k)} \sum_{u \in \mathcal{N}(v)} \frac{h_u^{(k)}}{\sqrt{\deg(v) \deg(u)}} \right)
$$

### 6.2 Graph Convolutional Networks (GCN)

**Kipf & Welling (2017)**:
$$
H^{(k+1)} = \sigma \left( \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(k)} W^{(k)} \right)
$$
- $\tilde{A} = A + I$ (자기 루프 추가)
- $\tilde{D} = \text{diag}(\tilde{A} \mathbf{1})$

**행렬 형태**:
```python
import torch
import torch.nn.functional as F

class GCNLayer(torch.nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = torch.nn.Linear(in_features, out_features)

    def forward(self, x, adj):
        """
        x: [num_nodes, in_features]
        adj: [num_nodes, num_nodes] (normalized)
        """
        out = torch.mm(adj, x)  # Aggregate neighbors
        out = self.linear(out)  # Transform
        return F.relu(out)
```

**스펙트럴 해석**:
- GCN = 1차 Chebyshev 다항식 근사
- $g(\lambda) \approx \theta_0 + \theta_1 \lambda$ (스펙트럴 필터)

### 6.3 GraphSAGE

**Hamilton et al. (2017)**:
샘플링 기반 aggregation

$$
h_v^{(k+1)} = \sigma \left( W^{(k)} \cdot \text{CONCAT} \left( h_v^{(k)}, \text{AGG} \left( \{h_u^{(k)} : u \in \mathcal{N}(v)\} \right) \right) \right)
$$

**Aggregators**:
- Mean: $\frac{1}{|\mathcal{N}(v)|} \sum_{u \in \mathcal{N}(v)} h_u$
- Max: $\max_{u \in \mathcal{N}(v)} h_u$
- LSTM: permutation-invariant pooling

**PyTorch Geometric 구현**:
```python
from torch_geometric.nn import SAGEConv

class GraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return x
```

### 6.4 Graph Attention Networks (GAT)

**Veličković et al. (2018)**:
Attention mechanism으로 이웃 가중치 학습

$$
h_v^{(k+1)} = \sigma \left( \sum_{u \in \mathcal{N}(v)} \alpha_{vu} W^{(k)} h_u^{(k)} \right)
$$

**Attention 계수**:
$$
\alpha_{vu} = \frac{\exp(\text{LeakyReLU}(a^\top [W h_v \| W h_u]))}{\sum_{u' \in \mathcal{N}(v)} \exp(\text{LeakyReLU}(a^\top [W h_v \| W h_{u'}]))}
$$

**Multi-head attention**:
$$
h_v^{(k+1)} = \|_{m=1}^M \sigma \left( \sum_{u \in \mathcal{N}(v)} \alpha_{vu}^m W^m h_u^{(k)} \right)
$$

**구현**:
```python
from torch_geometric.nn import GATConv

class GAT(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads=8):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv2(x, edge_index)
        return x
```

---

## 7. GNN 실전 가이드

### 7.1 과적합 방지

**문제**: 그래프가 하나만 있으면 과적합 쉬움

**해결책**:
- **Dropout**: 노드 특징 또는 엣지에 적용
- **DropEdge**: 학습 시 엣지 랜덤 제거
- **Early stopping**: Validation loss 기준

**예시**:
```python
x = F.dropout(x, p=0.5, training=self.training)
edge_index = dropout_adj(edge_index, p=0.2, training=self.training)[0]
```

### 7.2 Over-smoothing 문제

**문제**: 레이어가 깊어지면 모든 노드 표현이 수렴

**원인**: $L$층 후 각 노드가 $L$-hop 이웃의 평균

**해결책**:
- **Residual connections**: $h^{(k+1)} = h^{(k)} + \text{GNN}^{(k)}(h^{(k)})$
- **Initial residual**: $h^{(k+1)} = h^{(0)} + \text{GNN}^{(k)}(h^{(k)})$
- **Jumping Knowledge**: 모든 레이어 출력 concat

```python
class ResGCN(torch.nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv = GCNConv(channels, channels)

    def forward(self, x, edge_index):
        return x + F.relu(self.conv(x, edge_index))  # Residual
```

### 7.3 그래프 레벨 작업

**Node classification**: 각 노드에 label
**Graph classification**: 전체 그래프에 label

**Global pooling** (그래프 → 벡터):
```python
from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool

# Mean pooling
graph_repr = global_mean_pool(x, batch)  # [num_graphs, hidden_dim]

# Max pooling
graph_repr = global_max_pool(x, batch)

# Sum pooling
graph_repr = global_add_pool(x, batch)
```

**Hierarchical pooling** (DiffPool, SAGPool):
```python
from torch_geometric.nn import SAGPooling

class GraphClassifier(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, num_classes):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.pool = SAGPooling(hidden_channels, ratio=0.5)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.lin = torch.nn.Linear(hidden_channels, num_classes)

    def forward(self, x, edge_index, batch):
        x = F.relu(self.conv1(x, edge_index))
        x, edge_index, _, batch, _, _ = self.pool(x, edge_index, batch=batch)
        x = F.relu(self.conv2(x, edge_index))
        x = global_mean_pool(x, batch)
        return self.lin(x)
```

---

## 8. 실습 과제

1. **Laplacian 고유값 분석**
   - Karate Club 그래프의 $L$ 계산
   - 고유값 plot, $\lambda_2$ (Fiedler value) 확인
   - Fiedler vector로 2-way 분할, ground truth와 비교

2. **Spectral Clustering**
   - Ring of cliques 그래프 생성 (5개 clique, 각 10 nodes)
   - Spectral clustering ($k=5$) 수행
   - Clustering 결과 시각화

3. **GCN 노드 분류**
   - Cora 데이터셋 로드
   - 2-layer GCN 학습 (64 hidden units)
   - Train/val/test accuracy 비교
   - 레이어 수 증가 시 over-smoothing 관찰

4. **GAT vs GCN 비교**
   - 같은 데이터셋에서 GAT와 GCN 학습
   - Attention 가중치 시각화
   - 성능 비교 (accuracy, training time)

5. **그래프 분류**
   - MUTAG 또는 PROTEINS 데이터셋
   - GCN + global pooling으로 그래프 분류기 학습
   - Different pooling (mean, max, sum) 비교

---

## 9. 참고 자료

### 교과서
- Fan R. K. Chung, *Spectral Graph Theory*
- Ulrike von Luxburg, "A Tutorial on Spectral Clustering" (2007)

### 논문
- **GCN**: Kipf & Welling (2017), "Semi-Supervised Classification with Graph Convolutional Networks"
- **GraphSAGE**: Hamilton et al. (2017), "Inductive Representation Learning on Large Graphs"
- **GAT**: Veličković et al. (2018), "Graph Attention Networks"
- **Spectral Clustering**: Ng, Jordan & Weiss (2002), "On Spectral Clustering"

### 온라인 강의
- Stanford CS224W: Machine Learning with Graphs
- Petar Veličković, "Theoretical Foundations of Graph Neural Networks" (YouTube)

### 라이브러리
- PyTorch Geometric: https://pytorch-geometric.readthedocs.io/
- DGL: https://www.dgl.ai/
- NetworkX: https://networkx.org/

---

## 10. 다음 학습

Spectral Graph Theory와 GNN을 마쳤다면:

1. **선형대수 심화**
   - [[Math/Linear Algebra/고유값과 고유벡터|고유값]]
   - [[Math/Linear Algebra/특잇값 분해|SVD]]
   - 스펙트럴 분해와 그래프 특성

2. **최적화 연결**
   - [[Math/Optimization/최적화 기초|최적화 기초]]
   - GNN 학습, graph cut 최적화

3. **실전 응용**
   - 추천 시스템 (bipartite graph)
   - 지식 그래프 임베딩
   - 분자 특성 예측

4. **상위 개념**
   - [[그래프 이론|그래프 이론 허브]]
   - [[AI 기초 개요|AI 기초 전체 로드맵]]
