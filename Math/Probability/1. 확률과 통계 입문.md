## TL;DR
- í™•ë¥ ê³¼ í†µê³„ëŠ” **ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”**í•˜ê³  **ë°ì´í„°ë¡œë¶€í„° ì¶”ë¡ **í•˜ëŠ” ìˆ˜í•™ì  ë„êµ¬ë¡œ, ë”¥ëŸ¬ë‹ ì „ë°˜ì— í•„ìˆ˜ì ì´ë‹¤.
- ì´ ë…¸íŠ¸ëŠ” í™•ë¥ í†µê³„ ì „ì²´ ë‚´ìš©ì˜ í—ˆë¸Œë¡œ, í™•ë¥  ê¸°ì´ˆ â†’ í†µê³„ ì¶”ì • â†’ ì •ë³´ ì´ë¡  ìˆœì„œë¡œ í•™ìŠµí•˜ëŠ” ë¡œë“œë§µì„ ì œê³µí•œë‹¤.
- ê° ì£¼ì œë³„ ìƒì„¸ ë…¸íŠ¸ë¡œ ì—°ê²°í•˜ì—¬ ì†ì‹¤ í•¨ìˆ˜, ë² ì´ì§€ì•ˆ ì¶”ë¡ , ìƒì„± ëª¨ë¸ì„ ì²´ê³„ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.

---

## í•™ìŠµ ë¡œë“œë§µ

| ìˆœì„œ | ì£¼ì œ | í•µì‹¬ ì§ˆë¬¸ | ë°”ë¡œ ê°€ê¸° |
|------|------|-----------|-----------|
| 1 | í™•ë¥  ê¸°ì´ˆ | í™•ë¥  ê³µê°„, ë¶„í¬, ê¸°ëŒ“ê°’, ë² ì´ì¦ˆ ì •ë¦¬ë€? | [[í™•ë¥  ê¸°ì´ˆ]] |
| 2 | í†µê³„ ì¶”ì • | MLE, MAP, ì‹ ë¢°êµ¬ê°„, ê°€ì„¤ ê²€ì •ì€? | [[í†µê³„ì  ì¶”ë¡ ]] |
| 3 | ì •ë³´ ì´ë¡  | ì—”íŠ¸ë¡œí”¼, KL ë°œì‚°, Mutual Informationì€? | [[ì •ë³´ ì´ë¡ ]] |

---

## 1. í™•ë¥  ê¸°ì´ˆ

### í•µì‹¬ ê°œë…
- **í™•ë¥  ê³µê°„**: $(\Omega, \mathcal{F}, P)$ - ê³µë¦¬ì  ì •ì˜
- **í™•ë¥  ë³€ìˆ˜**: $X: \Omega \rightarrow \mathbb{R}$
- **ë¶„í¬**: PMF (ì´ì‚°), PDF (ì—°ì†), CDF
- **ê¸°ëŒ“ê°’ê³¼ ë¶„ì‚°**: $\mathbb{E}[X]$, $\operatorname{Var}(X)$, $\operatorname{Cov}(X, Y)$
- **ì¡°ê±´ë¶€ í™•ë¥ **: $P(A|B) = P(A \cap B) / P(B)$
- **ë² ì´ì¦ˆ ì •ë¦¬**: $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$

### ì£¼ìš” ë¶„í¬
**ì´ì‚°**:
- ë² ë¥´ëˆ„ì´, ì´í•­, í¬ì•„ì†¡

**ì—°ì†**:
- ê· ë“±, ì •ê·œ, ì§€ìˆ˜

**ë‹¤ë³€ëŸ‰**:
- ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ $N(\boldsymbol{\mu}, \Sigma)$

### ì™œ ì¤‘ìš”í•œê°€?
- ë² ì´ì¦ˆ ì •ë¦¬ â†’ ì‚¬ì „/ì‚¬í›„ ë¶„í¬, MAP ì¶”ì •
- ë‹¤ë³€ëŸ‰ ì •ê·œ â†’ VAE latent space, Gaussian process
- ì¡°ê±´ë¶€ í™•ë¥  â†’ ì¡°ê±´ë¶€ ìƒì„± ëª¨ë¸

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[í™•ë¥  ê¸°ì´ˆ|í™•ë¥  ê¸°ì´ˆ ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- ë² ì´ì¦ˆ ì •ë¦¬: posterior âˆ likelihood Ã— prior
- $\operatorname{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]$

---

## 2. í†µê³„ ì¶”ì •ê³¼ ì¶”ë¡ 

### í•µì‹¬ ê°œë…
- **í‘œë³¸ í†µê³„ëŸ‰**: $\bar{X}$, $S^2$
- **ì¤‘ì‹¬ê·¹í•œì •ë¦¬**: $\bar{X}_n \xrightarrow{d} N(\mu, \sigma^2/n)$
- **MLE**: $\hat{\theta}_{\text{MLE}} = \arg\max L(\theta; \mathbf{x})$
- **MAP**: $\hat{\theta}_{\text{MAP}} = \arg\max [L(\theta) + \log p(\theta)]$
- **ì‹ ë¢°êµ¬ê°„**: $P(\theta \in [L, U]) = 1 - \alpha$
- **ê°€ì„¤ ê²€ì •**: $H_0$ vs $H_1$, $p$-value

### ì™œ ì¤‘ìš”í•œê°€?
- MLE = ì‹ ê²½ë§ í•™ìŠµ (negative log-likelihood ìµœì†Œí™”)
- MAP = MLE + regularization (weight decay)
- ì‹ ë¢°êµ¬ê°„ â†’ ëª¨ë¸ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
- ê°€ì„¤ ê²€ì • â†’ A/B í…ŒìŠ¤íŠ¸, ì„±ëŠ¥ ë¹„êµ

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[í†µê³„ì  ì¶”ë¡ |í†µê³„ ì¶”ì • ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- ì¤‘ì‹¬ê·¹í•œì •ë¦¬: í‘œë³¸ í‰ê· ì€ ê·¼ì‚¬ì ìœ¼ë¡œ ì •ê·œë¶„í¬
- CramÃ©r-Rao: $\operatorname{Var}(\hat{\theta}) \geq 1/(nI(\theta))$

---

## 3. ì •ë³´ ì´ë¡ 

### í•µì‹¬ ê°œë…
- **ì—”íŠ¸ë¡œí”¼**: $H(X) = -\sum p(x) \log p(x)$ (ë¶ˆí™•ì‹¤ì„±)
- **êµì°¨ ì—”íŠ¸ë¡œí”¼**: $H(p, q) = -\sum p(x) \log q(x)$ (ë¶„ë¥˜ ì†ì‹¤)
- **KL ë°œì‚°**: $D_{\text{KL}}(p \| q) = \sum p(x) \log \frac{p(x)}{q(x)}$ (ë¶„í¬ ì°¨ì´)
- **Mutual Information**: $I(X; Y) = H(X) - H(X|Y)$ (ê³µìœ  ì •ë³´)
- **ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼**: $H(X|Y)$

### ì™œ ì¤‘ìš”í•œê°€?
- Cross-Entropy Loss = í‘œì¤€ ë¶„ë¥˜ ì†ì‹¤
- KL ë°œì‚° â†’ VAE ì •ê·œí™” í•­, distillation
- Mutual Information â†’ í‘œí˜„ í•™ìŠµ, contrastive learning
- ì •ë³´ ë³‘ëª© â†’ ì¼ë°˜í™” ì´ë¡ 

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[ì •ë³´ ì´ë¡ |ì •ë³´ ì´ë¡  ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- $H(p, q) = H(p) + D_{\text{KL}}(p \| q)$
- $I(X; Y) = D_{\text{KL}}(p(x,y) \| p(x)p(y))$

---

## ì£¼ì œë³„ ì—°ê²°

### ê°œë… ê°„ ê´€ê³„ë„

```
í™•ë¥  ê³µê°„ â”€â”€â”€â”€â”€â”€> í™•ë¥  ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€> ë¶„í¬ (PMF/PDF)
    â”‚                                   â”‚
    â–¼                                   â–¼
ì¡°ê±´ë¶€ í™•ë¥  â”€â”€â”€> ë² ì´ì¦ˆ ì •ë¦¬ â”€â”€â”€> ì‚¬ì „/ì‚¬í›„ë¶„í¬
    â”‚                                   â”‚
    â–¼                                   â–¼
í‘œë³¸ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€> í†µê³„ëŸ‰ â”€â”€â”€â”€â”€â”€> ì¶”ì • (MLE/MAP)
    â”‚                                   â”‚
    â–¼                                   â–¼
ì¤‘ì‹¬ê·¹í•œì •ë¦¬ â”€â”€â”€> ì‹ ë¢°êµ¬ê°„/ê²€ì • â”€â”€â”€> ì¶”ë¡ 
                                        â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
        ì •ë³´ ì´ë¡  â”€â”€â”€> ì—”íŠ¸ë¡œí”¼
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”
        â–¼       â–¼       â–¼
       KL    Cross-   Mutual
            Entropy  Information
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”
        â–¼       â–¼       â–¼
      Loss    VAE   Contrastive
     Function      Learning
```

### ë”¥ëŸ¬ë‹ ì†ì‹¤ í•¨ìˆ˜ ì—°ê²°

| ì†ì‹¤ í•¨ìˆ˜ | í™•ë¥ /ì •ë³´ ì´ë¡  | ìˆ˜ì‹ |
|----------|--------------|------|
| **Cross-Entropy** | êµì°¨ ì—”íŠ¸ë¡œí”¼ | $-\sum y \log \hat{y}$ |
| **NLL (Negative Log-Likelihood)** | MLE | $-\sum \log p(y_i \mid x_i; \theta)$ |
| **KL Loss** | KL ë°œì‚° | $\sum p \log(p/q)$ |
| **MSE** | ì •ê·œë¶„í¬ MLE | $(y - \hat{y})^2$ |
| **BCE (Binary Cross-Entropy)** | ë² ë¥´ëˆ„ì´ MLE | $-[y \log \hat{y} + (1-y) \log(1-\hat{y})]$ |

---

## ë¹ ë¥¸ ì°¸ì¡°

### ìì£¼ ì“°ëŠ” ê³µì‹

**í™•ë¥ **:
- ë² ì´ì¦ˆ: $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$
- ì „í™•ë¥ : $P(A) = \sum_i P(A|B_i)P(B_i)$
- ë¶„ì‚° ë¶„í•´: $\operatorname{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$

**ì£¼ìš” ë¶„í¬**:
- ì •ê·œ: $N(\mu, \sigma^2)$, PDF = $\frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu)^2/(2\sigma^2)}$
- ë² ë¥´ëˆ„ì´: $\mathbb{E}[X] = p$, $\operatorname{Var}(X) = p(1-p)$

**í†µê³„**:
- MLE: $\arg\max_{\theta} \prod p(x_i; \theta)$
- í‘œë³¸ í‰ê·  ë¶„ì‚°: $\operatorname{Var}(\bar{X}) = \sigma^2/n$
- 95% CI: $\bar{x} \pm 1.96\sigma/\sqrt{n}$

**ì •ë³´ ì´ë¡ **:
- ì—”íŠ¸ë¡œí”¼: $H(X) = -\sum p \log p$
- KL: $D_{\text{KL}}(p \| q) = \sum p \log(p/q)$
- MI: $I(X; Y) = H(X) - H(X|Y)$

---

## ì‹¤ìŠµ ê°€ì´ë“œ

### PyTorch ì£¼ìš” í•¨ìˆ˜

```python
import torch
from torch.distributions import *

# ë¶„í¬ ìƒì„±
normal = Normal(loc=0.0, scale=1.0)
bernoulli = Bernoulli(probs=0.5)
categorical = Categorical(probs=torch.tensor([0.25, 0.75]))

# ìƒ˜í”Œë§
samples = normal.sample((1000,))

# ë¡œê·¸ í™•ë¥ 
log_prob = normal.log_prob(samples)

# KL ë°œì‚°
from torch.distributions import kl_divergence
p = Normal(0.0, 1.0)
q = Normal(1.0, 2.0)
kl = kl_divergence(p, q)

# Cross-Entropy Loss
import torch.nn.functional as F
loss = F.cross_entropy(logits, targets)

# BCE Loss
bce = F.binary_cross_entropy(predictions, labels)
```

### í•™ìŠµ ì „ëµ

1. **í™•ë¥  ê¸°ì´ˆë¶€í„°**
   - ë² ì´ì¦ˆ ì •ë¦¬ ì™„ë²½ ì´í•´
   - ì£¼ìš” ë¶„í¬ì˜ íŠ¹ì„± ì•”ê¸°
   - ì¡°ê±´ë¶€ í™•ë¥  ì—°ìŠµ

2. **í†µê³„ ì¶”ì •**
   - MLE ì†ê³„ì‚° ì—°ìŠµ
   - ì¤‘ì‹¬ê·¹í•œì •ë¦¬ ì‹œë®¬ë ˆì´ì…˜
   - ì‹ ë¢°êµ¬ê°„ ì§ì ‘ êµ¬ì„±

3. **ì •ë³´ ì´ë¡ **
   - ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ë°˜ë³µ
   - KL ë°œì‚° ë¹„ëŒ€ì¹­ì„± ì²´ê°
   - Cross-Entropy Loss ìœ ë„

4. **ë”¥ëŸ¬ë‹ ì—°ê²°**
   - ê° ì†ì‹¤ í•¨ìˆ˜ì˜ í™•ë¥ ì  í•´ì„
   - MAP = MLE + regularization ì´í•´
   - VAE, GAN ë“±ì˜ ì´ë¡ ì  í† ëŒ€

---

## ë”¥ëŸ¬ë‹ ì—°ê²° í¬ì¸íŠ¸

### ì–´ë””ì— ì“°ì´ë‚˜?

| ë”¥ëŸ¬ë‹ ê°œë… | í™•ë¥ /í†µê³„ ë„êµ¬ | ê´€ë ¨ ë…¸íŠ¸ |
|-----------|--------------|----------|
| ë¶„ë¥˜ ì†ì‹¤ | Cross-Entropy | [[ì •ë³´ ì´ë¡ ]] |
| íšŒê·€ ì†ì‹¤ (MSE) | ì •ê·œë¶„í¬ MLE | [[í†µê³„ì  ì¶”ë¡ ]] |
| Weight decay | MAP (ì •ê·œ ì‚¬ì „ë¶„í¬) | [[í†µê³„ì  ì¶”ë¡ ]] |
| Dropout | ë² ë¥´ëˆ„ì´ ìƒ˜í”Œë§ | [[í™•ë¥  ê¸°ì´ˆ]] |
| Batch Normalization | í‘œë³¸ í‰ê· /ë¶„ì‚° | [[í†µê³„ì  ì¶”ë¡ ]] |
| VAE | KL ë°œì‚°, ë² ì´ì¦ˆ ì¶”ë¡  | [[ì •ë³´ ì´ë¡ ]] |
| GAN | Jensen-Shannon ë°œì‚° | [[ì •ë³´ ì´ë¡ ]] |
| Contrastive Learning | Mutual Information | [[ì •ë³´ ì´ë¡ ]] |
| Bayesian Deep Learning | ë² ì´ì¦ˆ ì •ë¦¬, MAP | [[í†µê³„ì  ì¶”ë¡ ]] |

---

## ì°¸ê³  ìë£Œ

### êµê³¼ì„œ
- Sheldon Ross, *A First Course in Probability*
- Casella & Berger, *Statistical Inference*
- Cover & Thomas, *Elements of Information Theory*
- Kevin P. Murphy, *Probabilistic Machine Learning*

### ì˜¨ë¼ì¸ ê°•ì˜
- MIT OCW 6.041 Probabilistic Systems Analysis
- Stanford CS229 Probability Review
- 3Blue1Brown, Probability Series

### ë”¥ëŸ¬ë‹ ê´€ì 
- Ian Goodfellow et al., *Deep Learning*, Chapter 3
- Christopher Bishop, *Pattern Recognition and Machine Learning*

---

## ë‹¤ìŒ í•™ìŠµ

í™•ë¥ í†µê³„ ê¸°ì´ˆë¥¼ ë§ˆì³¤ë‹¤ë©´:

1. **ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ ì—°ê²°**
   - [[ML Foundations/ì†ì‹¤ í•¨ìˆ˜ì™€ í‰ê°€ ì§€í‘œ|ì†ì‹¤ í•¨ìˆ˜ì™€ í‰ê°€]]
   - [[ML Foundations/í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„|í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„]]

2. **ìµœì í™”ì™€ ìœµí•©**
   - [[Math/Optimization/ìµœì í™” ê¸°ì´ˆ|ìµœì í™” ê¸°ì´ˆ]]
   - MLE/MAPì™€ gradient descent

3. **ê³ ê¸‰ ì£¼ì œ**
   - [[ML Foundations/ì •ê·œí™”ì™€ ì¼ë°˜í™”|ì •ê·œí™”ì™€ ì¼ë°˜í™”]]
   - Bayesian Deep Learning

4. **ìƒìœ„ ê°œë…**
   - [[AI ê¸°ì´ˆ ê°œìš”|AI ê¸°ì´ˆ ì „ì²´ ë¡œë“œë§µ]]
