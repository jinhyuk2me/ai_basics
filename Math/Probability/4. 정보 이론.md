## TL;DR
- 정보 이론(Information Theory)은 불확실성과 정보량을 정량화하는 수학적 체계로, 딥러닝의 손실 함수와 모델 평가의 이론적 토대다.
- 엔트로피는 불확실성의 척도, KL 발산은 분포 간 차이, 교차 엔트로피는 분류 문제의 손실 함수로 핵심적이다.
- Mutual Information, Channel Capacity 등은 표현 학습, 정보 병목, VAE 등 고급 딥러닝 기법에서 등장한다.

---

## 1. 엔트로피 (Entropy)

### 1.1 정의

**섀넌 엔트로피(Shannon Entropy)**: 이산 확률 변수 $X \sim p$의 불확실성
$$
H(X) = -\sum_{x} p(x) \log p(x) = -\mathbb{E}_{x \sim p}[\log p(x)]
$$

**규약**: $0 \log 0 = 0$

**단위**: 로그 밑에 따라
- $\log_2$: bits
- $\log_e$: nats (딥러닝에서 주로 사용)

### 1.2 직관

**정보량**: 사건 $x$가 발생했을 때의 놀라움(surprise)
$$
I(x) = -\log p(x)
$$

- 확률이 낮으면 ($p \to 0$) 정보량 큼 ($I \to \infty$)
- 확률이 높으면 ($p \to 1$) 정보량 작음 ($I \to 0$)

**엔트로피 = 평균 정보량**

### 1.3 예제

**예제 1 (동전)**:
- 공정한 동전: $p(H) = p(T) = 0.5$
$$
H = -0.5 \log_2 0.5 - 0.5 \log_2 0.5 = 1 \text{ bit}
$$

- 편향된 동전: $p(H) = 0.9$, $p(T) = 0.1$
$$
H = -0.9 \log_2 0.9 - 0.1 \log_2 0.1 \approx 0.47 \text{ bits}
$$

불확실성 감소!

**예제 2 (주사위)**:
$$
H = -6 \times \frac{1}{6}\log_2 \frac{1}{6} = \log_2 6 \approx 2.58 \text{ bits}
$$

### 1.4 성질

1. **비음수성**: $H(X) \geq 0$
2. **최댓값**: 균등분포일 때 최대
   $$H(X) \leq \log |X|$$
   (등호는 $p(x) = 1/|X|$일 때)
3. **최솟값**: 확정적일 때 0
   $$p(x_0) = 1 \Rightarrow H(X) = 0$$

---

## 2. 교차 엔트로피 (Cross-Entropy)

### 2.1 정의

**교차 엔트로피**: 진짜 분포 $p$를 모델 $q$로 근사할 때의 평균 정보량
$$
H(p, q) = -\sum_x p(x) \log q(x) = -\mathbb{E}_{x \sim p}[\log q(x)]
$$

### 2.2 성질

$$
H(p, q) = H(p) + D_{\text{KL}}(p \| q)
$$

따라서:
$$
H(p, q) \geq H(p)
$$

등호는 $p = q$일 때.

### 2.3 딥러닝 손실 함수

**분류 문제**: 진짜 라벨 $y$, 모델 예측 $\hat{y}$

$$
\mathcal{L} = -\sum_{c=1}^C y_c \log \hat{y}_c
$$

**이진 분류**:
$$
\mathcal{L} = -[y \log \hat{y} + (1-y) \log(1 - \hat{y})]
$$

**PyTorch**: `nn.CrossEntropyLoss`는 softmax + cross-entropy 결합

---

## 3. KL 발산 (Kullback-Leibler Divergence)

### 3.1 정의

**KL 발산**: 분포 $p$에서 $q$로의 상대 엔트로피
$$
D_{\text{KL}}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = \mathbb{E}_{x \sim p}\left[ \log \frac{p(x)}{q(x)} \right]
$$

연속:
$$
D_{\text{KL}}(p \| q) = \int p(x) \log \frac{p(x)}{q(x)}\,dx
$$

### 3.2 성질

1. **비음수성**: $D_{\text{KL}}(p \| q) \geq 0$
2. **등호 조건**: $D_{\text{KL}}(p \| q) = 0 \Leftrightarrow p = q$ (거의 확실히)
3. **비대칭**: $D_{\text{KL}}(p \| q) \neq D_{\text{KL}}(q \| p)$ (거리가 아님!)

**증명 (Gibbs' inequality)**:
$$
D_{\text{KL}}(p \| q) = -\sum p(x) \log \frac{q(x)}{p(x)} \geq -\log \sum p(x) \frac{q(x)}{p(x)} = -\log 1 = 0
$$
(Jensen's inequality 사용) ∎

### 3.3 예제

**예제**: $p = \text{Bern}(0.6)$, $q = \text{Bern}(0.4)$
$$
D_{\text{KL}}(p \| q) = 0.6 \log \frac{0.6}{0.4} + 0.4 \log \frac{0.4}{0.6}
$$
$$
= 0.6 \log 1.5 + 0.4 \log 0.667 \approx 0.081 \text{ nats}
$$

$$
D_{\text{KL}}(q \| p) = 0.4 \log \frac{0.4}{0.6} + 0.6 \log \frac{0.6}{0.4} \approx 0.081
$$

이 경우 대칭이지만 일반적으로는 아님!

### 3.4 딥러닝 응용

**VAE 손실**:
$$
\mathcal{L} = -\mathbb{E}_{z \sim q}[\log p(x|z)] + D_{\text{KL}}(q(z|x) \| p(z))
$$

**Knowledge Distillation**:
$$
\mathcal{L}_{\text{KD}} = D_{\text{KL}}(p_{\text{teacher}} \| p_{\text{student}})
$$

---

## 4. 조건부 엔트로피와 Mutual Information

### 4.1 조건부 엔트로피

**정의**: $Y$가 주어졌을 때 $X$의 불확실성
$$
H(X|Y) = \sum_y p(y) H(X|Y=y) = -\sum_{x,y} p(x, y) \log p(x|y)
$$

**체인 규칙**:
$$
H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
$$

**성질**:
$$
H(X|Y) \leq H(X)
$$
(조건이 주어지면 불확실성 감소)

### 4.2 Mutual Information

**정의**: $X$와 $Y$가 공유하는 정보량
$$
I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
$$

다르게 표현:
$$
I(X; Y) = D_{\text{KL}}(p(x,y) \| p(x)p(y))
$$

**대칭성**: $I(X; Y) = I(Y; X)$

**성질**:
- $I(X; Y) \geq 0$
- $I(X; Y) = 0 \Leftrightarrow X \perp Y$ (독립)
- $I(X; X) = H(X)$ (자기 자신과는 모든 정보 공유)

### 4.3 딥러닝 응용

**표현 학습**:
$$
\max I(X; Z) \quad \text{s.t.} \quad I(Y; Z) \text{ maximized}
$$

**InfoNCE** (Contrastive Learning):
$$
\mathcal{L}_{\text{InfoNCE}} = -\mathbb{E}\left[ \log \frac{e^{f(x, x^+)}}{\sum_{x'} e^{f(x, x')}} \right]
$$

MI 최대화의 하한.

---

## 5. 정보 병목 (Information Bottleneck)

### 5.1 원리

**목표**: 입력 $X$에서 중요한 정보만 추출한 표현 $Z$
$$
\min_{p(z|x)} I(X; Z) - \beta I(Y; Z)
$$

- $I(X; Z)$: 압축 (작게)
- $I(Y; Z)$: 예측 성능 (크게)
- $\beta$: trade-off

### 5.2 딥러닝 해석

신경망 학습:
1. **Fitting phase**: $I(Y; Z)$ 증가
2. **Compression phase**: $I(X; Z)$ 감소

일반화 = 불필요한 정보 제거!

---

## 6. 연속 엔트로피 (Differential Entropy)

### 6.1 정의

**연속 엔트로피**:
$$
h(X) = -\int f(x) \log f(x)\,dx
$$

**주의**: 음수 가능! (확률 밀도 > 1 허용)

### 6.2 예제

**균등분포** $U(a, b)$:
$$
h(X) = \log(b - a)
$$

**정규분포** $N(\mu, \sigma^2)$:
$$
h(X) = \frac{1}{2}\log(2\pi e \sigma^2)
$$

### 6.3 최대 엔트로피 원리

**정리**: 평균과 분산이 고정되었을 때, 정규분포가 최대 엔트로피.

**응용**: 제약 조건만 있고 다른 정보 없으면 → 정규분포 가정이 합리적

---

## 7. PyTorch 실습

```python
import torch
import torch.nn.functional as F

# 1. 엔트로피 계산
def entropy(p):
    """Shannon entropy H(p)"""
    return -(p * torch.log(p + 1e-10)).sum()

p1 = torch.tensor([0.5, 0.5])  # 균등
p2 = torch.tensor([0.9, 0.1])  # 편향

print(f"H(uniform) = {entropy(p1).item():.3f} nats")
print(f"H(biased) = {entropy(p2).item():.3f} nats")

# 2. 교차 엔트로피
def cross_entropy(p, q):
    """H(p, q)"""
    return -(p * torch.log(q + 1e-10)).sum()

p_true = torch.tensor([0.0, 1.0, 0.0])  # one-hot
q_pred = torch.tensor([0.1, 0.8, 0.1])  # softmax output

ce = cross_entropy(p_true, q_pred)
print(f"Cross-entropy: {ce.item():.3f}")

# PyTorch 내장
ce_pytorch = F.cross_entropy(
    torch.log(q_pred).unsqueeze(0),  # log probabilities
    torch.tensor([1])  # target class
)
print(f"PyTorch CE: {ce_pytorch.item():.3f}")

# 3. KL 발산
def kl_divergence(p, q):
    """KL(p || q)"""
    return (p * torch.log((p + 1e-10) / (q + 1e-10))).sum()

p = torch.tensor([0.6, 0.4])
q = torch.tensor([0.4, 0.6])

kl_pq = kl_divergence(p, q)
kl_qp = kl_divergence(q, p)

print(f"KL(p||q) = {kl_pq.item():.4f}")
print(f"KL(q||p) = {kl_qp.item():.4f}")
print(f"비대칭성: {abs(kl_pq - kl_qp).item():.4f}")

# H(p, q) = H(p) + KL(p||q) 검증
h_p = entropy(p)
h_pq = cross_entropy(p, q)
print(f"H(p) + KL(p||q) = {(h_p + kl_pq).item():.4f}")
print(f"H(p, q) = {h_pq.item():.4f}")

# 4. Mutual Information 추정 (이산)
def mutual_information(pxy):
    """I(X;Y) for joint distribution"""
    px = pxy.sum(dim=1, keepdim=True)
    py = pxy.sum(dim=0, keepdim=True)
    pxpy = px @ py

    # I(X;Y) = KL(p(x,y) || p(x)p(y))
    return (pxy * torch.log((pxy + 1e-10) / (pxpy + 1e-10))).sum()

# 독립
pxy_indep = torch.tensor([[0.25, 0.25], [0.25, 0.25]])
print(f"I(X;Y) 독립: {mutual_information(pxy_indep).item():.4f}")

# 종속
pxy_dep = torch.tensor([[0.4, 0.1], [0.1, 0.4]])
print(f"I(X;Y) 종속: {mutual_information(pxy_dep).item():.4f}")

# 5. 정규분포의 differential entropy
def differential_entropy_normal(sigma):
    """h(N(0, σ²))"""
    return 0.5 * torch.log(2 * torch.pi * torch.e * sigma**2)

for sigma in [1.0, 2.0, 0.5]:
    h = differential_entropy_normal(torch.tensor(sigma))
    print(f"h(N(0, {sigma}²)) = {h.item():.3f} nats")

# 6. KL between Gaussians
def kl_gaussian(mu1, sigma1, mu2, sigma2):
    """KL(N(μ₁,σ₁²) || N(μ₂,σ₂²))"""
    return (torch.log(sigma2/sigma1) +
            (sigma1**2 + (mu1 - mu2)**2) / (2*sigma2**2) - 0.5)

kl = kl_gaussian(
    torch.tensor(0.0), torch.tensor(1.0),
    torch.tensor(1.0), torch.tensor(2.0)
)
print(f"KL(N(0,1) || N(1,4)) = {kl.item():.3f}")

# PyTorch distributions
from torch.distributions import Normal, kl_divergence as kl_div

p = Normal(0.0, 1.0)
q = Normal(1.0, 2.0)
kl_pytorch = kl_div(p, q)
print(f"PyTorch KL: {kl_pytorch.item():.3f}")
```

---

## 8. 연습 문제

1. **엔트로피**
   주사위에서 짝수가 나올 확률이 2/3, 홀수가 1/3일 때 엔트로피를 계산하라.

2. **KL 발산**
   $p = N(0, 1)$, $q = N(\mu, 1)$일 때, $D_{\text{KL}}(p \| q)$를 $\mu$의 함수로 나타내라.

3. **Mutual Information**
   $X, Y$가 독립이면 $I(X; Y) = 0$임을 증명하라.

4. **체인 규칙**
   $H(X, Y, Z) = H(X) + H(Y|X) + H(Z|X,Y)$임을 보여라.

5. **최대 엔트로피**
   $n$개 값을 갖는 이산 분포의 최대 엔트로피는 $\log n$임을 라그랑주 승수로 증명하라.

---

## 9. 참고 자료

- Thomas M. Cover & Joy A. Thomas, *Elements of Information Theory*
- David MacKay, *Information Theory, Inference, and Learning Algorithms*
- Kolchinsky & Tracey, "Estimating Mutual Information" (NeurIPS 2017)
- Tishby & Zaslavsky, "Deep Learning and the Information Bottleneck Principle" (2015)

---

## 10. 다음 학습

- [[Math/Probability/2. 확률 기초|확률 기초]]
- [[Math/Probability/3. 통계적 추론|통계 추정]]
- [[ML Foundations/2. 손실 함수와 평가 지표|손실 함수]] (Cross-Entropy Loss)
- [[Math/Probability/1. 확률과 통계 입문|확률통계 전체 개요]]
