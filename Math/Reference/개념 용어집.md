## TL;DR
- 이 글은 고등학교 수준에서는 접하기 어려운 수학·선형대수·미적분 용어를 직관적인 설명, 숫자 예시, 간단한 그림 묘사, PyTorch 코드로 한 번에 정리한 참고서입니다.
- 정의만 나열하지 않고 “왜 필요한가, 어디에 쓰이지?”에 초점을 맞춰 딥러닝 문맥에서의 활용 예를 함께 제공합니다.
- 모르는 단어가 생기면 검색 전에 이 노트를 먼저 확인하고, 보다 자세한 내용은 각 학습 노트(선형대수, 미적분 등)로 이동하세요.

---

## 1. 기본 객체: 스칼라, 벡터, 행렬, 텐서

### 스칼라 (Scalar)
- 한 개의 수(숫자)로 표현되는 양. 예: 온도 25℃, 확률 0.7.
- 딥러닝에서 손실(loss), 학습률(learning rate) 같은 값이 스칼라입니다.

### 벡터 (Vector)
- 크기와 방향을 가진 값. 좌표 `(x, y)`처럼 여러 숫자를 일정한 순서로 모아 놓은 것.
- **예시**: 이미지 분류에서 한 장의 이미지(픽셀 784개)를 `[0.1, 0.4, …, 0.0]` 벡터로 표현.
- **시각화**: 평면 위 점(2, 1)은 원점에서 x 방향으로 2, y 방향으로 1만큼 이동한 화살표로 생각.

### 행렬 (Matrix)
- 숫자를 직사각형 그리드로 배치한 것. 여러 벡터를 한 번에 변환하는 선형 연산을 표현.
- **예시**: 2×2 행렬
  
  $$
  A = \begin{bmatrix} 2 & 1 \\ 0 & 3 \end{bmatrix}
  $$
  
  는 벡터 $(x, y)$를 $(2x + y,\; 3y)$로 바꾼다. 첫 행은 x 방향 변환, 두 번째 행은 y 방향 변환이다.

### 텐서 (Tensor)
- 벡터·행렬을 더 일반화한 다차원 배열. PyTorch에서 `Tensor`가 기본 데이터형.
- **예시**: RGB 이미지 한 장은 높이 × 폭 × 채널 = (H, W, 3) 텐서. 배치(batch)를 묶으면 (Batch, Channel, H, W).

---

## 2. 함수 종류와 표현 방식

| 용어 | 설명 | 예시 |
| --- | --- | --- |
| 스칼라 함수 | 벡터 입력 → 스칼라 출력 | $f(x, y) = x^2 + y^2$ (손실 함수와 유사) |
| 벡터 값 함수 | 벡터 입력 → 벡터 출력 | $F(x, y) = (2x + y,\; x - y)$ (좌표 변환) |
| 다변수 함수 | 여러 변수를 입력으로 받는 함수 | $g(x, y, z)$ |

> **꿀팁** / **기호 읽기**: 흔히 등장하는 $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ 표기는 “$n$차원 벡터를 입력 받아 $m$차원 벡터를 출력하는 함수”라는 뜻입니다.

---

## 3. 미분 관련 핵심 용어

### 편미분 (Partial Derivative)
- 여러 변수 중 하나만 변한다고 가정하고 미분한 것.
- $\frac{\partial f}{\partial x}$ 는 “$y$를 고정하고 $x$ 방향으로 조금 움직였을 때 $f$가 얼마만큼 변하는지”를 뜻합니다.
- **예시**: $f(x, y) = x^2 y$ 이면 $\frac{\partial f}{\partial x} = 2xy$, $\frac{\partial f}{\partial y} = x^2$.

### Gradient (그라디언트)
- 스칼라 함수의 모든 편미분을 모은 벡터. $\nabla f = [\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n}]$.
- “이 방향으로 움직이면 $f$가 가장 빠르게 증가한다”는 방향과 크기를 알려줍니다.
- 딥러닝에서 `loss.backward()`가 gradient를 계산해 파라미터를 업데이트합니다.

### Directional Derivative (방향 미분)
- 특정 방향 $u$로 움직였을 때 $f$가 얼마나 변하는지.
- $D_u f(x) = \nabla f(x) \cdot u$ (gradient와 방향 벡터의 내적).

### Jacobian (자코비안)
- 벡터 함수의 “미분표”. 출력의 각 원소가 입력의 각 원소에 어떻게 의존하는지 행렬로 나열.
- 
  $$
  J = \begin{bmatrix}
  \frac{\partial F_1}{\partial x_1} & \dots & \frac{\partial F_1}{\partial x_n} \\
  \vdots & \ddots & \vdots \\
  \frac{\partial F_m}{\partial x_1} & \dots & \frac{\partial F_m}{\partial x_n}
  \end{bmatrix}
  $$
- **예시**: $F(x, y) = (x^2,\; xy)$ → Jacobian은 $\begin{bmatrix} 2x & 0 \\ y & x \end{bmatrix}$.
- 신경망에서 레이어 출력에 대한 입력의 미분이 Jacobian이며, backpropagation의 중간 결과입니다.

### Hessian (헤시안)
- 스칼라 함수의 2차 편미분을 모은 행렬. 곡률(curvature)을 분석하는 도구.
- 
  $$
  H = \begin{bmatrix}
  \frac{\partial^2 f}{\partial x_1^2} & \dots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
  \vdots & \ddots & \vdots \\
  \frac{\partial^2 f}{\partial x_n \partial x_1} & \dots & \frac{\partial^2 f}{\partial x_n^2}
  \end{bmatrix}
  $$
- Hessian 고유값(λ)이 모두 양수 → 그 지점은 최소점, 모두 음수 → 최대점, 양/음 섞이면 saddle point.
- 2차 최적화(Newton method)나 curvature 기반 regularization에서 사용.

### Chain Rule (체인룰)
- 합성 함수의 미분 법칙: $f(g(x))$의 미분은 $f'(g(x)) \cdot g'(x)$.
- 딥러닝의 backpropagation은 거대한 체인룰입니다. 각 레이어가 다음 레이어에 gradient를 전달.

> **Visualization tip**: 3D에서 `f(x, y)`의 그래프를 언덕처럼 상상하면, gradient는 가장 가파르게 오르는 방향의 화살표, Hessian은 그 언덕이 얼마나 구부러져 있는지 말해줍니다.

### PyTorch로 직접 확인해 보기
```python
import torch
from torch.autograd.functional import jacobian, hessian

x = torch.tensor([2.0, 1.0], requires_grad=True)

def scalar_fn(v):
    return v[0]**2 + v[0] * v[1]

def vector_fn(v):
    return torch.stack([v[0]**2, v[0] * v[1]])

grad = torch.autograd.grad(scalar_fn(x), x, create_graph=True)[0]
print("gradient:", grad)        # [5., 2.]

J = jacobian(vector_fn, x)
print("Jacobian:\n", J)

H = hessian(scalar_fn, x)
print("Hessian:\n", H)
```

---

## 4. 선형대수에서 자주 쓰는 개념

### Inner Product (내적)
- 두 벡터의 “유사성”을 측정: `a · b = ||a|| ||b|| cosθ`.
- 0이면 서로 수직(orthogonal). 딥러닝에서 attention, embedding 유사도 계산에 사용.

### Norm (노름)
- 벡터의 크기를 재는 함수. L2 노름(`√(x₁² + … + xₙ²)`)이 가장 흔함.
- L1 노름(`|x₁| + … + |xₙ|`)은 희소성(sparsity)을 만들 때 유용.

### Orthogonality (직교)
- 벡터가 서로 수직, 내적이 0. 직교 기저는 서로 독립적인 축으로 생각.
- CNN 필터를 직교화하면 서로 다른 특징을 학습하도록 돕는다.

### Eigenvalue / Eigenvector (고유값 / 고유벡터)
- 선형 변환 A에 대해 방향은 유지되고 길이만 λ배 되는 벡터.
- PCA는 공분산 행렬의 고유벡터를 사용해 데이터를 가장 잘 설명하는 방향을 찾는다.

### Singular Value (특이값)
- SVD에서 나타나는 Σ의 대각 원소. 행렬이 데이터를 어떻게 늘리고 줄이는지 측정.
- 큰 특이값이 많으면 정보가 풍부하지만, 노이즈 포함 가능성이 커 regularization 필요.

---

## 5. 최적화·기하 용어

### Convex / Concave (볼록 / 오목)
- 볼록 함수 f: 임의의 두 점을 잇는 선분이 그래프 위에 있다. 전역 최소와 지역 최소가 일치.
- 딥러닝 손실은 대부분 비볼록이지만, 가슴역 경험상 큰 모델은 평평한 영역을 찾아 일반화가 잘 된다.

### Saddle Point (안장점)
- gradient=0이지만 최소/최대가 아닌 점. x 방향은 오르막, y 방향은 내리막.
- 딥러닝에서 학습이 멈춘 것처럼 보이나, noise로 빠져나갈 수 있다.

### Manifold (다양체)
- 고차원 공간 속에 숨어 있는 저차원 구조. 이미지나 문장 임베딩이 실제로는 낮은 차원의 다양체 위에 있다고 가정.

### Lipschitz Continuity (립시츠 연속)
- 함수 f가 입력 간 거리의 λ배 이상으로 급격히 변하지 않으면 Lipschitz constant λ를 가진다고 말함.
- GAN 안정화, adversarial training 등에서 활용.

### Convex Combination (볼록 결합)
- 가중치 합이 1이고 모두 비음수인 선형 결합. 확률 혼합과 유사.
- 예: Mixup 데이터 증강은 두 이미지를 볼록 결합하여 새 샘플을 만듦.

---

## 6. 자주 묻는 질문 (FAQ)

**Q1. 편미분과 gradient는 어떻게 다르나요?**
- 편미분은 한 변수만 변할 때의 변화율, gradient는 모든 편미분을 모아 “어느 방향으로 움직일지” 한눈에 보여주는 벡터입니다.

**Q2. Jacobian과 Hessian은 언제 써요?**
- Jacobian: 벡터 출력 함수의 local linear approximation, backpropagation/AutoDiff 기본 단위.
- Hessian: 곡률 정보, 2차 최적화, 불안정성 분석, Sharpness-aware minimization 등.

**Q3. Tensor와 Matrix 차이는?**
- 행렬은 2차원 배열, Tensor는 차원수 제한이 없습니다. Matrix ⊂ Tensor.

**Q4. Convex가 왜 중요해요?**
- Convex problem은 local minimum=global minimum → 이론적으로 안정. 비볼록 문제라도 convex 근사로 이해하려는 시도가 많습니다.

**Q5. Lipschitz 조건은 왜 등장하나요?**
- adversarial example에서 입력을 조금 바꿔도 출력이 급격히 변하지 않게 하려면 Lipschitz constant가 작아야 합니다.