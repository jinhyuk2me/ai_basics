## TL;DR
- ìµœì í™”(Optimization)ëŠ” **ë¹„ìš© í•¨ìˆ˜ $f(x)$ë¥¼ ìµœì†Œí™”(ë˜ëŠ” ìµœëŒ€í™”)í•˜ëŠ” $x$ë¥¼ ì°¾ëŠ” ë¬¸ì œ**ë¡œ, ë”¥ëŸ¬ë‹ í•™ìŠµì˜ ìˆ˜í•™ì  ë³¸ì§ˆì´ë‹¤.
- ì´ ë…¸íŠ¸ëŠ” ìµœì í™” ì „ì²´ ë‚´ìš©ì˜ í—ˆë¸Œë¡œ, 1ì°¨ ê¸°ë²• â†’ 2ì°¨ ê¸°ë²• â†’ Convexity ìˆœì„œë¡œ í•™ìŠµí•˜ëŠ” ë¡œë“œë§µì„ ì œê³µí•œë‹¤.
- ê° ì£¼ì œë³„ ìƒì„¸ ë…¸íŠ¸ë¡œ ì—°ê²°í•˜ì—¬ Gradient Descentë¶€í„° KKT ì¡°ê±´ê¹Œì§€ ì²´ê³„ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.

---

## í•™ìŠµ ë¡œë“œë§µ

| ìˆœì„œ | ì£¼ì œ | í•µì‹¬ ì§ˆë¬¸ | ë°”ë¡œ ê°€ê¸° |
|------|------|-----------|--------------|
| 1 | 1ì°¨ ìµœì í™” ê¸°ë²• | GD, Momentum, Adamì€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€? | [[2. ê²½ì‚¬í•˜ê°•ë²•ê³¼ ë³€í˜•]] |
| 2 | 2ì°¨ ìµœì í™” ê¸°ë²• | Newton, L-BFGSëŠ” ì™œ ë¹ ë¥¸ê°€? | [[3. 2ì°¨ ìµœì í™” ê¸°ë²•]] |
| 3 | Convexityì™€ ì œì•½ | Convex ë¬¸ì œë€? KKT ì¡°ê±´ì€? | [[4. ë³¼ë¡ì„±ê³¼ ì œì•½ ìµœì í™”]] |

---

## 1. 1ì°¨ ìµœì í™” ê¸°ë²•

### í•µì‹¬ ê°œë…
- **Gradient Descent**: $x_{t+1} = x_t - \eta \nabla f(x_t)$ (1ì°¨ Taylor ê·¼ì‚¬)
- **Momentum**: ê³¼ê±° gradientì˜ ì§€ìˆ˜ ì´ë™ í‰ê· ìœ¼ë¡œ ê´€ì„± ë¶€ì—¬
- **Nesterov**: Lookahead gradientë¡œ ë³´ì •
- **Adam/AdamW**: 1ì°¨/2ì°¨ ëª¨ë©˜íŠ¸ ì ì‘ì  í•™ìŠµë¥ 

### ì™œ ì¤‘ìš”í•œê°€?
- ë”¥ëŸ¬ë‹ í•™ìŠµì˜ í‘œì¤€ ë°©ë²•
- Adam/AdamW = í˜„ì¬ SOTA optimizer
- í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§(warmup, cosine)ê³¼ ê²°í•©

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[2. ê²½ì‚¬í•˜ê°•ë²•ê³¼ ë³€í˜•|1ì°¨ ìµœì í™” ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- Lipschitz smoothë©´ $\eta \le 1/L$ë¡œ ìˆ˜ë ´ ë³´ì¥
- Momentumì€ ill-conditioned ë¬¸ì œì—ì„œ ìˆ˜ë ´ ê°€ì†
- AdamW = Adam + decoupled weight decay

---

## 2. 2ì°¨ ìµœì í™” ê¸°ë²•

### í•µì‹¬ ê°œë…
- **Newton ë°©ë²•**: $x_{t+1} = x_t - H^{-1} \nabla f$ (2ì°¨ Taylor ê·¼ì‚¬)
- **Quasi-Newton**: Hessian ê·¼ì‚¬ $B_k$ë¥¼ ì ì§„ì  ì—…ë°ì´íŠ¸
- **L-BFGS**: Limited memory BFGS ($O(mn)$ ë©”ëª¨ë¦¬)
- **Natural Gradient**: Fisher Information Matrixë¡œ reconditioning

### ì™œ ì¤‘ìš”í•œê°€?
- Quadratic convergence (Newton)
- ì‘ì€ ë°ì´í„°/fine-tuningì— ìœ ìš© (L-BFGS)
- ê³¡ë¥  ì •ë³´ë¡œ ìµœì  step size ìë™ ê²°ì •

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[3. 2ì°¨ ìµœì í™” ê¸°ë²•|2ì°¨ ìµœì í™” ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- Newtonì€ quadratic í•¨ìˆ˜ì—ì„œ 1 step ìˆ˜ë ´
- BFGS secant equation: $B_{k+1} s_k = y_k$
- L-BFGSëŠ” full-batch gradient í•„ìš”

---

## 3. Convexityì™€ ì œì•½ ìµœì í™”

### í•µì‹¬ ê°œë…
- **Convex í•¨ìˆ˜**: $f(tx + (1-t)y) \le tf(x) + (1-t)f(y)$
- **Lagrangian**: $\mathcal{L}(x, \lambda, \nu) = f(x) + \sum \lambda_i g_i(x) + \sum \nu_j h_j(x)$
- **KKT ì¡°ê±´**: Stationarity, primal/dual feasibility, complementary slackness
- **Flat/Sharp minima**: Hessian ê³ ìœ ê°’ì˜ í¬ê¸°

### ì™œ ì¤‘ìš”í•œê°€?
- Convex ë¬¸ì œëŠ” ì§€ì—­ = ì „ì—­ ìµœì†Œ
- KKTë¡œ SVM, Lasso ë“± ë¶„ì„
- Flat minimaê°€ ì¼ë°˜í™” ì„±ëŠ¥ ì¢‹ìŒ

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[4. ë³¼ë¡ì„±ê³¼ ì œì•½ ìµœì í™”|Convexityì™€ KKT ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- Convex + smooth: $f(y) \ge f(x) + \nabla f^\top (y-x)$ (1ì°¨ ì¡°ê±´)
- Strongly convex: linear convergence for GD
- Complementary slackness: $\lambda_i g_i(x^*) = 0$

---

## ì£¼ì œë³„ ì—°ê²°

### ê°œë… ê°„ ê´€ê³„ë„

```
ìµœì í™” ë¬¸ì œ â”€â”€â”€â”€â”€â”€> ë¹„ì œì•½ / ì œì•½
    â”‚                     â”‚
    â–¼                     â–¼
ëª©ì  í•¨ìˆ˜ â”€â”€â”€â”€> Convex / Non-convex
    â”‚                     â”‚
    â–¼                     â–¼
1ì°¨ ì •ë³´ â”€â”€â”€â”€> Gradient Descent â”€â”€â”€> Momentum
    â”‚                     â”‚
    â–¼                     â–¼
ì ì‘ì  í•™ìŠµë¥  â”€â”€â”€> Adagrad â”€â”€â”€> RMSProp â”€â”€â”€> Adam/AdamW
                          â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼             â–¼             â–¼
        í•™ìŠµë¥          Warmup      Cosine
       ìŠ¤ì¼€ì¤„ë§                   Annealing

2ì°¨ ì •ë³´ â”€â”€â”€â”€> Hessian â”€â”€â”€> Newton Method
    â”‚                         â”‚
    â–¼                         â–¼
ê·¼ì‚¬ ë°©ë²• â”€â”€â”€> Quasi-Newton â”€â”€â”€> BFGS â”€â”€â”€> L-BFGS
    â”‚
    â–¼
Preconditioning â”€â”€â”€> Natural Gradient

ì œì•½ ìµœì í™” â”€â”€â”€> Lagrangian â”€â”€â”€> KKT ì¡°ê±´
    â”‚                             â”‚
    â–¼                             â–¼
ì‘ìš© â”€â”€â”€â”€â”€â”€> SVM, Lasso, L2 ì •ê·œí™”
```

### ìµœì í™” ë°©ë²• ë¹„êµ

| ë°©ë²• | ê³„ì‚° ë³µì¡ë„ | ë©”ëª¨ë¦¬ | ìˆ˜ë ´ ì†ë„ | ë”¥ëŸ¬ë‹ ì ìš© |
|------|------------|--------|----------|------------|
| **GD** | $O(n)$ | $O(n)$ | Linear | âœ… SGD |
| **Momentum** | $O(n)$ | $O(n)$ | Accelerated | âœ… SGD+Momentum |
| **Adam** | $O(n)$ | $O(n)$ | Adaptive | âœ… í‘œì¤€ |
| **Newton** | $O(n^3)$ | $O(n^2)$ | Quadratic | âŒ |
| **L-BFGS** | $O(mn)$ | $O(mn)$ | Superlinear | â–³ Full-batchë§Œ |

---

## ë¹ ë¥¸ ì°¸ì¡°

### ìì£¼ ì“°ëŠ” ê³µì‹

**1ì°¨ ë°©ë²•**:
- GD: $x_{t+1} = x_t - \eta \nabla f(x_t)$
- Momentum: $v_{t+1} = \beta v_t + \nabla f$, $x_{t+1} = x_t - \eta v_{t+1}$
- Adam: $m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla f$, $v_t = \beta_2 v_{t-1} + (1-\beta_2) \nabla f^2$

**2ì°¨ ë°©ë²•**:
- Newton: $x_{t+1} = x_t - H^{-1} \nabla f$
- BFGS: $B_{k+1} = B_k - \frac{B_k s_k s_k^\top B_k}{s_k^\top B_k s_k} + \frac{y_k y_k^\top}{y_k^\top s_k}$

**ìµœì ì„± ì¡°ê±´**:
- 1ì°¨ í•„ìš”: $\nabla f(x^*) = 0$
- 2ì°¨ ì¶©ë¶„: $\nabla f(x^*) = 0$, $\nabla^2 f(x^*) \succ 0$
- KKT: Stationarity + feasibility + complementarity

---

## ì‹¤ìŠµ ê°€ì´ë“œ

### PyTorch ì£¼ìš” í•¨ìˆ˜

```python
import torch
import torch.optim as optim

# ê¸°ë³¸ Optimizers
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

# L-BFGS (closure í•„ìš”)
optimizer = optim.LBFGS(model.parameters(), lr=1.0, max_iter=20)

def closure():
    optimizer.zero_grad()
    loss = loss_fn(model(x), y)
    loss.backward()
    return loss

optimizer.step(closure)

# Learning Rate Scheduler
from torch.optim.lr_scheduler import *

scheduler = CosineAnnealingLR(optimizer, T_max=100)
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
scheduler = OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=10)

# í•™ìŠµ ë£¨í”„
for epoch in range(num_epochs):
    for batch in train_loader:
        optimizer.zero_grad()
        loss = loss_fn(model(batch), target)
        loss.backward()
        optimizer.step()
    scheduler.step()
```

### í•™ìŠµ ì „ëµ

1. **Optimizer ì„ íƒ**
   - ì¼ë°˜ì : AdamW (lr=0.001)
   - Transformer: AdamW + warmup + cosine
   - CNN (ResNet): SGD + momentum (lr=0.1)

2. **í•™ìŠµë¥  íŠœë‹**
   - LR Range Testë¡œ ìµœì  ë²”ìœ„ íƒìƒ‰
   - Warmup (1-10% of total steps)
   - Cosine annealingìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ ê°ì†Œ

3. **ë°°ì¹˜ í¬ê¸°**
   - ì‘ì€ ë°°ì¹˜ (32-128): Flat minima, ë†’ì€ ì¼ë°˜í™”
   - í° ë°°ì¹˜ (512+): lr scaling í•„ìš”

4. **Gradient Clipping**
   - RNN/Transformer: `clip_grad_norm_(params, max_norm=1.0)`

---

## ë”¥ëŸ¬ë‹ ì—°ê²° í¬ì¸íŠ¸

### ì–´ë””ì— ì“°ì´ë‚˜?

| ë”¥ëŸ¬ë‹ ê°œë… | ìµœì í™” ë„êµ¬ | ê´€ë ¨ ë…¸íŠ¸ |
|-----------|----------|-----------|
| í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ | SGD, Adam | [[2. ê²½ì‚¬í•˜ê°•ë²•ê³¼ ë³€í˜•]] |
| Loss ê°ì†Œ | Gradient descent | [[2. ê²½ì‚¬í•˜ê°•ë²•ê³¼ ë³€í˜•]] |
| Regularization | Weight decay (MAP) | [[4. ë³¼ë¡ì„±ê³¼ ì œì•½ ìµœì í™”]] |
| Fine-tuning | L-BFGS | [[3. 2ì°¨ ìµœì í™” ê¸°ë²•]] |
| ì¼ë°˜í™” | Flat minima, ì‘ì€ ë°°ì¹˜ | [[4. ë³¼ë¡ì„±ê³¼ ì œì•½ ìµœì í™”]] |
| í•™ìŠµë¥  ì¡°ì • | Scheduler, warmup | [[2. ê²½ì‚¬í•˜ê°•ë²•ê³¼ ë³€í˜•]] |
| Sharpness | Hessian eigenvalues | [[4. ë³¼ë¡ì„±ê³¼ ì œì•½ ìµœì í™”]] |

---

## ì°¸ê³  ìë£Œ

### êµê³¼ì„œ
- Jorge Nocedal & Stephen Wright, *Numerical Optimization*
- Stephen Boyd & Lieven Vandenberghe, *Convex Optimization*
- Ian Goodfellow et al., *Deep Learning*, Chapter 8

### ë…¼ë¬¸
- Kingma & Ba (2015), "Adam: A Method for Stochastic Optimization"
- Loshchilov & Hutter (2019), "Decoupled Weight Decay Regularization" (AdamW)
- Keskar et al. (2017), "On Large-Batch Training: Generalization Gap and Sharp Minima"
- Foret et al. (2021), "Sharpness-Aware Minimization" (SAM)

### ì˜¨ë¼ì¸ ê°•ì˜
- Stanford EE364a: Convex Optimization
- CS231n: Optimization Lecture
- Sebastian Ruder, "An overview of gradient descent optimization algorithms" (blog)

---

## ë‹¤ìŒ í•™ìŠµ

ìµœì í™” ê¸°ì´ˆë¥¼ ë§ˆì³¤ë‹¤ë©´:

1. **ë¯¸ì ë¶„ê³¼ ìœµí•©**
   - [[Math/Calculus/1. ë¯¸ë¶„ê³¼ ë¯¸ë¶„ë²•|ë¯¸ì ë¶„ ê¸°ì´ˆ]]
   - Gradient, Hessian, Taylor ì „ê°œ

2. **í™•ë¥ í†µê³„ì™€ ì—°ê²°**
   - [[Math/Probability/1. í™•ë¥ ê³¼ í†µê³„ ì…ë¬¸|í™•ë¥ í†µê³„]]
   - MLE = negative log-likelihood ìµœì†Œí™”
   - MAP = MLE + regularization

3. **ì‹¤ì „ ì‘ìš©**
   - [[ML Foundations/3. ë¨¸ì‹ ëŸ¬ë‹ ìµœì í™”|MLì—ì„œì˜ ìµœì í™”]]
   - [[Training/Optimizers and Schedulers|Optimizer ì„ íƒ ê°€ì´ë“œ]]

4. **ê³ ê¸‰ ì£¼ì œ**
   - K-FAC, SAM, Lookahead
   - Meta-learningì—ì„œì˜ 2ì°¨ ë„í•¨ìˆ˜

5. **ìƒìœ„ ê°œë…**
   - [[AI ê¸°ì´ˆ ê°œìš”|AI ê¸°ì´ˆ ì „ì²´ ë¡œë“œë§µ]]
