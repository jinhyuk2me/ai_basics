## TL;DR
- Gradient Descent(GD)는 **1차 도함수(gradient)만을 이용해 함수의 최소값을 찾는 반복 알고리즘**으로, 딥러닝 학습의 근간이다.
- 기본 GD부터 Momentum, Nesterov, Adagrad, RMSProp, Adam/AdamW까지 다양한 변형이 존재하며, 각각 수렴 속도, 안정성, 일반화 성능에서 장단점이 있다.
- 학습률 스케줄링(warmup, cosine annealing 등)과 배치 크기는 최적화 궤적과 최종 성능에 결정적 영향을 미친다.

---

## 1. 핵심 개념

### 1.1 Gradient Descent의 직관

#### 1.1.1 개념: "내리막길 찾기"

함수 $L(\theta)$는 '손실(loss)'이다. 우리는 이 값이 가장 작아지는 $\theta^*$를 찾고 싶다. 그런데 $\theta$가 여러 개의 변수(가중치 벡터)일 때, 어디로 가야 줄어드는지 모른다.

그래서 gradient $\nabla L(\theta)$가 등장한다:
- $\nabla L(\theta)$는 "가장 빠르게 증가하는 방향"
- 따라서 감소시키려면 반대 방향으로 가야 함

즉,
$$
\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)
$$

여기서:
- $\eta$: 학습률(learning rate) — 한 번에 얼마나 움직일지
- $-\nabla L$: 손실이 줄어드는 가장 빠른 방향

#### 1.1.2 기하학적 직관: 산을 내려가는 과정

지형으로 비유하면:
- $L(\theta)$: 산의 높이
- $\theta$: 현재 위치 (x, y 좌표)
- $\nabla L$: 산이 가장 가파르게 올라가는 방향
- $-\nabla L$: 가장 가파르게 내려가는 방향

즉, gradient descent는 매번 현재 위치에서 "지형의 기울기"를 보고, 조금씩 아래쪽으로 걸어 내려가는 과정이다.

#### 1.1.3 수학적 표현

**알고리즘**:
$$
x_{t+1} = x_t - \eta \nabla f(x_t)
$$

여기서:
- $x_t$: $t$ 시점의 파라미터
- $\eta > 0$: 학습률(learning rate, step size)
- $\nabla f(x_t)$: $x_t$에서의 gradient

#### 1.1.4 딥러닝과의 연결

딥러닝에서는:
- $\theta$: 신경망의 가중치(weight) 전체
- $L(\theta)$: 손실 (예: MSE, CrossEntropy 등)
- $\nabla L$: 각 가중치별 손실의 편미분(gradient)

이 gradient를 **역전파(backpropagation)**로 계산하고, 그 결과를 gradient descent 식에 넣어서 매 스텝마다 weight를 조정하는 것이다.

#### 1.1.5 시각적 요약

| 개념 | 수학적 표현 | 의미 |
|------|------------|------|
| 기울기(gradient) | $\nabla L(\theta)$ | 가장 빠르게 증가하는 방향 |
| 하강 방향 | $-\nabla L(\theta)$ | 손실이 줄어드는 방향 |
| 한 스텝 이동 | $\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)$ | 새로운 가중치 갱신 |
| 학습률($\eta$) | 양의 상수 | 너무 크면 overshoot, 너무 작으면 느림 |

### 1.2 왜 동작하는가? (1차 테일러 근사)

Gradient descent는 결국 다변수 테일러 전개에서 나온다.

#### 1.2.1 수학적 유도

함수 $f$를 $x_t$ 근처에서 1차 테일러 전개:
$$
f(x_t + \Delta x) \approx f(x_t) + \nabla f(x_t)^\top \Delta x
$$

손실을 줄이려면 $\Delta x$가 $-\nabla f(x_t)$ 방향으로 가야 $f$가 줄어든다.

$\Delta x = -\eta \nabla f(x_t)$를 선택하면:
$$
f(x_{t+1}) \approx f(x_t) - \eta \|\nabla f(x_t)\|^2 < f(x_t)
$$
($\eta$가 충분히 작고 $\nabla f \ne 0$인 경우)

따라서 함수값이 단조 감소한다.

#### 1.2.2 기하학적 의미

이것은 "현재 위치 $x_t$에서 1차 근사를 했을 때, 어느 방향으로 가야 함수가 가장 빠르게 감소하는가?"에 대한 답이다.

- $\nabla f(x_t)^\top \Delta x$가 음수가 되려면 $\Delta x$와 $\nabla f$가 반대 방향
- 크기를 $\|\nabla f\|^2$만큼 최대화하려면 $\Delta x = -\nabla f$ 방향
- 이것이 바로 업데이트 법칙의 수학적 근거다.

### 1.3 수렴성 조건

경사하강법이 언제, 얼마나 빨리 최적점에 도달하는지를 수학적으로 분석하는 조건들입니다.

#### 1.3.1 Lipschitz Smooth 조건

- **수식**:
  $$
  \|\nabla f(x) - \nabla f(y)\| \le L \|x - y\|
  $$
- **수식 해설**:
  - **좌변 `\|\nabla f(x) - \nabla f(y)\|`**: 두 점 $x, y$에서 계산한 **그래디언트 벡터 간의 거리**입니다. 그래디언트의 변화량을 의미합니다.
  - **우변 `L \|x - y\|`**: 두 점 $x, y$ **자체의 거리에 상수 $L$을 곱한 값**입니다.
  - **의미**: "두 점 사이의 거리가 가까우면, 두 점에서의 그래디언트 값도 그에 비례해서 비슷해야 한다"는 뜻입니다. 즉, 그래디언트가 갑자기 크게 변하지 않는 '부드러운' 함수임을 의미합니다. $L$은 그 부드러운 정도(최대 곡률)를 나타내는 **Lipschitz 상수**입니다.

- **수렴 조건과 학습률($\eta \le 1/L$)**:
  - 이 조건은 **Descent Lemma**라 불리는 다음 보조 정리로부터 유도됩니다.
    $$
    f(y) \le f(x) + \nabla f(x)^T(y-x) + \frac{L}{2}\|y-x\|^2
    $$
    - 위 식은 L-smooth 함수는 어떤 점에서든 위로 볼록한 2차 함수보다는 아래에 있다는 것을 의미합니다.
  - 여기에 경사하강법 업데이트($y = x_{t+1} = x_t - \eta \nabla f(x_t)$)를 대입하면,
    $$
    \begin{aligned}
    f(x_{t+1}) &\le f(x_t) + \nabla f(x_t)^T(-\eta \nabla f(x_t)) + \frac{L}{2}\|-\eta \nabla f(x_t)\|^2 \\
    &= f(x_t) - \eta \|\nabla f(x_t)\|^2 + \frac{L\eta^2}{2}\|\nabla f(x_t)\|^2 \\
    &= f(x_t) - \eta(1 - \frac{L\eta}{2})\|\nabla f(x_t)\|^2
    \end{aligned}
    $$
  - 손실이 감소하려면($f(x_{t+1}) < f(x_t)$), 우변의 마지막 항 $\eta(1 - \frac{L\eta}{2})\|\nabla f(x_t)\|^2$이 양수여야 합니다. 즉, $(1 - \frac{L\eta}{2}) > 0$ 이어야 하므로 $\eta < 2/L$ 라는 조건을 얻습니다. 더 안정적인 수렴을 위해 보통 $\eta \le 1/L$ 이라는 더 엄격한 조건을 사용합니다. 학습률이 이보다 크면 업데이트 시 손실이 오히려 발산할 수 있습니다.

#### 1.3.2 Convex + Smooth 조건

Convex(볼록) 함수는 전역 최저점(global minimum)이 유일하게 존재하므로, 경사하강법이 그곳으로 수렴함을 보장할 수 있습니다. 여기서 수렴 속도는 **오차(error)** 가 얼마나 빨리 0으로 가는지를 의미합니다.

- **오차의 정의**: $T$번째 반복에서의 오차는 $f(x_T) - f^*$ 로 정의합니다.
  - $f(x_T)$: $T$번째 스텝에서의 함수 값
  - $f^*$: 함수가 도달할 수 있는 **전역 최솟값 (global minimum value)**
  - 즉, 오차는 '현재 값과 최적의 최소값의 차이' 입니다.

- **일반 Convex 함수의 수렴 속도 ($O(1/T)$)**:
  - **의미**: 오차 $f(x_T) - f^*$가 반복 횟수 $T$에 **반비례**해서 줄어듭니다. 즉, `오차 ≈ 상수 / T` 관계가 성립합니다.
  - **왜 느린가?**: 일반 Convex 함수는 '바닥이 넓고 평평한 그릇'과 같습니다. 최저점 근처로 갈수록 경사(그래디언트)가 0에 가까워져, 업데이트되는 스텝의 크기(`학습률 × 그래디언트`)가 점점 작아집니다. 이는 마치 목적지에 다가갈수록 보폭이 줄어들어 마지막 몇 미터를 가는 데 시간이 오래 걸리는 것과 같습니다.
  - **수치 예시**: 이 관계를 표로 보면 다음과 같습니다.

| 반복 횟수 (T) | 오차 (≈ 1/T) | 설명                                   |
| :-------- | :--------- | :----------------------------------- |
| 10        | 0.1        |                                      |
| 100       | 0.01       | 오차를 10배 줄이려면, 반복도 10배 필요             |
| 1000      | 0.001      | 오차를 다시 10배 줄이려면, 반복도 다시 10배 필요       |
| 10000     | 0.0001     | 이처럼 정확도를 한 자릿수 높일 때마다 엄청난 반복이 추가됩니다. |

- **Strongly Convex 함수의 수렴 속도 ($O(\gamma^T)$)**:
  - **핵심 비유 - "V"자 계곡**: 일반 Convex 함수가 바닥이 넓은 'U'자 계곡이라면, Strongly Convex 함수는 깎아지른 듯한 'V'자 계곡과 같습니다. 'V'자 계곡은 바닥(최저점)을 제외한 모든 곳의 경사가 가파르기 때문에, 어디에 있든 항상 바닥으로 강하게 미끄러져 내려갑니다.
  - **조건 수식과 비유의 연결**:
    $$
    f(x) \ge f(y) + \nabla f(y)^T(x-y) + \frac{\mu}{2}\|x-y\|^2
    $$
    - 수식의 `$+ \frac{\mu}{2}\|x-y\|^2$` 항이 바로 이 "V"자 모양을 만드는 핵심 장치입니다. 이 항은 "계곡의 어떤 지점도 최소 $\mu$라는 경사도를 가져야 한다"고 강제하는 역할을 합니다.
    - 이 조건 덕분에 최저점에 아무리 가까워져도 그래디언트가 0에 가깝게 사라지지 않고, 항상 충분한 크기를 유지하여 '의미 있는' 전진을 보장합니다.
  - **Linear 수렴 - "남은 거리의 10%씩 이동하기"**:
    - **의미**: Linear 수렴은 매 반복마다 오차가 '일정한 비율'로 줄어드는 것을 의미합니다. 이는 마치 마법 신발을 신고 계곡 바닥의 보물을 찾아가는 것과 같습니다. 이 신발은 현재 위치에서 보물까지 **남은 거리의 10%** 만큼 항상 이동시켜 줍니다.
    - **수치 예시**:
      - **스텝 1**: 보물까지 100m 남음 → 10m 이동 (90m 남음)
      - **스텝 2**: 90m 남음 → 9m 이동 (81m 남음)
      - **스텝 3**: 81m 남음 → 8.1m 이동 (72.9m 남음)
    - 이처럼, 목표에 가까워져도 나아가는 '비율'이 줄지 않기 때문에, 제자리걸음하는 Sublinear 수렴과 달리 매우 빠르게 목표에 도달할 수 있습니다. 여기서 10%씩 줄어드는 비율(0.9)이 바로 $\gamma$에 해당합니다.

| 함수 종류 | 수렴 속도 (오차) | 오차 10배 감소에 필요한 반복 횟수 (예시) |
|---|---|---|
| Convex | $O(1/T)$ | 10배 (e.g., 100 -> 1000) |
| Strongly Convex | $O(\gamma^T)$ | 고정값 (e.g., 22) |

#### 1.3.3 Non-convex 조건 (딥러닝의 경우)

딥러닝의 손실 함수는 대부분 Non-convex 입니다. 즉, 전역 최저점 외에도 수많은 지역 최저점(local minima)과 안장점(saddle point)이 존재합니다.

- **어떻게 Non-convex 형태가 결정되는가?**:
  사용자가 손실 함수의 수학식을 직접 정의하는 것은 아니지만, 함수의 형태를 결정하는 **'설계도'** 를 만듭니다. 이 설계도에 따라 함수의 최종적인 울퉁불퉁한 모양이 결정됩니다.
  - **모델 구조 (Architecture)**: 레이어를 몇 개나, 얼마나 넓게 쌓을지, 어떤 종류(CNN, RNN 등)의 계산을 할지 결정합니다. 모델이 깊고 복잡해질수록 함수 형태도 복잡해집니다.
  - **활성화 함수 (Activation Function)**: ReLU, Sigmoid 같은 **비선형 함수**를 선택합니다. 이 비선형 함수들을 여러 겹 중첩하는 것이 Non-convex 형태를 만드는 가장 결정적인 원인입니다.
  - **손실 함수 종류 (Loss Function)**: 최종 오차를 측정하는 방식(MSE, Cross-Entropy 등)을 선택합니다.

  > **비유: 레고(LEGO) 조립**
  > 딥러닝 모델링은 레고 조립과 같습니다. 사용자는 어떤 모양의 브릭(레이어, 활성화 함수)을 어떻게 쌓을지 '설계도'를 만듭니다. 이 설계에 따라 완성된 거대한 조각품의 '그림자 모양'(손실 함수의 형태)이 결정됩니다. 즉, 사용자는 함수의 형태를 직접 빚는 '조각가'라기보다, 최종 형태를 결정하는 '설계자(Architect)'에 가깝습니다.

- **이론적 한계**: 이러한 Non-convex 함수에서, 경사하강법은 그래디언트가 0인 지점($\nabla f(x)=0$)으로 수렴할 뿐, 그곳이 전역 최저점이라는 보장은 없습니다. 지역 최저점이나 안장점에 갇힐 수 있습니다.

- **그럼에도 실전에서 잘 작동하는 이유**:
  1.  **고차원의 축복 (Blessing of Dimensionality)**: 신경망의 파라미터 공간은 수백만 차원 이상으로 매우 높습니다. 이러한 고차원 공간에서는 대부분의 지역 최저점들이 실제 전역 최저점과 거의 비슷한 손실 값을 가집니다. 즉, '질 나쁜' 지역 최저점은 드뭅니다.
  2.  **안장점 문제**: 실제 최적화의 주된 장애물은 지역 최소값이 아니라 안장점(saddle point)입니다. 안장점은 특정 방향에서는 극소값, 다른 방향에서는 극대값인 지점으로, 그래디언트가 0에 가까워 학습이 매우 느려집니다.
  3.  **진보된 옵티마이저**: Adam, RMSProp과 같은 최신 옵티마이저들은 **관성(Momentum)**을 이용해 평평한 지역을 빠르게 통과하고, **적응적 학습률(Adaptive Learning Rate)**로 각 파라미터의 상황에 맞게 보폭을 조절하여 안장점에서 더 효율적으로 탈출합니다.
  4.  **확률적 경사하강법 (SGD)의 노이즈**: 전체 데이터가 아닌 무작위 미니배치를 사용하기 때문에 그래디언트에 노이즈가 발생합니다. 이 '불안정성'이 오히려 얕은 지역 최소값이나 안장점에서 탈출하도록 돕는 긍정적인 역할을 합니다.

---

## 2. 수학적 전개

### 2.1 최적 학습률 (Quadratic 함수)

2차 함수 $f(x) = \frac{1}{2} x^\top A x - b^\top x$ ($A \succ 0$, symmetric):

이 식이 어떻게 유도되는지 기하학적, 수학적 관점에서 살펴보겠습니다.

#### 기하학적 형태
먼저 함수 $f(x)$는 기하학적으로 어떤 형태를 가질까요? 행렬 $A$가 양의 정부 대칭 행렬($A=A^T, A \succ 0$)일 때, 이 함수는 아래로 볼록한 그릇 모양의 **n차원 포물면(paraboloid)** 형태를 가집니다. 따라서 유일한 전역 최저점이 존재합니다.

#### 그래디언트 계산
$\nabla f(x)$를 계산하기 위해 각 항을 나누어 미분해 보겠습니다.

**1. 이차 형식 항($\frac{1}{2} x^\top A x$)의 미분**

첫 번째 항은 **이차 형식(Quadratic Form)** 입니다. 벡터 미분에서 중요한 공식 중 하나는 다음과 같습니다.
$$
\nabla_x (x^\top A x) = (A + A^\top)x
$$
>  **(증명 스케치)**
>  $x^\top A x = \sum_i \sum_j A_{ij}x_i x_j$ 입니다. 
>  이를 $x_k$에 대해 편미분하면, $i=k$인 항과 $j=k$인 항이 남게 되어 $\sum_j A_{kj}x_j + \sum_i A_{ik}x_i$ 가 됩니다. 
>  이를 모든 $k$에 대해 벡터로 묶으면 $(Ax)_k + (A^\top x)_k$ 가 되어, 최종적으로 $(A+A^\top)x$가 됩니다.

문제에서는 $A$가 대칭 행렬($A=A^\top$)이라고 가정했으므로, 위 공식은 다음과 같이 간단해집니다.
$$
\nabla_x (x^\top A x) = 2Ax
$$
따라서, 원래 항에 있던 $\frac{1}{2}$과 상쇄되어 다음을 얻습니다.
$$
\nabla_x \left(\frac{1}{2} x^\top A x\right) = Ax
$$

**2. 선형 항($-b^\top x$)의 미분**

두 번째 항은 선형(Linear) 항입니다. $f(x) = -b^\top x = -\sum_i b_i x_i$ 이므로, 이를 각 $x_k$에 대해 편미분하면 $-b_k$가 됩니다. 이를 벡터로 표현하면 다음과 같습니다.
$$
\nabla_x (-b^\top x) = -b
$$

**3. 결과 종합**

위 두 결과를 합치면, 최종적으로 다음의 그래디언트 공식을 얻게 됩니다.
$$
\nabla f(x) = Ax - b
$$

#### 직관적 해석
- **$Ax$**: 이차항에서 오는 부분으로, 포물면의 각 위치에서 '중심'을 향하는 방향과 그릇의 '곡률'을 반영합니다.
- **$-b$**: 선형항에서 오는 부분으로, 전체 포물면의 최저점 위치를 결정하는 '이동(offset)' 역할을 합니다.

즉, 그래디언트 $\nabla f(x) = Ax - b$는 현재 위치 $x$에서 포물면의 최저점으로 가기 위한 방향과 크기에 대한 정보를 담고 있습니다.

**Gradient**:
$$
\nabla f(x) = Ax - b
$$

**GD 업데이트**:
$$
x_{t+1} = x_t - \eta (Ax_t - b) = (I - \eta A)x_t + \eta b
$$

#### 최적해로의 오차 유도
현재 위치 $x_t$와 최적해 $x^*$ 사이의 오차 $e_t = x_t - x^*$가 매 스텝마다 어떻게 변하는지 분석해 보겠습니다.

1.  **최적해($x^*$)의 조건**: 최적해에서는 그래디언트가 0이므로 $\nabla f(x^*) = Ax^* - b = 0$, 즉 $Ax^* = b$가 성립합니다.

2.  **오차의 재귀 관계식 유도**:
    - GD 업데이트 식의 양변에서 $x^*$를 빼줍니다.
      $$
      x_{t+1} - x^* = (I - \eta A)x_t + \eta b - x^*
      $$
    - 위 식에 $b = Ax^*$를 대입합니다.
      $$
      \begin{aligned}
      x_{t+1} - x^* &= (I - \eta A)x_t + \eta (Ax^*) - x^* \\
      &= (I - \eta A)x_t - (I - \eta A)x^*
      \end{aligned}
      $$
    - 우변을 $(I - \eta A)$로 묶어주고, 오차의 정의($e_t = x_t - x^*$)를 적용합니다.
      $$
      e_{t+1} = (I - \eta A)(x_t - x^*) = (I - \eta A)e_t
      $$

3.  **최종 오차 공식**: 위 재귀 관계를 $t$번 풀면, 초기 오차 $e_0$로부터 현재 오차 $e_t$를 다음과 같이 표현할 수 있습니다.
    $$
    e_t = (I - \eta A)^t e_0
    $$

#### 수렴 조건 유도
- **핵심 아이디어**: 오차 $e_t$가 0으로 수렴하려면, 행렬 $(I - \eta A)$를 계속 곱할수록 크기가 작아져야 합니다.
- **수학적 조건**: 행렬 $M$에 대해 $M^t \to 0$ 이 될 조건은 $M$의 **스펙트럴 반지름(spectral radius)** $\rho(M)$이 1보다 작은 것입니다. 스펙트럴 반지름은 행렬의 고유값(eigenvalue)들 중 가장 큰 절댓값입니다.
- **조건 적용**: 따라서 수렴 조건은 $\rho(I - \eta A) < 1$ 입니다.
  - 행렬 $A$의 고유값을 $\lambda_i$라 할 때, 행렬 $(I - \eta A)$의 고유값은 $(1 - \eta \lambda_i)$ 입니다.
  - 그러므로, 모든 고유값 $\lambda_i$에 대해 $|1 - \eta \lambda_i| < 1$ 이 성립해야 합니다.
  - 이 부등식을 풀면 $-1 < 1 - \eta \lambda_i < 1$, 즉 $0 < \eta\lambda_i < 2$ 이고, $\eta < 2 / \lambda_i$ 를 얻습니다.
- **최종 수렴 조건**: 이 조건이 모든 $\lambda_i$에 대해 만족되어야 하므로, 가장 큰 고유값 $\lambda_{\max}$을 기준으로 합니다.
  $$
  \eta < \frac{2}{\lambda_{\max}}
  $$

#### 최적 학습률($\eta^*$) 유도
- **핵심 아이디어**: 가장 빠른 수렴은 오차의 감소폭을 최대로 만드는 것, 즉 스펙트럴 반지름 $\rho(I - \eta A) = \max_i |1 - \eta \lambda_i|$를 **최소화**하는 $\eta$를 찾는 것입니다.
- **최소화 문제**: $\min_\eta \max_i |1 - \eta \lambda_i|$
  - 이 값은 보통 가장 작은 고유값 $\lambda_{\min}$과 가장 큰 고유값 $\lambda_{\max}$에 의해 결정됩니다. 즉, 우리는 $\max(|1 - \eta\lambda_{\min}|, |1 - \eta\lambda_{\max}|)$를 최소화하고 싶습니다.
- **최적 지점 찾기**: 위 값이 최소가 되는 지점은 두 항의 절댓값이 같아지는 지점입니다. 기하학적으로, $1$에서 $\eta\lambda$들을 뺐을 때, 0으로부터의 거리가 같아지는 지점입니다.
  $$
  1 - \eta\lambda_{\min} = -(1 - \eta\lambda_{\max})
  $$
- **최종 최적 학습률**: 위 방정식을 $\eta$에 대해 풀면 다음과 같습니다.
  $$
  \begin{aligned}
  1 - \eta\lambda_{\min} &= -1 + \eta\lambda_{\max} \\
  2 &= \eta(\lambda_{\min} + \lambda_{\max}) \\
  \eta^* &= \frac{2}{\lambda_{\min} + \lambda_{\max}}
  \end{aligned}
  $$
- **조건수와 수렴 속도**:
  - 조건수 $\kappa = \lambda_{\max}/\lambda_{\min}$가 크면(ill-conditioned), 즉 가장 큰 고유값과 가장 작은 고유값의 차이가 크면 수렴이 느려집니다. 이는 마치 길고 좁은 협곡 모양의 손실 함수에서 지그재그로 움직이며 비효율적으로 탐색하는 것과 같습니다.

### 2.2 Line Search

각 step마다 최적의 $\eta_t$를 찾는 방법:

**Exact line search**:
$$
\eta_t = \arg\min_{\eta > 0} f(x_t - \eta \nabla f(x_t))
$$

**Backtracking line search** (Armijo 조건):
- 초기 $\eta$에서 시작해 조건을 만족할 때까지 $\eta \leftarrow \beta \eta$ ($\beta < 1$)
- 조건: $f(x_t - \eta \nabla f) \le f(x_t) - c \eta \|\nabla f\|^2$ ($c \in (0, 1)$)

**딥러닝에서는**:
- Line search는 비용이 너무 커서 거의 사용 안 함
- 대신 학습률 스케줄링 + adaptive methods 사용

---

## 3. Momentum과 Nesterov

### 3.1 Momentum (Heavy Ball Method)

**동기**:
- GD는 협곡(valley) 형태의 손실 함수에서 지그재그로 진동
- 과거 이동 방향에 관성을 부여해 진동 감소

**알고리즘**:
$$
\begin{align}
v_{t+1} &= \beta v_t + \nabla f(x_t) \\
x_{t+1} &= x_t - \eta v_{t+1}
\end{align}
$$
- $v_t$: velocity (속도)
- $\beta \in [0, 1)$: momentum coefficient (보통 0.9)

**다른 표현** (PyTorch 스타일):
$$
\begin{align}
v_{t+1} &= \beta v_t + (1 - \beta) \nabla f(x_t) \\
x_{t+1} &= x_t - \eta v_{t+1}
\end{align}
$$
(velocity를 지수 이동 평균으로 해석)

**효과**:
- 일관된 방향으로는 가속
- 진동하는 방향은 상쇄
- Convex quadratic에서 수렴 속도 $O(e^{-\sqrt{\kappa}T})$ → $O(e^{-\kappa T})$보다 빠름

### 3.2 Nesterov Accelerated Gradient (NAG)

**개선점**:
- Momentum은 현재 위치에서 gradient 계산
- NAG는 **lookahead 위치** $x_t - \beta v_t$에서 gradient 계산

**알고리즘**:
$$
\begin{align}
v_{t+1} &= \beta v_t + \nabla f(x_t - \beta v_t) \\
x_{t+1} &= x_t - \eta v_{t+1}
\end{align}
$$

**직관**:
- 관성으로 가려는 곳을 미리 보고(lookahead) gradient 계산
- 잘못된 방향으로 가는 것을 미리 감지해 보정

**이론적 개선**:
- Convex smooth에서 $O(1/T^2)$ convergence rate (GD는 $O(1/T)$)
- 실전에서는 Momentum과 비슷한 경우가 많음

---

## 4. Adaptive Learning Rate Methods

### 4.1 Adagrad (Adaptive Gradient)

**동기**:
- 희소 데이터에서 드물게 등장하는 feature는 큰 학습률 필요
- 자주 업데이트되는 파라미터는 작은 학습률 필요

**알고리즘**:
$$
\begin{align}
G_{t} &= G_{t-1} + \nabla f(x_t) \odot \nabla f(x_t) \quad \text{(누적 제곱)} \\
x_{t+1} &= x_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot \nabla f(x_t)
\end{align}
$$
- $G_t$: gradient 제곱의 누적 합
- $\odot$: element-wise 곱/나눗셈
- $\epsilon \approx 10^{-8}$: 0으로 나누기 방지

**장점**:
- 파라미터별로 학습률 자동 조정
- 희소 gradient에 강함 (NLP embedding 등)

**단점**:
- $G_t$가 계속 증가 → 학습률이 너무 빨리 감소
- 깊은 네트워크 학습에서 조기 종료 문제

### 4.2 RMSProp (Root Mean Square Propagation)

**개선점**:
- 누적 합 대신 **지수 이동 평균** 사용
- 과거 정보를 서서히 망각

**알고리즘**:
$$
\begin{align}
v_t &= \beta v_{t-1} + (1 - \beta) \nabla f(x_t) \odot \nabla f(x_t) \\
x_{t+1} &= x_t - \frac{\eta}{\sqrt{v_t + \epsilon}} \odot \nabla f(x_t)
\end{align}
$$
- $\beta \approx 0.9$: decay rate
- $v_t$: gradient 제곱의 지수 이동 평균

**효과**:
- Adagrad의 학습률 감소 문제 해결
- 비정상적(non-stationary) 목적 함수에 적응

### 4.3 Adam (Adaptive Moment Estimation)

**핵심 아이디어**:
- Momentum (1차 모멘트) + RMSProp (2차 모멘트)
- Bias correction으로 초기 학습 안정화

**알고리즘**:
$$
\begin{align}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) \nabla f(x_t) \quad &\text{(1차 모멘트, 평균)} \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) \nabla f(x_t)^2 \quad &\text{(2차 모멘트, 분산)} \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \quad &\text{(bias correction)} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
x_{t+1} &= x_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align}
$$

**하이퍼파라미터** (기본값):
- $\beta_1 = 0.9$: 1차 모멘트 decay
- $\beta_2 = 0.999$: 2차 모멘트 decay
- $\epsilon = 10^{-8}$
- $\eta = 0.001$ (또는 0.0001)

**Bias Correction의 필요성**:
- $m_0 = v_0 = 0$으로 초기화
- 초기 단계에서 $m_t$, $v_t$가 0으로 편향됨
- $1 - \beta^t$로 나눠 보정 ($t$가 클수록 1에 근접)

### 4.4 AdamW (Adam with Decoupled Weight Decay)

**원래 Adam의 문제**:
- L2 정규화 $\lambda \|x\|^2$를 loss에 추가하면
  $$
  \nabla (f + \lambda \|x\|^2) = \nabla f + 2\lambda x
  $$
- 이것이 adaptive scaling에 의해 왜곡됨

**AdamW 해결책**:
- Weight decay를 gradient 계산과 **분리(decouple)**
  $$
  x_{t+1} = (1 - \eta \lambda) x_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
  $$
- Weight decay가 적응적 스케일링의 영향을 받지 않음

**효과**:
- 일반화 성능 향상 (특히 Transformer 등 대규모 모델)
- 현재 딥러닝 표준 optimizer 중 하나

---

## 5. 학습률 스케줄링

### 5.1 주요 전략

**Fixed (고정)**:
- $\eta_t = \eta_0$ (상수)
- 간단하지만 최적 아닐 수 있음

**Step Decay**:
- $\eta_t = \eta_0 \cdot \gamma^{\lfloor t / k \rfloor}$
- 일정 epoch마다 $\gamma$ (예: 0.1) 배로 감소

**Exponential Decay**:
- $\eta_t = \eta_0 e^{-\alpha t}$

**Inverse Decay**:
- $\eta_t = \eta_0 / (1 + \alpha t)$

**Cosine Annealing**:
$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\pi t / T))
$$
- $T$: 전체 step 수
- 부드럽게 감소, 마지막에 fine-tuning

**OneCycle**:
- Warmup (선형 증가) → Annealing (cosine 감소)
- 빠른 수렴 + 좋은 일반화

### 5.2 Warmup

**동기**:
- 초기 파라미터가 무작위 → gradient 불안정
- 작은 학습률로 시작해 서서히 증가

**구현**:
$$
\eta_t = \eta_{\max} \cdot \min(1, t / T_{\text{warmup}})
$$
- $T_{\text{warmup}}$: warmup step 수 (보통 전체의 1~10%)

**효과**:
- Transformer, BERT 등 대규모 모델에서 필수
- Adam과 함께 사용 시 특히 효과적

---

## 6. PyTorch 구현

### 6.1 기본 Gradient Descent

```python
import torch

# Rosenbrock 함수: f(x, y) = (1-x)^2 + 100(y - x^2)^2
def rosenbrock(x):
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

x = torch.tensor([0.0, 0.0], requires_grad=True)
optimizer = torch.optim.SGD([x], lr=0.001)  # 기본 GD (momentum=0)

for step in range(1000):
    optimizer.zero_grad()
    loss = rosenbrock(x)
    loss.backward()
    optimizer.step()

    if step % 100 == 0:
        print(f"Step {step}: x={x.data.numpy()}, f(x)={loss.item():.6f}")
```

### 6.2 Momentum

```python
# SGD with momentum
optimizer = torch.optim.SGD([x], lr=0.001, momentum=0.9)

# 위와 동일한 루프
for step in range(1000):
    optimizer.zero_grad()
    loss = rosenbrock(x)
    loss.backward()
    optimizer.step()
```

### 6.3 Adam

```python
optimizer = torch.optim.Adam([x], lr=0.01, betas=(0.9, 0.999))

for step in range(1000):
    optimizer.zero_grad()
    loss = rosenbrock(x)
    loss.backward()
    optimizer.step()
```

### 6.4 AdamW with Scheduler

```python
from torch.optim.lr_scheduler import CosineAnnealingLR

x = torch.tensor([0.0, 0.0], requires_grad=True)
optimizer = torch.optim.AdamW([x], lr=0.01, weight_decay=0.01)
scheduler = CosineAnnealingLR(optimizer, T_max=1000)

for step in range(1000):
    optimizer.zero_grad()
    loss = rosenbrock(x)
    loss.backward()
    optimizer.step()
    scheduler.step()

    if step % 100 == 0:
        current_lr = scheduler.get_last_lr()[0]
        print(f"Step {step}: lr={current_lr:.6f}, f(x)={loss.item():.6f}")
```

### 6.5 학습률별 비교 시각화

```python
import matplotlib.pyplot as plt
import numpy as np

def optimize_with_method(optimizer_class, **kwargs):
    x = torch.tensor([0.0, 0.0], requires_grad=True)
    optimizer = optimizer_class([x], **kwargs)

    path = []
    for _ in range(500):
        optimizer.zero_grad()
        loss = rosenbrock(x)
        loss.backward()
        optimizer.step()
        path.append(x.detach().clone().numpy())

    return np.array(path)

# 여러 방법 비교
methods = [
    ("GD (lr=0.001)", torch.optim.SGD, {"lr": 0.001}),
    ("Momentum", torch.optim.SGD, {"lr": 0.001, "momentum": 0.9}),
    ("Adam", torch.optim.Adam, {"lr": 0.01}),
    ("AdamW", torch.optim.AdamW, {"lr": 0.01, "weight_decay": 0.01}),
]

plt.figure(figsize=(10, 8))
for name, opt_class, kwargs in methods:
    path = optimize_with_method(opt_class, **kwargs)
    plt.plot(path[:, 0], path[:, 1], label=name, alpha=0.7)

plt.plot(1, 1, 'r*', markersize=15, label='Optimum (1, 1)')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Optimization Paths on Rosenbrock Function')
plt.legend()
plt.grid(True)
plt.show()
```

---

## 7. 실전 가이드

### 7.1 Optimizer 선택

| 상황 | 추천 Optimizer | 이유 |
|------|---------------|------|
| **일반적 경우** | AdamW | 빠른 수렴 + 좋은 일반화 |
| **대규모 Transformer** | AdamW + warmup + cosine | 표준 레시피 |
| **CNN (ResNet 등)** | SGD + momentum | 장기 학습 시 더 나은 일반화 |
| **희소 데이터 (NLP embedding)** | Adagrad, Adam | 파라미터별 adaptive 필요 |
| **Fine-tuning** | AdamW (작은 lr) | 기존 가중치 보존 |

### 7.2 학습률 튜닝

**LR Range Test** (Leslie Smith):
1. 매우 작은 lr (예: $10^{-7}$)에서 시작
2. 매 iteration마다 lr 지수 증가
3. Loss 발산 직전까지의 최대 lr 찾기
4. 그 값의 1/3~1/10 사용

**일반적 시작점**:
- Adam/AdamW: 0.001 또는 0.0001
- SGD+Momentum: 0.01~0.1
- 배치 크기에 비례 스케일링: $\text{lr} \propto \sqrt{\text{batch size}}$

### 7.3 Gradient Clipping

큰 gradient로 인한 불안정성 방지:

```python
# Norm clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# Value clipping
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
```

**언제 사용**:
- RNN/LSTM (exploding gradient 문제)
- Transformer (선택적, 보통 필요 없음)

---

## 8. 실습 과제

1. **Rosenbrock 최적화**
   - GD, Momentum, NAG, Adam으로 각각 최적화
   - 수렴 속도와 경로를 시각화하고 비교

2. **학습률 민감도**
   - Quadratic 함수 $f(x, y) = x^2 + 10y^2$에서
   - lr = 0.01, 0.1, 0.3, 0.5로 GD 수행
   - 각 경우의 수렴 여부와 속도 분석

3. **Momentum 효과 시연**
   - 협곡형 함수 (conditioning number가 큰 quadratic)
   - $\beta = 0, 0.5, 0.9, 0.99$로 변화시키며 관찰

4. **Adam vs SGD 비교**
   - MNIST/CIFAR-10에서 동일한 CNN 학습
   - 각 optimizer의 train/val loss 곡선 비교
   - 최종 test accuracy 비교

5. **Learning Rate Schedule**
   - Constant, Step Decay, Cosine Annealing 비교
   - Warmup 유무에 따른 초기 학습 안정성 관찰