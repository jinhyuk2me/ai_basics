## TL;DR
- Gradient Descent(GD)는 **1차 도함수(gradient)만을 이용해 함수의 최소값을 찾는 반복 알고리즘**으로, 딥러닝 학습의 근간이다.
- 기본 GD부터 Momentum, Nesterov, Adagrad, RMSProp, Adam/AdamW까지 다양한 변형이 존재하며, 각각 수렴 속도, 안정성, 일반화 성능에서 장단점이 있다.
- 학습률 스케줄링(warmup, cosine annealing 등)과 배치 크기는 최적화 궤적과 최종 성능에 결정적 영향을 미친다.

---

## 1. 핵심 개념

### 1.1 Gradient Descent의 직관

**기하학적 해석**:
- Gradient $\nabla f(x)$는 함수가 **가장 빠르게 증가하는 방향**
- 반대 방향 $-\nabla f(x)$로 이동하면 함수값이 가장 빠르게 감소
- 작은 스텝 $\eta$만큼 반복 이동 → 지역 최소값에 수렴

**알고리즘**:
$$
x_{t+1} = x_t - \eta \nabla f(x_t)
$$
- $x_t$: $t$ 시점의 파라미터
- $\eta > 0$: 학습률(learning rate, step size)
- $\nabla f(x_t)$: $x_t$에서의 gradient

### 1.2 왜 동작하는가? (1차 테일러 근사)

함수 $f$를 $x_t$ 근처에서 1차 테일러 전개:
$$
f(x_t + \Delta x) \approx f(x_t) + \nabla f(x_t)^\top \Delta x
$$

$\Delta x = -\eta \nabla f(x_t)$를 선택하면:
$$
f(x_{t+1}) \approx f(x_t) - \eta \|\nabla f(x_t)\|^2 < f(x_t)
$$
($\eta$가 충분히 작고 $\nabla f \ne 0$인 경우)

따라서 함수값이 단조 감소한다.

### 1.3 수렴성 조건

**Lipschitz Smooth 함수**에서는 적절한 $\eta$로 수렴 보장:
- $\|\nabla f(x) - \nabla f(y)\| \le L \|x - y\|$ (Lipschitz 연속 gradient)
- $\eta \le 1/L$이면 $f(x_t) \to f^*$ (최소값)

**Convex + Smooth 함수**:
- $T$ 번 반복 후 $f(x_T) - f^* = O(1/T)$ (sublinear convergence)
- Strongly convex면 $O(e^{-\alpha T})$ (linear convergence)

**Non-convex (딥러닝)**:
- 지역 최소값/saddle point로 수렴
- 이론적 보장 약하지만 실전에서는 잘 작동

---

## 2. 수학적 전개

### 2.1 최적 학습률 (Quadratic 함수)

2차 함수 $f(x) = \frac{1}{2} x^\top A x - b^\top x$ ($A \succ 0$, symmetric):

**Gradient**:
$$
\nabla f(x) = Ax - b
$$

**GD 업데이트**:
$$
x_{t+1} = x_t - \eta (Ax_t - b) = (I - \eta A)x_t + \eta b
$$

**최적해로의 오차**:
$$
e_t = x_t - x^* = (I - \eta A)^t e_0
$$

**수렴 조건**:
- $\rho(I - \eta A) < 1$ (spectral radius < 1)
- $A$의 고유값 $\lambda_i$에 대해 $|1 - \eta \lambda_i| < 1$
- 따라서 $\eta < 2 / \lambda_{\max}$

**최적 학습률**:
$$
\eta^* = \frac{2}{\lambda_{\min} + \lambda_{\max}}
$$
- 조건수 $\kappa = \lambda_{\max}/\lambda_{\min}$가 크면 수렴이 느림
- 이것이 **ill-conditioned 문제**

### 2.2 Line Search

각 step마다 최적의 $\eta_t$를 찾는 방법:

**Exact line search**:
$$
\eta_t = \arg\min_{\eta > 0} f(x_t - \eta \nabla f(x_t))
$$

**Backtracking line search** (Armijo 조건):
- 초기 $\eta$에서 시작해 조건을 만족할 때까지 $\eta \leftarrow \beta \eta$ ($\beta < 1$)
- 조건: $f(x_t - \eta \nabla f) \le f(x_t) - c \eta \|\nabla f\|^2$ ($c \in (0, 1)$)

**딥러닝에서는**:
- Line search는 비용이 너무 커서 거의 사용 안 함
- 대신 학습률 스케줄링 + adaptive methods 사용

---

## 3. Momentum과 Nesterov

### 3.1 Momentum (Heavy Ball Method)

**동기**:
- GD는 협곡(valley) 형태의 손실 함수에서 지그재그로 진동
- 과거 이동 방향에 관성을 부여해 진동 감소

**알고리즘**:
$$
\begin{align}
v_{t+1} &= \beta v_t + \nabla f(x_t) \\
x_{t+1} &= x_t - \eta v_{t+1}
\end{align}
$$
- $v_t$: velocity (속도)
- $\beta \in [0, 1)$: momentum coefficient (보통 0.9)

**다른 표현** (PyTorch 스타일):
$$
\begin{align}
v_{t+1} &= \beta v_t + (1 - \beta) \nabla f(x_t) \\
x_{t+1} &= x_t - \eta v_{t+1}
\end{align}
$$
(velocity를 지수 이동 평균으로 해석)

**효과**:
- 일관된 방향으로는 가속
- 진동하는 방향은 상쇄
- Convex quadratic에서 수렴 속도 $O(e^{-\sqrt{\kappa}T})$ → $O(e^{-\kappa T})$보다 빠름

### 3.2 Nesterov Accelerated Gradient (NAG)

**개선점**:
- Momentum은 현재 위치에서 gradient 계산
- NAG는 **lookahead 위치** $x_t - \beta v_t$에서 gradient 계산

**알고리즘**:
$$
\begin{align}
v_{t+1} &= \beta v_t + \nabla f(x_t - \beta v_t) \\
x_{t+1} &= x_t - \eta v_{t+1}
\end{align}
$$

**직관**:
- 관성으로 가려는 곳을 미리 보고(lookahead) gradient 계산
- 잘못된 방향으로 가는 것을 미리 감지해 보정

**이론적 개선**:
- Convex smooth에서 $O(1/T^2)$ convergence rate (GD는 $O(1/T)$)
- 실전에서는 Momentum과 비슷한 경우가 많음

---

## 4. Adaptive Learning Rate Methods

### 4.1 Adagrad (Adaptive Gradient)

**동기**:
- 희소 데이터에서 드물게 등장하는 feature는 큰 학습률 필요
- 자주 업데이트되는 파라미터는 작은 학습률 필요

**알고리즘**:
$$
\begin{align}
G_{t} &= G_{t-1} + \nabla f(x_t) \odot \nabla f(x_t) \quad \text{(누적 제곱)} \\
x_{t+1} &= x_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot \nabla f(x_t)
\end{align}
$$
- $G_t$: gradient 제곱의 누적 합
- $\odot$: element-wise 곱/나눗셈
- $\epsilon \approx 10^{-8}$: 0으로 나누기 방지

**장점**:
- 파라미터별로 학습률 자동 조정
- 희소 gradient에 강함 (NLP embedding 등)

**단점**:
- $G_t$가 계속 증가 → 학습률이 너무 빨리 감소
- 깊은 네트워크 학습에서 조기 종료 문제

### 4.2 RMSProp (Root Mean Square Propagation)

**개선점**:
- 누적 합 대신 **지수 이동 평균** 사용
- 과거 정보를 서서히 망각

**알고리즘**:
$$
\begin{align}
v_t &= \beta v_{t-1} + (1 - \beta) \nabla f(x_t) \odot \nabla f(x_t) \\
x_{t+1} &= x_t - \frac{\eta}{\sqrt{v_t + \epsilon}} \odot \nabla f(x_t)
\end{align}
$$
- $\beta \approx 0.9$: decay rate
- $v_t$: gradient 제곱의 지수 이동 평균

**효과**:
- Adagrad의 학습률 감소 문제 해결
- 비정상적(non-stationary) 목적 함수에 적응

### 4.3 Adam (Adaptive Moment Estimation)

**핵심 아이디어**:
- Momentum (1차 모멘트) + RMSProp (2차 모멘트)
- Bias correction으로 초기 학습 안정화

**알고리즘**:
$$
\begin{align}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) \nabla f(x_t) \quad &\text{(1차 모멘트, 평균)} \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) \nabla f(x_t)^2 \quad &\text{(2차 모멘트, 분산)} \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \quad &\text{(bias correction)} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
x_{t+1} &= x_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align}
$$

**하이퍼파라미터** (기본값):
- $\beta_1 = 0.9$: 1차 모멘트 decay
- $\beta_2 = 0.999$: 2차 모멘트 decay
- $\epsilon = 10^{-8}$
- $\eta = 0.001$ (또는 0.0001)

**Bias Correction의 필요성**:
- $m_0 = v_0 = 0$으로 초기화
- 초기 단계에서 $m_t$, $v_t$가 0으로 편향됨
- $1 - \beta^t$로 나눠 보정 ($t$가 클수록 1에 근접)

### 4.4 AdamW (Adam with Decoupled Weight Decay)

**원래 Adam의 문제**:
- L2 정규화 $\lambda \|x\|^2$를 loss에 추가하면
  $$
  \nabla (f + \lambda \|x\|^2) = \nabla f + 2\lambda x
  $$
- 이것이 adaptive scaling에 의해 왜곡됨

**AdamW 해결책**:
- Weight decay를 gradient 계산과 **분리(decouple)**
  $$
  x_{t+1} = (1 - \eta \lambda) x_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
  $$
- Weight decay가 적응적 스케일링의 영향을 받지 않음

**효과**:
- 일반화 성능 향상 (특히 Transformer 등 대규모 모델)
- 현재 딥러닝 표준 optimizer 중 하나

---

## 5. 학습률 스케줄링

### 5.1 주요 전략

**Fixed (고정)**:
- $\eta_t = \eta_0$ (상수)
- 간단하지만 최적 아닐 수 있음

**Step Decay**:
- $\eta_t = \eta_0 \cdot \gamma^{\lfloor t / k \rfloor}$
- 일정 epoch마다 $\gamma$ (예: 0.1) 배로 감소

**Exponential Decay**:
- $\eta_t = \eta_0 e^{-\alpha t}$

**Inverse Decay**:
- $\eta_t = \eta_0 / (1 + \alpha t)$

**Cosine Annealing**:
$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\pi t / T))
$$
- $T$: 전체 step 수
- 부드럽게 감소, 마지막에 fine-tuning

**OneCycle**:
- Warmup (선형 증가) → Annealing (cosine 감소)
- 빠른 수렴 + 좋은 일반화

### 5.2 Warmup

**동기**:
- 초기 파라미터가 무작위 → gradient 불안정
- 작은 학습률로 시작해 서서히 증가

**구현**:
$$
\eta_t = \eta_{\max} \cdot \min(1, t / T_{\text{warmup}})
$$
- $T_{\text{warmup}}$: warmup step 수 (보통 전체의 1~10%)

**효과**:
- Transformer, BERT 등 대규모 모델에서 필수
- Adam과 함께 사용 시 특히 효과적

---

## 6. PyTorch 구현

### 6.1 기본 Gradient Descent

```python
import torch

# Rosenbrock 함수: f(x, y) = (1-x)^2 + 100(y - x^2)^2
def rosenbrock(x):
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

x = torch.tensor([0.0, 0.0], requires_grad=True)
optimizer = torch.optim.SGD([x], lr=0.001)  # 기본 GD (momentum=0)

for step in range(1000):
    optimizer.zero_grad()
    loss = rosenbrock(x)
    loss.backward()
    optimizer.step()

    if step % 100 == 0:
        print(f"Step {step}: x={x.data.numpy()}, f(x)={loss.item():.6f}")
```

### 6.2 Momentum

```python
# SGD with momentum
optimizer = torch.optim.SGD([x], lr=0.001, momentum=0.9)

# 위와 동일한 루프
for step in range(1000):
    optimizer.zero_grad()
    loss = rosenbrock(x)
    loss.backward()
    optimizer.step()
```

### 6.3 Adam

```python
optimizer = torch.optim.Adam([x], lr=0.01, betas=(0.9, 0.999))

for step in range(1000):
    optimizer.zero_grad()
    loss = rosenbrock(x)
    loss.backward()
    optimizer.step()
```

### 6.4 AdamW with Scheduler

```python
from torch.optim.lr_scheduler import CosineAnnealingLR

x = torch.tensor([0.0, 0.0], requires_grad=True)
optimizer = torch.optim.AdamW([x], lr=0.01, weight_decay=0.01)
scheduler = CosineAnnealingLR(optimizer, T_max=1000)

for step in range(1000):
    optimizer.zero_grad()
    loss = rosenbrock(x)
    loss.backward()
    optimizer.step()
    scheduler.step()

    if step % 100 == 0:
        current_lr = scheduler.get_last_lr()[0]
        print(f"Step {step}: lr={current_lr:.6f}, f(x)={loss.item():.6f}")
```

### 6.5 학습률별 비교 시각화

```python
import matplotlib.pyplot as plt
import numpy as np

def optimize_with_method(optimizer_class, **kwargs):
    x = torch.tensor([0.0, 0.0], requires_grad=True)
    optimizer = optimizer_class([x], **kwargs)

    path = []
    for _ in range(500):
        optimizer.zero_grad()
        loss = rosenbrock(x)
        loss.backward()
        optimizer.step()
        path.append(x.detach().clone().numpy())

    return np.array(path)

# 여러 방법 비교
methods = [
    ("GD (lr=0.001)", torch.optim.SGD, {"lr": 0.001}),
    ("Momentum", torch.optim.SGD, {"lr": 0.001, "momentum": 0.9}),
    ("Adam", torch.optim.Adam, {"lr": 0.01}),
    ("AdamW", torch.optim.AdamW, {"lr": 0.01, "weight_decay": 0.01}),
]

plt.figure(figsize=(10, 8))
for name, opt_class, kwargs in methods:
    path = optimize_with_method(opt_class, **kwargs)
    plt.plot(path[:, 0], path[:, 1], label=name, alpha=0.7)

plt.plot(1, 1, 'r*', markersize=15, label='Optimum (1, 1)')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Optimization Paths on Rosenbrock Function')
plt.legend()
plt.grid(True)
plt.show()
```

---

## 7. 실전 가이드

### 7.1 Optimizer 선택

| 상황 | 추천 Optimizer | 이유 |
|------|---------------|------|
| **일반적 경우** | AdamW | 빠른 수렴 + 좋은 일반화 |
| **대규모 Transformer** | AdamW + warmup + cosine | 표준 레시피 |
| **CNN (ResNet 등)** | SGD + momentum | 장기 학습 시 더 나은 일반화 |
| **희소 데이터 (NLP embedding)** | Adagrad, Adam | 파라미터별 adaptive 필요 |
| **Fine-tuning** | AdamW (작은 lr) | 기존 가중치 보존 |

### 7.2 학습률 튜닝

**LR Range Test** (Leslie Smith):
1. 매우 작은 lr (예: $10^{-7}$)에서 시작
2. 매 iteration마다 lr 지수 증가
3. Loss 발산 직전까지의 최대 lr 찾기
4. 그 값의 1/3~1/10 사용

**일반적 시작점**:
- Adam/AdamW: 0.001 또는 0.0001
- SGD+Momentum: 0.01~0.1
- 배치 크기에 비례 스케일링: $\text{lr} \propto \sqrt{\text{batch size}}$

### 7.3 Gradient Clipping

큰 gradient로 인한 불안정성 방지:

```python
# Norm clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# Value clipping
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
```

**언제 사용**:
- RNN/LSTM (exploding gradient 문제)
- Transformer (선택적, 보통 필요 없음)

---

## 8. 실습 과제

1. **Rosenbrock 최적화**
   - GD, Momentum, NAG, Adam으로 각각 최적화
   - 수렴 속도와 경로를 시각화하고 비교

2. **학습률 민감도**
   - Quadratic 함수 $f(x, y) = x^2 + 10y^2$에서
   - lr = 0.01, 0.1, 0.3, 0.5로 GD 수행
   - 각 경우의 수렴 여부와 속도 분석

3. **Momentum 효과 시연**
   - 협곡형 함수 (conditioning number가 큰 quadratic)
   - $\beta = 0, 0.5, 0.9, 0.99$로 변화시키며 관찰

4. **Adam vs SGD 비교**
   - MNIST/CIFAR-10에서 동일한 CNN 학습
   - 각 optimizer의 train/val loss 곡선 비교
   - 최종 test accuracy 비교

5. **Learning Rate Schedule**
   - Constant, Step Decay, Cosine Annealing 비교
   - Warmup 유무에 따른 초기 학습 안정성 관찰

---

## 9. 참고 자료

### 논문
- Nesterov (1983), "A method for solving the convex programming problem with convergence rate O(1/k²)"
- Duchi et al. (2011), "Adaptive Subgradient Methods for Online Learning"
- Kingma & Ba (2015), "Adam: A Method for Stochastic Optimization"
- Loshchilov & Hutter (2019), "Decoupled Weight Decay Regularization" (AdamW)
- Smith (2017), "Cyclical Learning Rates for Training Neural Networks"

### 교과서
- Goodfellow et al., *Deep Learning*, Chapter 8
- Ruder (2016), "An overview of gradient descent optimization algorithms" (blog post, 필독)

### 온라인 강의
- CS231n: Lecture on Optimization
- Fast.ai: Lesson on Learning Rate Finder and OneCycle

---

## 10. 다음 학습

1차 최적화를 마쳤다면:

1. **2차 방법으로**
   - [[2차 최적화 기법|2차 최적화 기법]]
   - Newton, BFGS, L-BFGS

2. **이론적 분석**
   - [[볼록성과 제약 최적화|볼록성과 제약 최적화]]
   - 수렴성 증명, KKT 조건

3. **실전 응용**
   - [[ML Foundations/머신러닝 최적화|ML에서의 최적화]]
   - [[Training/Optimizers and Schedulers|Optimizer와 Scheduler 실전]]

4. **상위 개념**
   - [[최적화 기초|최적화 기초 허브]]
