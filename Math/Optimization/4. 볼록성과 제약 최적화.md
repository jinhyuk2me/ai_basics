## TL;DR
- **Convex 최적화**는 지역 최소값이 곧 전역 최소값이 되는 이론적으로 깔끔한 영역으로, SVM, Lasso 등 전통적 ML의 이론적 기초다.
- **KKT 조건**은 제약이 있는 최적화 문제의 최적해가 만족해야 할 필요조건으로, L1/L2 정규화, SVM 등을 수학적으로 분석하는 핵심 도구다.
- 딥러닝은 대부분 **non-convex**이지만, SGD의 implicit bias, flat/sharp minima, loss landscape 분석 등을 통해 경험적 성공을 이해할 수 있다.

---

## 1. 핵심 개념

### 1.1 Convex Set

**정의**:
집합 $C \subseteq \mathbb{R}^n$이 convex이다 $\Leftrightarrow$
$$
\forall x, y \in C, \; \forall t \in [0, 1]: \quad tx + (1-t)y \in C
$$
(두 점을 잇는 선분이 전부 집합 안에 포함)

**예시**:
- **Convex**: $\mathbb{R}^n$, 초평면, 공, 타원, 반공간
- **Non-convex**: 도넛 모양, 두 개의 분리된 공

**연산**:
- 교집합: Convex 집합들의 교집합은 convex
- Affine 변환: $f(C) = \{Ax + b : x \in C\}$ (convex 보존)

### 1.2 Convex Function

**정의**:
함수 $f: \mathbb{R}^n \to \mathbb{R}$이 convex이다 $\Leftrightarrow$
$$
\forall x, y, \; \forall t \in [0, 1]: \quad f(tx + (1-t)y) \le tf(x) + (1-t)f(y)
$$
(함수 그래프가 두 점을 잇는 직선 아래에 위치)

**Strictly convex**:
위 부등식이 $x \ne y$, $t \in (0, 1)$일 때 strict ($<$)

**1차 조건** (미분 가능 시):
$$
f \text{ is convex} \Leftrightarrow f(y) \ge f(x) + \nabla f(x)^\top (y - x), \; \forall x, y
$$
(함수가 항상 접선 위에)

**2차 조건** (2번 미분 가능 시):
$$
f \text{ is convex} \Leftrightarrow \nabla^2 f(x) \succeq 0, \; \forall x
$$
(Hessian이 everywhere positive semidefinite)

### 1.3 왜 Convexity가 중요한가?

**이론적 장점**:
1. **지역 최소 = 전역 최소**
   - 지역 최소값 $x^*$: $f(x^*) \le f(x)$ for $\|x - x^*\| < \epsilon$
   - Convex면 자동으로 전역 최소값

2. **최적성 조건이 필요충분**
   - $\nabla f(x^*) = 0$ $\Leftrightarrow$ $x^*$는 global minimum

3. **수렴 보장**
   - Gradient descent 등 알고리즘이 provable convergence

**실전 응용**:
- SVM: Hinge loss + L2 정규화 → convex
- Lasso: L1 정규화 → convex (미분 불가능하지만)
- Logistic regression: Cross-entropy + L2 → convex

---

## 2. 수학적 전개

### 2.1 Convex Function의 성질

**명제 1**: $f$가 convex이고 미분 가능하면
$$
(\nabla f(x) - \nabla f(y))^\top (x - y) \ge 0, \quad \forall x, y
$$
(gradient가 monotone)

**증명**:
1차 조건을 $x$와 $y$에 각각 적용:
$$
\begin{align}
f(y) &\ge f(x) + \nabla f(x)^\top (y - x) \\
f(x) &\ge f(y) + \nabla f(y)^\top (x - y)
\end{align}
$$
두 식을 더하면:
$$
0 \ge \nabla f(x)^\top (y - x) + \nabla f(y)^\top (x - y) = -(\nabla f(x) - \nabla f(y))^\top (x - y)
$$
∎

**명제 2**: $f$가 convex이고 $\nabla f$가 $L$-Lipschitz면 (smooth)
$$
f(y) \le f(x) + \nabla f(x)^\top (y - x) + \frac{L}{2} \|y - x\|^2
$$
(2차 상한)

### 2.2 Strong Convexity

**정의**:
$f$가 $m$-strongly convex $\Leftrightarrow$ $\exists m > 0$:
$$
f(tx + (1-t)y) \le tf(x) + (1-t)f(y) - \frac{m}{2} t(1-t) \|x - y\|^2
$$

**또는 (2차 조건)**:
$$
\nabla^2 f(x) \succeq mI, \quad \forall x
$$
(Hessian의 최소 고유값 $\ge m$)

**효과**:
- 최소값이 **유일**
- Gradient descent가 **linear convergence**: $\|x_k - x^*\| \le (1 - m/L)^k \|x_0 - x^*\|$

**예시**:
- $f(x) = \frac{1}{2} x^\top A x - b^\top x$ ($A \succ 0$): $m = \lambda_{\min}(A)$

### 2.3 Jensen's Inequality

Convex function $f$에 대해:
$$
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)]
$$

**이산 형태**:
$$
f\left( \sum_{i=1}^n \lambda_i x_i \right) \le \sum_{i=1}^n \lambda_i f(x_i), \quad \lambda_i \ge 0, \sum \lambda_i = 1
$$

**응용**:
- KL divergence non-negativity 증명
- EM 알고리즘의 이론적 근거

---

## 3. 제약 최적화

### 3.1 문제 형태

**일반 형태**:
$$
\begin{align}
\min_{x \in \mathbb{R}^n} \quad & f(x) \\
\text{subject to} \quad & g_i(x) \le 0, \quad i = 1, \ldots, m \\
& h_j(x) = 0, \quad j = 1, \ldots, p
\end{align}
$$
- $f$: 목적 함수
- $g_i$: 부등식 제약
- $h_j$: 등식 제약

**Feasible set**:
$$
\mathcal{F} = \{x : g_i(x) \le 0, \; h_j(x) = 0, \; \forall i, j\}
$$

### 3.2 Lagrangian

**정의**:
$$
\mathcal{L}(x, \lambda, \nu) = f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{j=1}^p \nu_j h_j(x)
$$
- $\lambda_i \ge 0$: 부등식 제약의 Lagrange 승수
- $\nu_j \in \mathbb{R}$: 등식 제약의 Lagrange 승수

**직관**:
- 제약을 만족하지 않으면 penalty 부여
- $\lambda_i$가 클수록 $g_i(x) \le 0$ 위반 시 큰 벌점

### 3.3 KKT 조건 (Karush-Kuhn-Tucker)

최적해 $(x^*, \lambda^*, \nu^*)$가 만족하는 조건:

**1. Stationarity** (정류 조건):
$$
\nabla_x \mathcal{L}(x^*, \lambda^*, \nu^*) = \nabla f(x^*) + \sum_{i=1}^m \lambda_i^* \nabla g_i(x^*) + \sum_{j=1}^p \nu_j^* \nabla h_j(x^*) = 0
$$

**2. Primal Feasibility** (원 문제 실행 가능성):
$$
g_i(x^*) \le 0, \quad h_j(x^*) = 0, \quad \forall i, j
$$

**3. Dual Feasibility** (쌍대 실행 가능성):
$$
\lambda_i^* \ge 0, \quad \forall i
$$

**4. Complementary Slackness** (상호 보완성):
$$
\lambda_i^* g_i(x^*) = 0, \quad \forall i
$$
(제약이 active하지 않으면 $\lambda_i^* = 0$, active면 $g_i(x^*) = 0$)

**의미**:
- Convex 문제 + constraint qualification → KKT는 **필요충분조건**
- Non-convex에서는 **필요조건**만

---

## 4. KKT 조건의 응용

### 4.1 예제 1: Equality Constraint

**문제**:
$$
\min \; x^2 + y^2 \quad \text{s.t.} \quad x + y = 1
$$

**Lagrangian**:
$$
\mathcal{L}(x, y, \nu) = x^2 + y^2 + \nu (x + y - 1)
$$

**KKT 조건**:
$$
\begin{align}
\frac{\partial \mathcal{L}}{\partial x} &= 2x + \nu = 0 \\
\frac{\partial \mathcal{L}}{\partial y} &= 2y + \nu = 0 \\
x + y &= 1
\end{align}
$$

**풀이**:
- $2x + \nu = 0$, $2y + \nu = 0$ → $x = y$
- $x + y = 1$ → $x = y = 1/2$
- $\nu = -1$

**최소값**: $f(1/2, 1/2) = 1/2$

### 4.2 예제 2: Inequality Constraint

**문제**:
$$
\min \; (x - 2)^2 + (y - 1)^2 \quad \text{s.t.} \quad x + y \le 1
$$

**Lagrangian**:
$$
\mathcal{L}(x, y, \lambda) = (x-2)^2 + (y-1)^2 + \lambda(x + y - 1)
$$

**KKT 조건**:
$$
\begin{align}
2(x - 2) + \lambda &= 0 \\
2(y - 1) + \lambda &= 0 \\
x + y &\le 1 \\
\lambda &\ge 0 \\
\lambda (x + y - 1) &= 0
\end{align}
$$

**Case 1**: $\lambda = 0$ (제약 inactive)
- $x = 2, y = 1$ → $x + y = 3 > 1$ (불가능)

**Case 2**: $\lambda > 0$ (제약 active, $x + y = 1$)
- $x = 2 - \lambda/2$, $y = 1 - \lambda/2$
- $x + y = 3 - \lambda = 1$ → $\lambda = 2$
- $x = 1, y = 0$

**최소값**: $f(1, 0) = 1 + 1 = 2$

### 4.3 SVM의 KKT 조건

**SVM Primal**:
$$
\min_{w, b} \; \frac{1}{2} \|w\|^2 \quad \text{s.t.} \quad y_i(w^\top x_i + b) \ge 1, \; \forall i
$$

**Lagrangian**:
$$
\mathcal{L}(w, b, \alpha) = \frac{1}{2} \|w\|^2 - \sum_{i=1}^n \alpha_i [y_i(w^\top x_i + b) - 1]
$$

**KKT 조건**:
$$
\begin{align}
\nabla_w \mathcal{L} &= w - \sum_i \alpha_i y_i x_i = 0 \quad \Rightarrow \quad w = \sum_i \alpha_i y_i x_i \\
\frac{\partial \mathcal{L}}{\partial b} &= -\sum_i \alpha_i y_i = 0 \\
\alpha_i &\ge 0 \\
y_i(w^\top x_i + b) &\ge 1 \\
\alpha_i [y_i(w^\top x_i + b) - 1] &= 0 \quad \text{(complementarity)}
\end{align}
$$

**해석**:
- $\alpha_i > 0$인 $x_i$만 support vector ($y_i(w^\top x_i + b) = 1$)
- 대부분의 $\alpha_i = 0$ → sparse solution

---

## 5. Non-convex 최적화 (딥러닝)

### 5.1 딥러닝의 Non-convexity

**왜 non-convex?**
- ReLU, sigmoid 등 비선형 activation
- 여러 층의 합성 → 복잡한 loss landscape
- Hessian이 indefinite (양/음 고유값 혼재)

**문제점**:
- 지역 최소값이 무수히 많음
- Saddle point (안장점) 존재
- 전역 최적 보장 없음

### 5.2 왜 실전에서는 잘 작동하는가?

**이론 1: Over-parameterization**
- 파라미터 수 >> 데이터 수
- 대부분의 지역 최소값이 비슷한 성능 (loss)
- Saddle point는 gradient가 0이 아니라 탈출 가능

**이론 2: SGD의 Implicit Regularization**
- SGD의 noise가 sharp minima 탈출 유도
- Flat minima로 수렴 → 일반화 성능 좋음

**이론 3: Loss Landscape 구조**
- 많은 실험에서 지역 최소값들이 "connected by low-loss path"
- Mode connectivity: 두 solution 사이를 보간해도 loss 크게 안 오름

### 5.3 Flat vs Sharp Minima

**정의** (Hochreiter & Schmidhuber, 1997):
- **Sharp minimum**: Hessian 고유값 큼 → 파라미터 작은 변화에 loss 급변
- **Flat minimum**: Hessian 고유값 작음 → 파라미터 변화에 robust

**일반화 관계**:
- Flat minimum이 test error 낮음
- Sharp minimum은 training data에 overfitting

**Sharpness 측정**:
$$
\text{Sharpness} = \max_{\|\epsilon\| \le \rho} \frac{L(\theta + \epsilon) - L(\theta)}{1 + L(\theta)}
$$

**SAM (Sharpness-Aware Minimization)**:
$$
\min_\theta \max_{\|\epsilon\| \le \rho} L(\theta + \epsilon)
$$
- 명시적으로 flat region 찾기
- SOTA 성능 in image classification

### 5.4 배치 크기와 Generalization

**경험적 관찰** (Keskar et al., 2017):
- **작은 배치 (32~128)**: Flat minima 선호, 높은 test accuracy
- **큰 배치 (1024~4096)**: Sharp minima, 낮은 test accuracy

**이유**:
- 작은 배치 → gradient noise 큼 → sharp minimum에서 탈출
- 큰 배치 → gradient 정확 → sharp minimum에 갇힘

**해결책**:
- Learning rate 스케일링: $\text{lr} \propto \sqrt{\text{batch size}}$
- Warmup + 긴 학습 시간

---

## 6. PyTorch 구현

### 6.1 Convexity 확인

```python
import torch
import numpy as np

def is_convex_hessian(f, x_samples):
    """Check if f is convex by examining Hessian eigenvalues"""
    from torch.autograd.functional import hessian

    for x in x_samples:
        H = hessian(f, x)
        eigvals = torch.linalg.eigvalsh(H)
        if (eigvals < -1e-6).any():  # Negative eigenvalue
            return False
    return True

# 예: f(x, y) = x^2 + 2y^2 (convex)
def f_convex(x):
    return x[0]**2 + 2*x[1]**2

samples = [torch.randn(2) for _ in range(10)]
print("Convex?", is_convex_hessian(f_convex, samples))
```

### 6.2 KKT 조건으로 Lasso 해석

**Lasso**:
$$
\min_w \; \frac{1}{2} \|y - Xw\|^2 + \lambda \|w\|_1
$$

**Subgradient 조건** ($\|w\|_1$은 미분 불가):
$$
-X^\top (y - Xw) + \lambda \partial \|w\|_1 \ni 0
$$

**$w_i \ne 0$일 때**:
$$
-X_i^\top (y - Xw) + \lambda \cdot \text{sign}(w_i) = 0
$$

**$w_i = 0$일 때**:
$$
|X_i^\top (y - Xw)| \le \lambda
$$

**해석**: gradient가 $\lambda$보다 작으면 $w_i = 0$ (sparsity)

### 6.3 Sharpness 측정

```python
import torch

def compute_sharpness(model, loss_fn, dataloader, rho=0.05):
    """Compute sharpness of a trained model"""
    params = [p for p in model.parameters() if p.requires_grad]

    # Original loss
    model.eval()
    with torch.no_grad():
        loss_orig = sum(loss_fn(model(x), y).item()
                        for x, y in dataloader) / len(dataloader)

    # Perturb parameters
    perturbations = [torch.randn_like(p) for p in params]
    norm = torch.sqrt(sum((p**2).sum() for p in perturbations))
    for p, pert in zip(params, perturbations):
        p.data += rho * pert / norm

    # Perturbed loss
    with torch.no_grad():
        loss_pert = sum(loss_fn(model(x), y).item()
                        for x, y in dataloader) / len(dataloader)

    # Restore
    for p, pert in zip(params, perturbations):
        p.data -= rho * pert / norm

    sharpness = (loss_pert - loss_orig) / (1 + loss_orig)
    return sharpness

# 사용 예
sharpness = compute_sharpness(model, nn.CrossEntropyLoss(), test_loader)
print(f"Sharpness: {sharpness:.4f}")
```

---

## 7. 실전 가이드

### 7.1 Convex vs Non-convex 판별

| 모델/손실 | Convex? | 이유 |
|----------|---------|------|
| **Linear Regression (MSE)** | ✅ | $\|y - Xw\|^2$는 quadratic |
| **Logistic Regression** | ✅ | Log-loss는 convex |
| **SVM (Hinge Loss)** | ✅ | Max는 convex 보존 |
| **Lasso (L1)** | ✅ | Norm은 convex |
| **Neural Network** | ❌ | 비선형 합성 |
| **Matrix Factorization** | ❌ | Bilinear ($UV^\top$) |

### 7.2 제약 최적화 도구

**PyTorch에는 제약 최적화 직접 지원 없음** → 대안:

**1. Penalty Method**:
$$
\min_x \; f(x) + \mu \sum_i \max(0, g_i(x))^2
$$
제약 위반에 큰 penalty

**2. Projected Gradient Descent**:
- GD step 후 feasible set으로 projection
- 예: $\|w\| \le R$ → projection: $w \leftarrow R \cdot w / \max(R, \|w\|)$

**3. Augmented Lagrangian** (ADMM):
- Lagrangian에 quadratic penalty 추가
- 반복적으로 $x$와 $\lambda$ 업데이트

**4. 외부 라이브러리**:
- `cvxpy` (Python convex optimization)
- `scipy.optimize.minimize` (method='SLSQP')

---

## 8. 실습 과제

1. **Convex Function 확인**
   - $f(x, y) = e^x + e^y$, $g(x, y) = -\log(x) - \log(y)$ ($x, y > 0$)
   - 각각의 Hessian을 계산하고 PSD 확인
   - Contour plot으로 convexity 시각화

2. **KKT 조건 풀이**
   - $\min \; x^2 + y^2 + z^2$ subject to $x + 2y + 3z = 6$
   - Lagrangian 세우고 KKT로 해 구하기
   - `scipy.optimize` 결과와 비교

3. **Lasso Path**
   - $\lambda$를 0.01에서 10까지 변화시키며 Lasso 풀기
   - 각 $\lambda$에서 $w$의 sparsity (0인 원소 개수) 계산
   - $\lambda$-sparsity plot 그리기

4. **Sharp vs Flat Minima 실험**
   - CIFAR-10에서 배치 크기 32 vs 512로 학습
   - 각 모델의 sharpness 측정
   - Test accuracy와 sharpness 상관관계 분석

5. **Constrained Optimization**
   - 2D Rosenbrock 함수에 $x^2 + y^2 \le 1$ 제약
   - Penalty method로 풀기
   - Projected GD와 비교
