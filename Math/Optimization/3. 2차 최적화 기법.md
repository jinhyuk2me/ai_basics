## TL;DR
- **2차 최적화 기법**은 Hessian(2차 도함수 행렬)을 활용해 함수의 곡률 정보를 반영하여 더 빠른 수렴을 달성한다.
- Newton 방법은 이론적으로 2차 수렴(quadratic convergence)을 보이지만, $O(n^3)$ 계산 복잡도로 대규모 딥러닝에는 직접 적용이 어렵다.
- Quasi-Newton 방법(BFGS, L-BFGS)은 Hessian 근사를 통해 메모리와 계산 비용을 절감하며, 중소규모 문제나 full-batch 최적화에서 유용하다.

---

## 1. 핵심 개념

### 1.1 왜 2차 정보가 필요한가?

**1차 방법 (Gradient Descent)의 한계**:
- Gradient는 **순간 변화율**만 제공
- 함수의 **곡률(curvature)** 정보 없음
- 방향별로 기울기가 다르면 (ill-conditioned) 수렴이 매우 느림

**2차 방법의 장점**:
- Hessian $H = \nabla^2 f$가 곡률 정보 제공
- 최적 step size와 방향을 자동으로 결정
- Quadratic 함수에서는 **한 번에 수렴**

**Trade-off**:
- Hessian 계산: $O(n^2)$ 메모리, $O(n^3)$ 시간 (역행렬)
- 파라미터 $n$이 백만~억 단위인 딥러닝에서는 비현실적
- 근사 방법(Quasi-Newton, natural gradient)으로 절충

### 1.2 2차 Taylor 근사

함수 $f$를 $x$ 근처에서 2차 근사:
$$
f(x + \Delta x) \approx f(x) + \nabla f(x)^\top \Delta x + \frac{1}{2} \Delta x^\top H(x) \Delta x
$$

**최소값 조건**:
위 근사를 $\Delta x$에 대해 최소화:
$$
\nabla_{\Delta x} \left[ f(x) + \nabla f^\top \Delta x + \frac{1}{2} \Delta x^\top H \Delta x \right] = \nabla f + H \Delta x = 0
$$

따라서:
$$
\Delta x^* = -H^{-1} \nabla f
$$

이것이 **Newton step**이다.

---

## 2. Newton 방법

### 2.1 알고리즘

**반복식**:
$$
x_{k+1} = x_k - H_k^{-1} \nabla f(x_k)
$$
- $H_k = \nabla^2 f(x_k)$: Hessian at $x_k$
- $\nabla f(x_k)$: Gradient at $x_k$

**실제 구현**:
- Hessian 역행렬을 직접 구하지 않고 선형 방정식 풀기:
  $$
  H_k \Delta x = -\nabla f(x_k)
  $$
- Cholesky 분해 ($H \succ 0$일 때) 또는 LU 분해 사용

### 2.2 수렴성 분석

**Quadratic 함수** $f(x) = \frac{1}{2} x^\top A x - b^\top x$:
- Hessian $H = A$ (상수)
- Newton step: $\Delta x = -A^{-1} (Ax - b) = x^* - x$
- **한 번에 정확한 최소값 도달**

**일반 함수** (strongly convex, smooth):
- 최적값 근처에서 **2차 수렴(quadratic convergence)**:
  $$
  \|x_{k+1} - x^*\| \le C \|x_k - x^*\|^2
  $$
- Gradient Descent의 선형 수렴 $\|x_{k+1} - x^*\| \le \rho \|x_k - x^*\|$ ($\rho < 1$)보다 훨씬 빠름

**초기값에 민감**:
- 최적값에서 멀면 Hessian이 positive definite 보장 안 됨
- Line search 또는 trust region과 결합 필요

### 2.3 Hessian 계산

**정의**:
$$
H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
$$

**PyTorch autograd**:
```python
import torch

x = torch.tensor([1.0, 2.0], requires_grad=True)
f = (x[0]**2 + 2*x[1]**2).sum()

# Gradient
grad_f = torch.autograd.grad(f, x, create_graph=True)[0]

# Hessian (각 gradient 성분을 다시 미분)
hessian = torch.zeros(2, 2)
for i in range(2):
    hessian[i] = torch.autograd.grad(grad_f[i], x, retain_graph=True)[0]

print(hessian)
# tensor([[2., 0.],
#         [0., 4.]])
```

**또는 `torch.autograd.functional.hessian`**:
```python
from torch.autograd.functional import hessian

def f(x):
    return (x[0]**2 + 2*x[1]**2)

x = torch.tensor([1.0, 2.0])
H = hessian(f, x)
print(H)
```

**딥러닝에서의 문제**:
- 파라미터 $n = 10^8$이면 Hessian은 $10^{16}$ 원소 → 메모리 불가능
- Hessian-vector product $Hv$는 $O(n)$에 계산 가능 (자동 미분)

---

## 3. Quasi-Newton 방법

### 3.1 핵심 아이디어

**문제**:
- Newton 방법은 $H^{-1}$ 계산이 $O(n^3)$
- 매 iteration마다 Hessian 재계산 필요

**해결책**:
- Hessian의 **근사 행렬** $B_k$를 유지
- Gradient 정보만으로 $B_k$를 **점진적으로 업데이트**
- $B_k$가 실제 Hessian에 근사하도록 조건 부여

### 3.2 Secant Equation (Quasi-Newton 조건)

Iteration $k$에서 $k+1$로 이동 시:
$$
\begin{align}
s_k &= x_{k+1} - x_k \\
y_k &= \nabla f(x_{k+1}) - \nabla f(x_k)
\end{align}
$$

**평균값 정리**에 의해:
$$
y_k \approx H_{k+1} s_k
$$

따라서 $B_{k+1}$이 만족해야 할 조건:
$$
B_{k+1} s_k = y_k \quad \text{(Secant equation)}
$$

이 조건을 만족하면서 $B_k$에서 최소한으로 변하는 $B_{k+1}$을 구한다.

### 3.3 BFGS (Broyden-Fletcher-Goldfarb-Shanno)

**가장 널리 쓰이는 Quasi-Newton 방법**

**업데이트 공식** (Hessian 근사 $B_k$):
$$
B_{k+1} = B_k - \frac{B_k s_k s_k^\top B_k}{s_k^\top B_k s_k} + \frac{y_k y_k^\top}{y_k^\top s_k}
$$

**또는 역 Hessian 근사 $H_k \approx B_k^{-1}$** (Sherman-Morrison-Woodbury 공식):
$$
H_{k+1} = \left( I - \frac{s_k y_k^\top}{y_k^\top s_k} \right) H_k \left( I - \frac{y_k s_k^\top}{y_k^\top s_k} \right) + \frac{s_k s_k^\top}{y_k^\top s_k}
$$

**장점**:
- $O(n^2)$ 메모리, $O(n^2)$ 시간 (역행렬 계산 불필요)
- Superlinear convergence (Newton보다는 느리지만 GD보다 빠름)

**조건**:
- $y_k^\top s_k > 0$ (curvature condition) 필요
- Line search로 보장 가능

### 3.4 L-BFGS (Limited-memory BFGS)

**동기**:
- BFGS는 $O(n^2)$ 메모리 → $n$이 크면 여전히 부담
- 전체 $H_k$를 저장하지 않고 최근 $m$개의 $(s_k, y_k)$ 쌍만 저장

**메모리**:
- $O(mn)$ ($m \sim 5$~$20$)
- $n = 10^6$, $m = 10$이면 약 10MB (실용적)

**알고리즘**:
1. 최근 $m$개의 $(s_i, y_i)$ 저장
2. 두 루프(two-loop recursion)로 $H_k \nabla f$를 $O(mn)$에 계산
3. $x_{k+1} = x_k - \alpha H_k \nabla f(x_k)$ (line search로 $\alpha$ 결정)

**PyTorch 구현**:
```python
import torch.optim as optim

optimizer = optim.LBFGS(model.parameters(), lr=1.0, max_iter=20, history_size=10)

def closure():
    optimizer.zero_grad()
    output = model(input)
    loss = criterion(output, target)
    loss.backward()
    return loss

optimizer.step(closure)
```

**주의사항**:
- **Full-batch gradient** 필요 (mini-batch에서는 noise 때문에 불안정)
- Closure 함수로 여러 번 forward/backward 가능
- 작은 데이터셋이나 fine-tuning에 적합

---

## 4. Natural Gradient와 Preconditioning

### 4.1 Natural Gradient

**동기**:
- Euclidean gradient는 파라미터 공간의 좌표계에 의존
- 확률 모델 $p(x; \theta)$의 경우 **Fisher Information Matrix** $F(\theta)$가 자연스러운 metric

**정의**:
$$
\tilde{\nabla} f = F^{-1} \nabla f
$$
- $F_{ij} = \mathbb{E}_{x \sim p}\left[ \frac{\partial \log p}{\partial \theta_i} \frac{\partial \log p}{\partial \theta_j} \right]$

**효과**:
- 파라미터화에 불변(invariant)
- KL divergence의 기하학적 구조 반영

**문제**:
- Fisher matrix 계산과 역행렬이 비쌈
- 근사 방법: K-FAC, natural gradient with diagonal approximation

### 4.2 Preconditioning

**일반 개념**:
- 변수 변환으로 문제의 조건수(condition number) 개선
- $\tilde{x} = P^{-1/2} x$ → Hessian이 $P^{-1/2} H P^{-1/2}$로 변환

**Adaptive methods (Adam 등)의 해석**:
- Adam의 $\sqrt{v_t}$가 일종의 diagonal preconditioner
- 파라미터별로 다른 학습률 → 효과적인 reconditioning

---

## 5. 딥러닝에서의 2차 방법

### 5.1 사용 현황

**거의 사용 안 함**:
- SGD, Adam/AdamW가 표준
- Mini-batch 기반 학습에서 2차 방법 불안정

**예외적 사용**:
- Full-batch 최적화 (작은 데이터, 과학 컴퓨팅)
- Fine-tuning 마지막 단계 (L-BFGS로 polish)
- Meta-learning (MAML 등에서 2차 도함수)

### 5.2 왜 잘 안 쓰이는가?

**이유 1: Stochastic Noise**
- Mini-batch gradient에 noise → Hessian 근사 불안정
- Quasi-Newton의 secant equation이 성립 안 함

**이유 2: 비용**
- L-BFGS도 full-batch forward/backward 여러 번 필요
- 대규모 데이터셋에서 비현실적

**이유 3: Non-convexity**
- Hessian이 indefinite (음의 고유값 존재) → Newton step이 상승 방향일 수 있음
- Trust region, line search 등 추가 장치 필요

**이유 4: 경험적 성능**
- Adam + lr scheduling이 충분히 잘 작동
- Flat minima 선호 등 최적화 너머의 요인 중요

### 5.3 Hessian-free Optimization

**아이디어**:
- Hessian을 명시적으로 구하지 않고 **Hessian-vector product** $Hv$ 계산
- Conjugate Gradient로 $H \Delta x = -\nabla f$ 풀기

**장점**:
- $Hv$는 자동 미분으로 $O(n)$
- 메모리 효율적

**단점**:
- 여전히 여러 번의 forward/backward 필요
- RNN 학습 등에서 제한적으로 사용됨 (Martens, 2010)

---

## 6. PyTorch 구현 예제

### 6.1 Newton 방법 (작은 문제)

```python
import torch

# 2차 함수: f(x, y) = x^2 + 10y^2
def f(x):
    return x[0]**2 + 10 * x[1]**2

def newton_step(x):
    x_var = x.clone().requires_grad_(True)
    loss = f(x_var)

    # Gradient
    grad = torch.autograd.grad(loss, x_var, create_graph=True)[0]

    # Hessian
    H = torch.zeros(2, 2)
    for i in range(2):
        H[i] = torch.autograd.grad(grad[i], x_var, retain_graph=True)[0]

    # Newton step: solve H * delta = -grad
    delta = -torch.linalg.solve(H, grad)
    return x + delta

# 초기값
x = torch.tensor([5.0, 5.0])

print("Iteration 0:", x.numpy(), f(x).item())
x = newton_step(x)
print("Iteration 1:", x.numpy(), f(x).item())  # 거의 0으로 수렴
```

**출력**:
```
Iteration 0: [5. 5.] 275.0
Iteration 1: [3.7252903e-08 9.3132257e-09] 1.5544133e-15
```
→ Quadratic이므로 1 step에 최적해 도달

### 6.2 L-BFGS 최적화

```python
import torch
import torch.optim as optim

# Rosenbrock 함수
def rosenbrock(x):
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

x = torch.tensor([0.0, 0.0], requires_grad=True)
optimizer = optim.LBFGS([x], lr=1, max_iter=20, history_size=10)

def closure():
    optimizer.zero_grad()
    loss = rosenbrock(x)
    loss.backward()
    return loss

# L-BFGS는 step()이 closure를 여러 번 호출 가능
for step in range(10):
    loss = optimizer.step(closure)
    if step % 2 == 0:
        print(f"Step {step}: x={x.detach().numpy()}, f(x)={loss.item():.6f}")
```

**비교**: Adam은 수백~수천 step 필요, L-BFGS는 10~50 step

### 6.3 Hessian Eigenvalues (Sharpness 분석)

```python
from torch.autograd.functional import hessian
import torch

def loss_fn(params):
    # 간단한 예: 학습된 모델의 loss
    x, y = params[0], params[1]
    return (x - 1)**2 + 10 * (y - 2)**2

# 최소값 근처
params = torch.tensor([1.0, 2.0])
H = hessian(loss_fn, params)

# Eigenvalues
eigenvalues = torch.linalg.eigvalsh(H)
print("Hessian eigenvalues:", eigenvalues.numpy())
print("Condition number:", (eigenvalues.max() / eigenvalues.min()).item())
```

**Sharpness**:
- 고유값이 크면 sharp minimum (일반화 나쁨)
- 고유값이 작으면 flat minimum (일반화 좋음)

---

## 7. 실전 가이드

### 7.1 2차 방법 사용 시점

| 상황 | 방법 | 설정 |
|------|------|------|
| **작은 데이터 (< 10K)** | L-BFGS | Full-batch, history_size=10 |
| **Fine-tuning 마지막** | L-BFGS | 수십 step, lr=1.0 |
| **과학 컴퓨팅 (물리 시뮬레이션)** | Newton + line search | 정확한 수렴 필요 |
| **일반 딥러닝** | Adam/SGD | 2차 방법 불필요 |

### 7.2 L-BFGS 하이퍼파라미터

```python
optim.LBFGS(
    params,
    lr=1.0,              # 보통 1.0 (line search가 자동 조정)
    max_iter=20,         # Closure 호출 최대 횟수
    history_size=10,     # 저장할 (s, y) 쌍 개수
    line_search_fn='strong_wolfe'  # Line search 방법
)
```

**Closure 함수 필수**:
- Line search 중 여러 번 loss 계산 필요
- `closure()`는 zero_grad, forward, backward, return loss

### 7.3 주의사항

**1. Full-batch gradient 권장**:
```python
# 전체 데이터로 gradient 계산
for inputs, targets in dataloader:
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
optimizer.step(closure)
```

**2. 메모리 사용**:
- Closure 내부에서 매번 전체 데이터 forward → GPU 메모리 주의

**3. Deterministic 환경**:
- Dropout 등 stochastic layer는 evaluation mode로
- 또는 seed 고정

---

## 8. 실습 과제

1. **Newton vs GD 수렴 속도**
   - Quadratic $f(x) = \frac{1}{2} x^\top A x$ ($A$ symmetric PD)
   - Condition number $\kappa = 10, 100, 1000$으로 변화
   - GD와 Newton의 수렴 iteration 비교

2. **L-BFGS on Rosenbrock**
   - 다양한 초기값에서 L-BFGS 수렴 경로 시각화
   - Adam과 수렴 속도 비교 (iteration 수, wall-clock time)

3. **Hessian Eigenvalue 분석**
   - 학습된 CNN의 loss landscape에서 Hessian 추정
   - Top/bottom eigenvalue, trace 계산
   - Sharpness와 test accuracy 상관관계

4. **Mini-batch L-BFGS**
   - MNIST에서 L-BFGS를 mini-batch로 시도
   - Full-batch와 성능/안정성 비교
   - Noise의 영향 관찰

5. **Curvature Visualization**
   - 2D 파라미터 slice를 잡고 loss surface 그리기
   - GD, Momentum, Newton 경로를 같은 plot에 표시
   - 각 방법이 곡률을 어떻게 활용하는지 분석