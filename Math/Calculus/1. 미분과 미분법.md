## TL;DR
- ë¯¸ë¶„ì€ "ì…ë ¥ê°’ì„ ì•„ì£¼ ì¡°ê¸ˆ ë°”ê¿¨ì„ ë•Œ í•¨ìˆ˜ ê°’ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ê°€"ë¥¼ ì •ëŸ‰í™”í•˜ëŠ” ë„êµ¬ë¡œ, ë”¥ëŸ¬ë‹ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•œ **gradient descent**ì˜ í•µì‹¬ ì¬ë£Œë‹¤.
- ì´ ë…¸íŠ¸ëŠ” ë¯¸ì ë¶„ ì „ì²´ ë‚´ìš©ì˜ í—ˆë¸Œë¡œ, ë‹¨ë³€ìˆ˜ â†’ ë‹¤ë³€ìˆ˜ â†’ ìë™ ë¯¸ë¶„ â†’ ìµœì í™” ìˆœì„œë¡œ í•™ìŠµí•˜ëŠ” ë¡œë“œë§µì„ ì œê³µí•œë‹¤.
- ê° ì£¼ì œë³„ ìƒì„¸ ë…¸íŠ¸ë¡œ ì—°ê²°í•˜ì—¬ ì²´ê³„ì ìœ¼ë¡œ backpropagationê³¼ ìµœì í™” ì´ë¡ ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤.

---

## í•™ìŠµ ë¡œë“œë§µ

| ìˆœì„œ  | ì£¼ì œ       | í•µì‹¬ ì§ˆë¬¸                                     | ë°”ë¡œ ê°€ê¸°              |
| --- | -------- | ----------------------------------------- | ------------------ |
| 1   | ë‹¨ë³€ìˆ˜ ë¯¸ë¶„   | ê·¹í•œ, ë„í•¨ìˆ˜, ê·¹ê°’ì€ ì–´ë–»ê²Œ ì •ì˜í•˜ëŠ”ê°€?                   | [[2. ë‹¨ë³€ìˆ˜ ë¯¸ì ë¶„]]     |
| 2   | ë‹¤ë³€ìˆ˜ ë¯¸ë¶„   | í¸ë¯¸ë¶„, gradient, Jacobian, Hessianì´ë€?       | [[3. ë‹¤ë³€ìˆ˜ ë¯¸ì ë¶„]]     |
| 3   | ìë™ ë¯¸ë¶„    | Chain ruleì„ ì–´ë–»ê²Œ ìë™í™”í•˜ëŠ”ê°€? Backpropagationì€? | [[5. ìë™ ë¯¸ë¶„]]       |
| 4   | í…Œì¼ëŸ¬ì™€ ìµœì í™” | í•¨ìˆ˜ ê·¼ì‚¬ì™€ ìµœì ì„± ì¡°ê±´ì€? Flat/Sharp minimaë€?       | [[4. í…Œì¼ëŸ¬ ê¸‰ìˆ˜ì™€ ìµœì í™”]] |

---

## 1. ë‹¨ë³€ìˆ˜ ë¯¸ë¶„

### í•µì‹¬ ê°œë…
- **ê·¹í•œ**: $\lim_{x \to a} f(x) = L$ (Îµ-Î´ ì •ì˜)
- **ë„í•¨ìˆ˜**: $f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$
- **ë¯¸ë¶„ ê·œì¹™**: ê³±ì…ˆ, ëª«, chain rule
- **ê·¹ê°’**: 1ì°¨/2ì°¨ ë„í•¨ìˆ˜ íŒì •ë²•
- **í‰ê· ê°’ ì •ë¦¬**: í‰ê·  ë³€í™”ìœ¨ = ì–´ë”˜ê°€ì˜ ìˆœê°„ ë³€í™”ìœ¨

### ì™œ ì¤‘ìš”í•œê°€?
- 1ì°¨ì› ì§ê´€ì´ ë‹¤ë³€ìˆ˜ ì´í•´ì˜ ê¸°ì´ˆ
- Chain ruleì´ backpropagationì˜ ì¶œë°œì 
- ê·¹ê°’ ì¡°ê±´ì´ ìµœì í™” ì´ë¡ ì˜ í† ëŒ€

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[2. ë‹¨ë³€ìˆ˜ ë¯¸ì ë¶„|ë‹¨ë³€ìˆ˜ ë¯¸ì ë¶„ ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- Chain rule: $(f \circ g)' = f'(g(x)) \cdot g'(x)$
- í‰ê· ê°’ ì •ë¦¬: $f'(c) = \frac{f(b) - f(a)}{b - a}$ for some $c \in (a, b)$

---

## 2. ë‹¤ë³€ìˆ˜ ë¯¸ë¶„

### í•µì‹¬ ê°œë…
- **í¸ë¯¸ë¶„**: $\frac{\partial f}{\partial x_i}$ (ë‹¤ë¥¸ ë³€ìˆ˜ ê³ ì •)
- **Gradient**: $\nabla f = [\frac{\partial f}{\partial x_1}, \ldots, \frac{\partial f}{\partial x_n}]^\top$ (ê°€ì¥ ë¹ ë¥¸ ì¦ê°€ ë°©í–¥)
- **Jacobian**: ë²¡í„° í•¨ìˆ˜ì˜ ë¯¸ë¶„ í–‰ë ¬ ($m \times n$)
- **Hessian**: 2ì°¨ í¸ë¯¸ë¶„ í–‰ë ¬ (ê³¡ë¥  ì •ë³´)
- **Chain rule**: $\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}$

### ì™œ ì¤‘ìš”í•œê°€?
- Gradient descent: $\theta_{t+1} = \theta_t - \eta \nabla L$
- Hessianìœ¼ë¡œ ê³¡ë¥  ë¶„ì„ (Newton ë°©ë²•, ì¡°ê±´ìˆ˜)
- Jacobianì´ backpropagationì˜ ìˆ˜í•™ì  í‘œí˜„

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[3. ë‹¤ë³€ìˆ˜ ë¯¸ì ë¶„|ë‹¤ë³€ìˆ˜ ë¯¸ì ë¶„ ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- ë°©í–¥ ë¯¸ë¶„: $D_{\mathbf{u}} f = \nabla f \cdot \mathbf{u}$
- Schwarz: $\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}$ (Hessian ëŒ€ì¹­)

---

## 3. ìë™ ë¯¸ë¶„ê³¼ Backpropagation

### í•µì‹¬ ê°œë…
- **ê³„ì‚° ê·¸ë˜í”„**: ì—°ì‚°ì„ ë…¸ë“œ, ë°ì´í„°ë¥¼ ì—£ì§€ë¡œ í‘œí˜„
- **Forward mode**: Jacobian-vector product (ì…ë ¥ â†’ ì¶œë ¥)
- **Reverse mode**: Vector-Jacobian product (ì¶œë ¥ â†’ ì…ë ¥)
- **Backpropagation**: Reverse modeì˜ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ (ìŠ¤ì¹¼ë¼ ì¶œë ¥)
- **PyTorch autograd**: `requires_grad`, `backward()`

### ì™œ ì¤‘ìš”í•œê°€?
- ë”¥ëŸ¬ë‹ í•™ìŠµì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜
- ìˆ˜ë°±ë§Œ íŒŒë¼ë¯¸í„°ì˜ gradientë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°
- Forward: $O(n)$, Backward: $O(n)$ (í•œ ë²ˆì— ëª¨ë“  gradient!)

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[5. ìë™ ë¯¸ë¶„|ìë™ ë¯¸ë¶„ ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì•Œê³ ë¦¬ì¦˜**:
```
Forward: x â†’ vâ‚ â†’ vâ‚‚ â†’ ... â†’ y
Backward: È³=1 â†’ vÌ„â‚™ â†’ vÌ„â‚™â‚‹â‚ â†’ ... â†’ xÌ„
```

---

## 4. í…Œì¼ëŸ¬ ì „ê°œì™€ ìµœì í™”

### í•µì‹¬ ê°œë…
- **1ì°¨ í…Œì¼ëŸ¬**: $f(\mathbf{x} + \Delta\mathbf{x}) \approx f(\mathbf{x}) + \nabla f^\top \Delta\mathbf{x}$
- **2ì°¨ í…Œì¼ëŸ¬**: $+ \frac{1}{2}\Delta\mathbf{x}^\top H \Delta\mathbf{x}$
- **ìµœì ì„± ì¡°ê±´**: 1ì°¨ í•„ìš” ($\nabla f = 0$), 2ì°¨ ì¶©ë¶„ ($H \succ 0$)
- **Newton ë°©ë²•**: $\mathbf{x}_{k+1} = \mathbf{x}_k - H^{-1} \nabla f$
- **Flat/Sharp minima**: Hessian ê³ ìœ ê°’ì˜ í¬ê¸°

### ì™œ ì¤‘ìš”í•œê°€?
- Gradient descentì˜ ìˆ˜ë ´ì„± ì´ë¡ ì  ê·¼ê±°
- Newton, Quasi-Newton ìµœì í™” ì•Œê³ ë¦¬ì¦˜
- Flat minimaê°€ ì¼ë°˜í™”ì— ìœ ë¦¬ (SAM ë“±)

### ìƒì„¸ í•™ìŠµ
ğŸ‘‰ [[4. í…Œì¼ëŸ¬ ê¸‰ìˆ˜ì™€ ìµœì í™”|í…Œì¼ëŸ¬ì™€ ìµœì í™” ìƒì„¸ ë…¸íŠ¸]]

**ì£¼ìš” ì •ë¦¬**:
- 2ì°¨ ì¶©ë¶„ì¡°ê±´: $\nabla f = 0$, $H \succ 0$ â†’ strict local min
- Sharpness: $\max_{\|\epsilon\| \leq \rho} f(\mathbf{x}^* + \epsilon) - f(\mathbf{x}^*)$

---

## ì£¼ì œë³„ ì—°ê²°

### ê°œë… ê°„ ê´€ê³„ë„

```
ë‹¨ë³€ìˆ˜ ë¯¸ë¶„ â”€â”€â”€â”€â”€â”€> ê·¹í•œ, ë„í•¨ìˆ˜ ì •ì˜
    â”‚                     â”‚
    â–¼                     â–¼
Chain Rule â”€â”€â”€â”€> ë‹¤ë³€ìˆ˜ í™•ì¥ â”€â”€â”€â”€> Gradient, Jacobian
    â”‚                                    â”‚
    â–¼                                    â–¼
ê³„ì‚° ê·¸ë˜í”„ â”€â”€â”€â”€> Backpropagation â—„â”€â”€â”€â”€ Reverse Mode
    â”‚                     â”‚
    â–¼                     â–¼
ìë™ ë¯¸ë¶„ â”€â”€â”€â”€â”€â”€â”€â”€> PyTorch Autograd
                          â”‚
                          â–¼
            í…Œì¼ëŸ¬ ì „ê°œ â”€â”€â”€â”€> 1ì°¨ ê·¼ì‚¬ (GD)
                          â”‚
                          â–¼
                    2ì°¨ ê·¼ì‚¬ (Newton)
                          â”‚
                          â–¼
                  ìµœì ì„± ì¡°ê±´, Hessian
```

### ë¯¸ë¶„ ë„êµ¬ ë¹„êµ

| ë°©ë²• | ì •í™•ë„ | ì†ë„ | ë©”ëª¨ë¦¬ | ìš©ë„ |
|------|--------|------|--------|------|
| **ìˆ˜ì¹˜ ë¯¸ë¶„** | ê·¼ì‚¬ | ëŠë¦¼ $O(n)$ | ì ìŒ | Gradient checking |
| **ê¸°í˜¸ ë¯¸ë¶„** | ì •í™• | í‘œí˜„ì‹ í­ë°œ | ë§ìŒ | ìˆ˜ì‹ ìœ ë„ |
| **ìë™ ë¯¸ë¶„ (Forward)** | ì •í™• | $O(n)$ per input | ì ìŒ | $n \ll m$ |
| **ìë™ ë¯¸ë¶„ (Reverse)** | ì •í™• | $O(1)$ for all | ì¤‘ê°„ | ë”¥ëŸ¬ë‹ ($m=1$) |

---

## ë¹ ë¥¸ ì°¸ì¡°

### ìì£¼ ì“°ëŠ” ë¯¸ë¶„ ê³µì‹

**ë‹¨ë³€ìˆ˜**:
- $(x^n)' = nx^{n-1}$
- $(e^x)' = e^x$
- $(\ln x)' = 1/x$
- $(\sin x)' = \cos x$
- $(f \cdot g)' = f'g + fg'$
- $(f/g)' = (f'g - fg')/g^2$
- $(f \circ g)' = f'(g) \cdot g'$

**ë‹¤ë³€ìˆ˜**:
- Gradient: $\nabla f = [\frac{\partial f}{\partial x_1}, \ldots, \frac{\partial f}{\partial x_n}]^\top$
- ë°©í–¥ ë¯¸ë¶„: $D_{\mathbf{u}} f = \nabla f \cdot \mathbf{u}$
- Chain: $\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$

**í–‰ë ¬ ë¯¸ë¶„** (ìì£¼ ì“°ëŠ” ê²ƒë§Œ):
- $\frac{\partial}{\partial \mathbf{x}}(\mathbf{a}^\top \mathbf{x}) = \mathbf{a}$
- $\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^\top A \mathbf{x}) = (A + A^\top)\mathbf{x}$
- $\frac{\partial}{\partial X}(\text{tr}(AX)) = A^\top$

---

## ì‹¤ìŠµ ê°€ì´ë“œ

### PyTorch ì£¼ìš” í•¨ìˆ˜

```python
import torch

# ê¸°ë³¸ ì„¤ì •
x = torch.tensor([1.0, 2.0], requires_grad=True)
y = (x ** 2).sum()

# Gradient ê³„ì‚°
y.backward()
print(x.grad)  # dy/dx

# Jacobian
from torch.autograd.functional import jacobian
J = jacobian(lambda x: x**2, x)

# Hessian
from torch.autograd.functional import hessian
H = hessian(lambda x: (x**2).sum(), x)

# ê³ ì°¨ ë„í•¨ìˆ˜
x = torch.tensor(2.0, requires_grad=True)
y = x ** 3
dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]
d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]
```

### í•™ìŠµ ì „ëµ

1. **ë‹¨ë³€ìˆ˜ë¶€í„° ì‹œì‘**
   - ê·¹í•œ, ë„í•¨ìˆ˜ ì •ì˜ í™•ì‹¤íˆ
   - Chain rule ì™„ë²½íˆ ì´í•´
   - ê·¹ê°’ íŒë³„ ì—°ìŠµ

2. **ë‹¤ë³€ìˆ˜ë¡œ í™•ì¥**
   - í¸ë¯¸ë¶„ê³¼ gradient êµ¬ë³„
   - Jacobian/Hessian ê³„ì‚° ì—°ìŠµ
   - 2x2, 3x3ë¶€í„° ì†ìœ¼ë¡œ ê³„ì‚°

3. **ìë™ ë¯¸ë¶„ ì‹¤ìŠµ**
   - ì‘ì€ ê³„ì‚° ê·¸ë˜í”„ ì†ìœ¼ë¡œ ê·¸ë¦¬ê¸°
   - Forward/backward pass ì§ì ‘ ê³„ì‚°
   - PyTorchë¡œ ê²€ì¦

4. **ìµœì í™” ì—°ê²°**
   - Gradient descent ì§ì ‘ êµ¬í˜„
   - Learning rateì™€ ìˆ˜ë ´ ê´€ê³„ ê´€ì°°
   - Newton vs GD ë¹„êµ ì‹¤í—˜

---

## ë”¥ëŸ¬ë‹ ì—°ê²° í¬ì¸íŠ¸

### ì–´ë””ì— ì“°ì´ë‚˜?

| ë”¥ëŸ¬ë‹ ê°œë… | ë¯¸ì ë¶„ ë„êµ¬ | ê´€ë ¨ ë…¸íŠ¸ |
|-----------|-----------|----------|
| Loss function | ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ | [[3. ë‹¤ë³€ìˆ˜ ë¯¸ì ë¶„]] |
| Gradient descent | Gradient, 1ì°¨ í…Œì¼ëŸ¬ | [[4. í…Œì¼ëŸ¬ ê¸‰ìˆ˜ì™€ ìµœì í™”]] |
| Backpropagation | Chain rule, Reverse mode | [[5. ìë™ ë¯¸ë¶„]] |
| Learning rate | í…Œì¼ëŸ¬ ê·¼ì‚¬ ë²”ìœ„ | [[4. í…Œì¼ëŸ¬ ê¸‰ìˆ˜ì™€ ìµœì í™”]] |
| Newton, L-BFGS | Hessian, 2ì°¨ í…Œì¼ëŸ¬ | [[4. í…Œì¼ëŸ¬ ê¸‰ìˆ˜ì™€ ìµœì í™”]] |
| Weight decay | Gradient ìˆ˜ì • | [[3. ë‹¤ë³€ìˆ˜ ë¯¸ì ë¶„]] |
| Batch size | Gradient ë¶„ì‚° | [[3. ë‹¤ë³€ìˆ˜ ë¯¸ì ë¶„]] |
| Activation function | Chain rule ì ìš© | [[5. ìë™ ë¯¸ë¶„]] |
| Skip connections | Gradient flow | [[5. ìë™ ë¯¸ë¶„]] |

---

## ì°¸ê³  ìë£Œ

### êµê³¼ì„œ
- James Stewart, *Calculus*, 8th ed.
- Tom M. Apostol, *Calculus*, Volume 1-2
- Michael Spivak, *Calculus*

### ì˜¨ë¼ì¸ ê°•ì˜
- MIT OCW 18.01 Single Variable Calculus
- MIT OCW 18.02 Multivariable Calculus
- 3Blue1Brown, *Essence of Calculus* (YouTube)

### ë”¥ëŸ¬ë‹ ê´€ì 
- Ian Goodfellow et al., *Deep Learning*, Chapter 4
- CS231n: "Backpropagation, Intuitions"
- Griewank & Walther, *Evaluating Derivatives*

---

## ë‹¤ìŒ í•™ìŠµ

ë¯¸ì ë¶„ ê¸°ì´ˆë¥¼ ë§ˆì³¤ë‹¤ë©´:

1. **ìµœì í™”ë¡œ ì—°ê²°**
   - [[Math/Optimization/1. ìµœì í™” ê¸°ì´ˆ|ìµœì í™” ê¸°ì´ˆ]]
   - Gradient descent, Newton, Quasi-Newton

2. **ì„ í˜•ëŒ€ìˆ˜ì™€ ìœµí•©**
   - [[Math/Linear Algebra/4. ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°|ê³ ìœ ê°’]] (Hessian ë¶„ì„)
   - [[Math/Linear Algebra/5. íŠ¹ì‡ê°’ ë¶„í•´|SVD]] (Jacobian ë¶„í•´)

3. **ì‹¤ì „ ì‘ìš©**
   - [[ML Foundations/3. ë¨¸ì‹ ëŸ¬ë‹ ìµœì í™”|MLì—ì„œì˜ ìµœì í™”]]
   - [[ML Foundations/2. ì†ì‹¤ í•¨ìˆ˜ì™€ í‰ê°€ ì§€í‘œ|ì†ì‹¤ í•¨ìˆ˜ì™€ í‰ê°€]]

4. **ìƒìœ„ ê°œë…**
   - [[AI ê¸°ì´ˆ ê°œìš”|AI ê¸°ì´ˆ ì „ì²´ ë¡œë“œë§µ]]
