## TL;DR
- 테일러 급수(Taylor Series)는 함수를 다항식으로 근사하는 도구로, 딥러닝에서 손실 함수의 local 구조를 분석하는 데 핵심적이다.
- 1차 근사(선형)는 gradient descent의 이론적 근거를, 2차 근사(이차)는 Newton 방법과 곡률 분석의 토대를 제공한다.
- 최적성 조건(1차 필요조건, 2차 충분조건)은 극값 판별과 수렴 보장에 필수적이며, Hessian의 고유값 분석으로 flat/sharp minima를 구별한다.

---

## 1. 테일러 급수

### 1.1 단변수 테일러 급수

**정의**: 함수 $f$가 $x = a$ 근처에서 $n$번 미분 가능하면:
$$
f(x) = f(a) + f'(a)(x - a) + \frac{f''(a)}{2!}(x - a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x - a)^n + R_n(x)
$$

여기서:
- 앞의 항들: $n$차 테일러 다항식 (근사값)
- $R_n(x)$: 나머지항 (오차)

**Maclaurin 급수**: $a = 0$인 특수한 경우.

#### 1.1.1 Lagrange 형태의 나머지항

**정리**: 나머지항 $R_n(x)$는 다음과 같이 표현된다:
$$
R_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}(x - a)^{n+1}
$$

여기서 $\xi$는 $a$와 $x$ 사이의 어떤 점이다 (즉, $\xi \in (a, x)$ 또는 $\xi \in (x, a)$).

**기하학적 의미**:
Lagrange 형태의 나머지항은 "테일러 다항식이 원래 함수에 얼마나 근접하냐"를 표현한다.

즉, **나머지항은 오차(error term)의 크기를 실제 도함수로 평가한 공식**이다.

**왜 중요한가**:
1. **근사 오차 추정**: $|R_n(x)|$를 통해 근사가 얼마나 정확한지 정량적으로 평가 가능
2. **수렴성 분석**: $R_n(x) \to 0$이면 테일러 급수가 $f(x)$로 수렴
3. **최적화 이론**: Gradient descent에서 스텝 크기가 너무 클 때의 오차 분석

#### 1.1.2 나머지항의 직관

$n$차 테일러 다항식은 $f$의 $n$차 미분까지만 사용한다.

하지만 실제 함수는 그 이상으로 휘어질 수 있다. 그 "추가 휘어짐"이 바로 나머지항이다.

- $n = 0$: 상수 근사 → 나머지항은 1차 도함수로 결정
- $n = 1$: 선형 근사 → 나머지항은 2차 도함수(곡률)로 결정
- $n = 2$: 이차 근사 → 나머지항은 3차 도함수로 결정

**예시**: $f(x) = e^x$를 $a = 0$에서 1차 근사

1차 테일러:
$$
e^x \approx 1 + x
$$

나머지항:
$$
R_1(x) = \frac{e^\xi}{2!}x^2 = \frac{e^\xi}{2}x^2, \quad \xi \in (0, x)
$$

$x = 0.1$일 때:
- 근사값: $1 + 0.1 = 1.1$
- 실제값: $e^{0.1} = 1.10517...$
- 오차: $|R_1(0.1)| = \frac{e^\xi}{2}(0.1)^2$

$\xi \in (0, 0.1)$이므로 $1 < e^\xi < e^{0.1} \approx 1.105$:
$$
|R_1(0.1)| < \frac{1.105}{2}(0.01) \approx 0.0055
$$

실제 오차 $0.00517$는 이 범위 안에 있다.

#### 1.1.3 나머지항의 활용

**활용 1: 근사 정밀도 결정**

$n$을 얼마나 크게 해야 원하는 정밀도를 얻을 수 있는가?

$$
|R_n(x)| \le \frac{M}{(n+1)!}|x - a|^{n+1}
$$

여기서 $M = \max_{\xi \in [a,x]} |f^{(n+1)}(\xi)|$.

**활용 2: Gradient Descent 스텝 크기 분석**

1차 테일러:
$$
f(\mathbf{x} - \eta \nabla f) \approx f(\mathbf{x}) - \eta \|\nabla f\|^2
$$

나머지항:
$$
R_1 \approx \frac{\eta^2}{2} \nabla f^\top H \nabla f
$$

$\eta$가 너무 크면 나머지항이 커져서 손실이 오히려 증가할 수 있다!

### 1.2 Maclaurin 급수

#### 1.2.1 정의와 의미

**정의**: Maclaurin 급수는 $a = 0$을 중심으로 한 테일러 급수의 특수한 경우다.

$$
f(x) = f(0) + f'(0)x + \frac{f''(0)}{2!}x^2 + \frac{f'''(0)}{3!}x^3 + \cdots = \sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!}x^n
$$

**왜 $a = 0$을 많이 쓰나?**
1. **계산 간편**: 0에서의 도함수는 보통 계산이 쉽다
2. **표준화**: 다양한 함수를 원점 기준으로 표준화할 수 있다
3. **대칭성**: 짝함수/홀함수 같은 대칭성이 드러난다
4. **딥러닝 활성화 함수**: 많은 활성화 함수가 원점 근처에서 분석된다

**직관**: "함수를 원점에서 관찰했을 때, 다항식으로 어떻게 보이는가?"

#### 1.2.2 주요 함수의 Maclaurin 급수

**1) 지수 함수**
$$
e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots = \sum_{n=0}^{\infty} \frac{x^n}{n!}
$$

특징:
- 모든 계수가 양수
- 모든 $x$에서 수렴 (수렴 반경 $R = \infty$)
- 딥러닝 활용: softmax, 손실 함수 분석

**2) 삼각 함수**
$$
\sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!}
$$
$$
\cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{(2n)!}
$$

특징:
- $\sin x$: 홀함수 (홀수 차수 항만)
- $\cos x$: 짝함수 (짝수 차수 항만)
- 교대 급수 (부호가 번갈아 바뀜)
- 모든 $x$에서 수렴
- 딥러닝 활용: 위치 인코딩 (Transformer)

**3) 로그 함수**
$$
\ln(1 + x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \cdots = \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}x^n, \quad |x| < 1
$$

특징:
- 수렴 반경 $R = 1$ (제한적)
- $x = 1$에서도 수렴 ($\ln 2 = 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \cdots$)
- 딥러닝 활용: 로그 손실, 엔트로피

**4) 일반 이항 급수**
$$
(1 + x)^{\alpha} = 1 + \alpha x + \frac{\alpha(\alpha - 1)}{2!}x^2 + \frac{\alpha(\alpha - 1)(\alpha - 2)}{3!}x^3 + \cdots, \quad |x| < 1
$$

특수 경우:
- $\alpha = 1$: $(1+x)^1 = 1 + x$
- $\alpha = -1$: $\frac{1}{1+x} = 1 - x + x^2 - x^3 + \cdots$ (기하급수)
- $\alpha = 1/2$: $\sqrt{1+x} = 1 + \frac{x}{2} - \frac{x^2}{8} + \cdots$

**5) 쌍곡 함수**
$$
\sinh x = x + \frac{x^3}{3!} + \frac{x^5}{5!} + \cdots = \sum_{n=0}^{\infty} \frac{x^{2n+1}}{(2n+1)!}
$$
$$
\cosh x = 1 + \frac{x^2}{2!} + \frac{x^4}{4!} + \cdots = \sum_{n=0}^{\infty} \frac{x^{2n}}{(2n)!}
$$

관계식: $e^x = \cosh x + \sinh x$

#### 1.2.3 Maclaurin 급수의 활용

**활용 1: 복잡한 함수의 근사**

$f(x) = \frac{e^x - e^{-x}}{2} = \sinh x$를 계산하고 싶을 때:
$$
\sinh x \approx x + \frac{x^3}{6} \quad (x \text{가 작을 때})
$$

**활용 2: 극한 계산**

$$
\lim_{x \to 0} \frac{\sin x - x}{x^3} = \lim_{x \to 0} \frac{(x - \frac{x^3}{6} + \cdots) - x}{x^3} = -\frac{1}{6}
$$

**활용 3: 활성화 함수 분석**

Sigmoid: $\sigma(x) = \frac{1}{1 + e^{-x}}$

$x \approx 0$ 근처:
$$
\sigma(x) \approx \frac{1}{2} + \frac{x}{4} - \frac{x^3}{48} + \cdots
$$

미분: $\sigma'(0) = 1/4$ (gradient 크기 분석)

**활용 4: 수치 안정성**

$\log(1 + x)$를 $x$가 작을 때 직접 계산하면 부동소수점 오차가 크다. 대신:
$$
\log(1 + x) \approx x - \frac{x^2}{2} \quad (x \ll 1)
$$

#### 1.2.4 Maclaurin vs Taylor

| 구분 | Maclaurin 급수 | 일반 Taylor 급수 |
|------|---------------|-----------------|
| 중심점 | $a = 0$ (원점) | 임의의 점 $a$ |
| 형태 | $\sum \frac{f^{(n)}(0)}{n!}x^n$ | $\sum \frac{f^{(n)}(a)}{n!}(x-a)^n$ |
| 장점 | 계산 간단, 표준화 | 국소 근사 정확 |
| 활용 | 전역 함수 표현 | 특정 영역 근사 |

### 1.3 예제

**예제**: $f(x) = e^x$를 $x = 0$에서 2차까지 근사

$$
f(x) \approx 1 + x + \frac{x^2}{2}
$$

$x = 0.1$에서:
- 근사값: $1 + 0.1 + 0.005 = 1.105$
- 실제값: $e^{0.1} = 1.10517...$
- 오차: $0.00017$

---

## 2. 다변수 테일러 전개

### 2.1 1차 테일러 (선형 근사)

$$
f(\mathbf{x} + \Delta\mathbf{x}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\top \Delta\mathbf{x}
$$

**의미**: 현재 점에서 gradient 방향으로 선형 외삽.

**Gradient Descent 연결**:
$$
f(\mathbf{x} - \eta \nabla f) \approx f(\mathbf{x}) - \eta \|\nabla f\|^2
$$

$\eta$가 작으면 손실 감소 보장!

### 2.2 2차 테일러 (이차 근사)

$$
f(\mathbf{x} + \Delta\mathbf{x}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\top \Delta\mathbf{x} + \frac{1}{2} \Delta\mathbf{x}^\top H_f(\mathbf{x}) \Delta\mathbf{x}
$$

**의미**: 곡률(Hessian)까지 고려한 근사.

### 2.3 예제

**예제**: $f(x, y) = x^2 + xy + y^2$를 $(1, 1)$에서 근사

1. $f(1, 1) = 3$
2. $\nabla f = (2x + y, x + 2y)$, $\nabla f(1, 1) = (3, 3)$
3. $H = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$

점 $(1.1, 1.2)$에서:
$$
\Delta\mathbf{x} = (0.1, 0.2)
$$

**1차 근사**:
$$
f \approx 3 + (3, 3) \cdot (0.1, 0.2) = 3 + 0.9 = 3.9
$$

**2차 근사**:
$$
f \approx 3 + 0.9 + \frac{1}{2}(0.1, 0.2) \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix}
$$
$$
= 3.9 + \frac{1}{2}(0.1, 0.2) \begin{bmatrix} 0.4 \\ 0.5 \end{bmatrix} = 3.9 + \frac{1}{2}(0.14) = 3.97
$$

**실제값**: $f(1.1, 1.2) = 1.21 + 1.32 + 1.44 = 3.97$ ✓

---

## 3. 최적성 조건

### 3.1 1차 필요조건 (First-Order Necessary Condition)

**정리**: $\mathbf{x}^*$가 미분 가능한 $f$의 local minimum이면:
$$
\nabla f(\mathbf{x}^*) = \mathbf{0}
$$

**역은 성립 안 함**: $\nabla f = \mathbf{0}$이어도 안장점일 수 있음.

**증명 스케치**:
- $\nabla f(\mathbf{x}^*) \neq \mathbf{0}$이면 $-\nabla f$ 방향으로 이동하면 감소
- 모순! 

### 3.2 2차 필요조건

**정리**: $\mathbf{x}^*$가 local minimum이고 $f$가 $C^2$이면:
$$
\nabla f(\mathbf{x}^*) = \mathbf{0} \quad \text{and} \quad H_f(\mathbf{x}^*) \succeq 0 \text{ (positive semidefinite)}
$$

### 3.3 2차 충분조건

**정리**:
$$
\nabla f(\mathbf{x}^*) = \mathbf{0} \quad \text{and} \quad H_f(\mathbf{x}^*) \succ 0 \text{ (positive definite)}
$$
이면 $\mathbf{x}^*$는 **strict local minimum**.

**증명**: 2차 테일러 전개
$$
f(\mathbf{x}^* + \Delta\mathbf{x}) \approx f(\mathbf{x}^*) + \frac{1}{2}\Delta\mathbf{x}^\top H \Delta\mathbf{x}
$$
$H \succ 0$이면 우변 > 0 for $\Delta\mathbf{x} \neq \mathbf{0}$.

### 3.4 정리 표

| 조건 | $\nabla f$ | $H_f$ 고유값 | 결론 |
|------|-----------|-------------|------|
| 1차 필요 | $= \mathbf{0}$ | - | 임계점 |
| 2차 충분 (최소) | $= \mathbf{0}$ | 모두 > 0 | Strict local min |
| 2차 충분 (최대) | $= \mathbf{0}$ | 모두 < 0 | Strict local max |
| 안장점 | $= \mathbf{0}$ | 양/음 혼재 | Saddle point |

---

## 4. Newton 방법

### 4.1 아이디어

2차 근사를 최소화:
$$
f(\mathbf{x} + \Delta\mathbf{x}) \approx f(\mathbf{x}) + \nabla f^\top \Delta\mathbf{x} + \frac{1}{2}\Delta\mathbf{x}^\top H \Delta\mathbf{x}
$$

미분하여 0:
$$
\nabla f + H \Delta\mathbf{x} = \mathbf{0}
$$
$$
\Delta\mathbf{x} = -H^{-1} \nabla f
$$

### 4.2 Newton 업데이트

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - H_f(\mathbf{x}_k)^{-1} \nabla f(\mathbf{x}_k)
$$

**장점**: 2차 수렴 (매우 빠름)
**단점**:
- Hessian 역행렬 계산 $O(n^3)$
- Hessian이 양정부가 아니면 발산 가능

### 4.3 Quasi-Newton

Hessian 근사 $B_k$를 반복적으로 업데이트 (BFGS, L-BFGS):
$$
\mathbf{x}_{k+1} = \mathbf{x}_k - B_k^{-1} \nabla f(\mathbf{x}_k)
$$

---

## 5. 곡률과 일반화

### 5.1 Flat vs Sharp Minima

**Flat minimum**: Hessian 고유값이 작음 → 넓은 골짜기
**Sharp minimum**: Hessian 고유값이 큼 → 뾰족한 골짜기

**일반화 연결**:
- Flat minima는 테스트 데이터에 robust
- Sharp minima는 overfitting 경향

### 5.2 Sharpness 측정

$$
\text{Sharpness} = \max_{\|\epsilon\| \leq \rho} f(\mathbf{x}^* + \epsilon) - f(\mathbf{x}^*)
$$

또는 Hessian 최대 고유값 $\lambda_{\max}(H)$

### 5.3 Sharpness-Aware Minimization (SAM)

**아이디어**: Flat region을 찾도록 학습:
$$
\min_{\mathbf{w}} \max_{\|\epsilon\| \leq \rho} L(\mathbf{w} + \epsilon)
$$

1차 근사:
$$
\epsilon^* \approx \rho \frac{\nabla L}{\|\nabla L\|}
$$

업데이트:
$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla L(\mathbf{w} + \epsilon^*)
$$

---

## 6. PyTorch 실습

```python
import torch

# 1. 테일러 근사 시각화
def f(x):
    return torch.exp(x)

x0 = torch.tensor(0.0)
x = torch.linspace(-0.5, 0.5, 100)

# 0차 근사
approx_0 = f(x0) * torch.ones_like(x)

# 1차 근사
approx_1 = f(x0) + f(x0) * (x - x0)

# 2차 근사
approx_2 = f(x0) + f(x0) * (x - x0) + 0.5 * f(x0) * (x - x0)**2

print(f"x=0.1에서:")
print(f"실제: {f(torch.tensor(0.1)).item():.6f}")
print(f"1차 근사: {(1 + 0.1).item():.6f}")
print(f"2차 근사: {(1 + 0.1 + 0.005).item():.6f}")

# 2. 최적성 조건 확인
x = torch.tensor([1.0, 1.0], requires_grad=True)

def g(x):
    return x[0]**2 + x[1]**2 - 2*x[0] - 4*x[1] + 5

# Gradient
loss = g(x)
loss.backward()
print(f"∇g(1,1) = {x.grad}")  # [0, 0]이어야 최소

# Hessian
x_opt = torch.tensor([1.0, 2.0], requires_grad=True)
H = torch.autograd.functional.hessian(g, x_opt)
print(f"Hessian:\n{H}")
eigenvalues = torch.linalg.eigvalsh(H)
print(f"고유값: {eigenvalues}")  # 모두 양수 → 최소

# 3. Newton 방법
def newton_step(f, x):
    """한 번의 Newton step"""
    grad = torch.autograd.functional.jacobian(f, x)
    hess = torch.autograd.functional.hessian(f, x)
    return x - torch.linalg.solve(hess, grad)

def quadratic(x):
    return x[0]**2 + 4*x[1]**2 - 2*x[0] - 8*x[1]

x = torch.tensor([0.0, 0.0], dtype=torch.float64)
for i in range(5):
    x = newton_step(quadratic, x)
    print(f"Step {i+1}: x = {x}, f = {quadratic(x).item():.6f}")

print(f"최적해: [1, 1] (이론값)")

# 4. Sharpness 측정
def measure_sharpness(f, x_star, rho=0.1, n_samples=100):
    """주변 영역에서 최대 증가량 측정"""
    f_star = f(x_star).item()
    max_increase = 0.0

    for _ in range(n_samples):
        epsilon = torch.randn_like(x_star)
        epsilon = rho * epsilon / epsilon.norm()
        f_perturbed = f(x_star + epsilon).item()
        max_increase = max(max_increase, f_perturbed - f_star)

    return max_increase

# Flat minimum
def flat_fn(x):
    return 0.1 * x[0]**2 + 0.1 * x[1]**2

# Sharp minimum
def sharp_fn(x):
    return 10 * x[0]**2 + 10 * x[1]**2

x_opt = torch.tensor([0.0, 0.0])
print(f"Flat sharpness: {measure_sharpness(flat_fn, x_opt):.6f}")
print(f"Sharp sharpness: {measure_sharpness(sharp_fn, x_opt):.6f}")
```

---

## 7. 연습 문제

1. **테일러 급수**
   $f(x) = \ln(1 + x)$를 $x = 0$에서 3차까지 전개하고, $\ln(1.1)$의 근사값을 구하라.

2. **다변수 근사**
   $f(x, y) = e^{x + y}$를 $(0, 0)$에서 2차까지 전개하고, $f(0.1, 0.2)$를 근사하라.

3. **임계점 분류**
   $f(x, y) = x^3 - 3xy + y^3$의 모든 임계점을 찾고, Hessian으로 분류하라.

4. **Newton vs Gradient Descent**
   $f(x, y) = 10x^2 + y^2$를 $(5, 5)$에서 시작하여 Newton과 GD(lr=0.01)로 최적화하고 수렴 속도를 비교하라.

5. **Sharpness 비교**
   동일한 최솟값을 갖지만 곡률이 다른 두 함수를 만들고, Hessian 고유값과 sharpness를 비교하라.
