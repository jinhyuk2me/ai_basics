## TL;DR
- 테일러 급수(Taylor Series)는 함수를 다항식으로 근사하는 도구로, 딥러닝에서 손실 함수의 local 구조를 분석하는 데 핵심적이다.
- 1차 근사(선형)는 gradient descent의 이론적 근거를, 2차 근사(이차)는 Newton 방법과 곡률 분석의 토대를 제공한다.
- 최적성 조건(1차 필요조건, 2차 충분조건)은 극값 판별과 수렴 보장에 필수적이며, Hessian의 고유값 분석으로 flat/sharp minima를 구별한다.

---

## 1. 테일러 급수

### 1.1 단변수 테일러 급수

**정의**: 함수 $f$가 $x = a$ 근처에서 $n$번 미분 가능하면:
$$
f(x) = f(a) + f'(a)(x - a) + \frac{f''(a)}{2!}(x - a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x - a)^n + R_n(x)
$$

**나머지 항 (Lagrange form)**:
$$
R_n(x) = \frac{f^{(n+1)}(c)}{(n+1)!}(x - a)^{n+1}
$$
for some $c$ between $a$ and $x$.

**Maclaurin 급수**: $a = 0$인 특수한 경우.

### 1.2 주요 함수의 Maclaurin 급수

$$
e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots = \sum_{n=0}^{\infty} \frac{x^n}{n!}
$$

$$
\sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!}
$$

$$
\cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{(2n)!}
$$

$$
\ln(1 + x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \cdots, \quad |x| < 1
$$

$$
(1 + x)^{\alpha} = 1 + \alpha x + \frac{\alpha(\alpha - 1)}{2!}x^2 + \cdots
$$

### 1.3 예제

**예제**: $f(x) = e^x$를 $x = 0$에서 2차까지 근사

$$
f(x) \approx 1 + x + \frac{x^2}{2}
$$

$x = 0.1$에서:
- 근사값: $1 + 0.1 + 0.005 = 1.105$
- 실제값: $e^{0.1} = 1.10517...$
- 오차: $0.00017$

---

## 2. 다변수 테일러 전개

### 2.1 1차 테일러 (선형 근사)

$$
f(\mathbf{x} + \Delta\mathbf{x}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\top \Delta\mathbf{x}
$$

**의미**: 현재 점에서 gradient 방향으로 선형 외삽.

**Gradient Descent 연결**:
$$
f(\mathbf{x} - \eta \nabla f) \approx f(\mathbf{x}) - \eta \|\nabla f\|^2
$$

$\eta$가 작으면 손실 감소 보장!

### 2.2 2차 테일러 (이차 근사)

$$
f(\mathbf{x} + \Delta\mathbf{x}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\top \Delta\mathbf{x} + \frac{1}{2} \Delta\mathbf{x}^\top H_f(\mathbf{x}) \Delta\mathbf{x}
$$

**의미**: 곡률(Hessian)까지 고려한 근사.

### 2.3 예제

**예제**: $f(x, y) = x^2 + xy + y^2$를 $(1, 1)$에서 근사

1. $f(1, 1) = 3$
2. $\nabla f = (2x + y, x + 2y)$, $\nabla f(1, 1) = (3, 3)$
3. $H = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$

점 $(1.1, 1.2)$에서:
$$
\Delta\mathbf{x} = (0.1, 0.2)
$$

**1차 근사**:
$$
f \approx 3 + (3, 3) \cdot (0.1, 0.2) = 3 + 0.9 = 3.9
$$

**2차 근사**:
$$
f \approx 3 + 0.9 + \frac{1}{2}(0.1, 0.2) \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix}
$$
$$
= 3.9 + \frac{1}{2}(0.1, 0.2) \begin{bmatrix} 0.4 \\ 0.5 \end{bmatrix} = 3.9 + \frac{1}{2}(0.14) = 3.97
$$

**실제값**: $f(1.1, 1.2) = 1.21 + 1.32 + 1.44 = 3.97$ ✓

---

## 3. 최적성 조건

### 3.1 1차 필요조건 (First-Order Necessary Condition)

**정리**: $\mathbf{x}^*$가 미분 가능한 $f$의 local minimum이면:
$$
\nabla f(\mathbf{x}^*) = \mathbf{0}
$$

**역은 성립 안 함**: $\nabla f = \mathbf{0}$이어도 안장점일 수 있음.

**증명 스케치**:
- $\nabla f(\mathbf{x}^*) \neq \mathbf{0}$이면 $-\nabla f$ 방향으로 이동하면 감소
- 모순! ∎

### 3.2 2차 필요조건

**정리**: $\mathbf{x}^*$가 local minimum이고 $f$가 $C^2$이면:
$$
\nabla f(\mathbf{x}^*) = \mathbf{0} \quad \text{and} \quad H_f(\mathbf{x}^*) \succeq 0 \text{ (positive semidefinite)}
$$

### 3.3 2차 충분조건

**정리**:
$$
\nabla f(\mathbf{x}^*) = \mathbf{0} \quad \text{and} \quad H_f(\mathbf{x}^*) \succ 0 \text{ (positive definite)}
$$
이면 $\mathbf{x}^*$는 **strict local minimum**.

**증명**: 2차 테일러 전개
$$
f(\mathbf{x}^* + \Delta\mathbf{x}) \approx f(\mathbf{x}^*) + \frac{1}{2}\Delta\mathbf{x}^\top H \Delta\mathbf{x}
$$
$H \succ 0$이면 우변 > 0 for $\Delta\mathbf{x} \neq \mathbf{0}$. ∎

### 3.4 정리 표

| 조건 | $\nabla f$ | $H_f$ 고유값 | 결론 |
|------|-----------|-------------|------|
| 1차 필요 | $= \mathbf{0}$ | - | 임계점 |
| 2차 충분 (최소) | $= \mathbf{0}$ | 모두 > 0 | Strict local min |
| 2차 충분 (최대) | $= \mathbf{0}$ | 모두 < 0 | Strict local max |
| 안장점 | $= \mathbf{0}$ | 양/음 혼재 | Saddle point |

---

## 4. Newton 방법

### 4.1 아이디어

2차 근사를 최소화:
$$
f(\mathbf{x} + \Delta\mathbf{x}) \approx f(\mathbf{x}) + \nabla f^\top \Delta\mathbf{x} + \frac{1}{2}\Delta\mathbf{x}^\top H \Delta\mathbf{x}
$$

미분하여 0:
$$
\nabla f + H \Delta\mathbf{x} = \mathbf{0}
$$
$$
\Delta\mathbf{x} = -H^{-1} \nabla f
$$

### 4.2 Newton 업데이트

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - H_f(\mathbf{x}_k)^{-1} \nabla f(\mathbf{x}_k)
$$

**장점**: 2차 수렴 (매우 빠름)
**단점**:
- Hessian 역행렬 계산 $O(n^3)$
- Hessian이 양정부가 아니면 발산 가능

### 4.3 Quasi-Newton

Hessian 근사 $B_k$를 반복적으로 업데이트 (BFGS, L-BFGS):
$$
\mathbf{x}_{k+1} = \mathbf{x}_k - B_k^{-1} \nabla f(\mathbf{x}_k)
$$

---

## 5. 곡률과 일반화

### 5.1 Flat vs Sharp Minima

**Flat minimum**: Hessian 고유값이 작음 → 넓은 골짜기
**Sharp minimum**: Hessian 고유값이 큼 → 뾰족한 골짜기

**일반화 연결**:
- Flat minima는 테스트 데이터에 robust
- Sharp minima는 overfitting 경향

### 5.2 Sharpness 측정

$$
\text{Sharpness} = \max_{\|\epsilon\| \leq \rho} f(\mathbf{x}^* + \epsilon) - f(\mathbf{x}^*)
$$

또는 Hessian 최대 고유값 $\lambda_{\max}(H)$

### 5.3 Sharpness-Aware Minimization (SAM)

**아이디어**: Flat region을 찾도록 학습:
$$
\min_{\mathbf{w}} \max_{\|\epsilon\| \leq \rho} L(\mathbf{w} + \epsilon)
$$

1차 근사:
$$
\epsilon^* \approx \rho \frac{\nabla L}{\|\nabla L\|}
$$

업데이트:
$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla L(\mathbf{w} + \epsilon^*)
$$

---

## 6. PyTorch 실습

```python
import torch

# 1. 테일러 근사 시각화
def f(x):
    return torch.exp(x)

x0 = torch.tensor(0.0)
x = torch.linspace(-0.5, 0.5, 100)

# 0차 근사
approx_0 = f(x0) * torch.ones_like(x)

# 1차 근사
approx_1 = f(x0) + f(x0) * (x - x0)

# 2차 근사
approx_2 = f(x0) + f(x0) * (x - x0) + 0.5 * f(x0) * (x - x0)**2

print(f"x=0.1에서:")
print(f"실제: {f(torch.tensor(0.1)).item():.6f}")
print(f"1차 근사: {(1 + 0.1).item():.6f}")
print(f"2차 근사: {(1 + 0.1 + 0.005).item():.6f}")

# 2. 최적성 조건 확인
x = torch.tensor([1.0, 1.0], requires_grad=True)

def g(x):
    return x[0]**2 + x[1]**2 - 2*x[0] - 4*x[1] + 5

# Gradient
loss = g(x)
loss.backward()
print(f"∇g(1,1) = {x.grad}")  # [0, 0]이어야 최소

# Hessian
x_opt = torch.tensor([1.0, 2.0], requires_grad=True)
H = torch.autograd.functional.hessian(g, x_opt)
print(f"Hessian:\n{H}")
eigenvalues = torch.linalg.eigvalsh(H)
print(f"고유값: {eigenvalues}")  # 모두 양수 → 최소

# 3. Newton 방법
def newton_step(f, x):
    """한 번의 Newton step"""
    grad = torch.autograd.functional.jacobian(f, x)
    hess = torch.autograd.functional.hessian(f, x)
    return x - torch.linalg.solve(hess, grad)

def quadratic(x):
    return x[0]**2 + 4*x[1]**2 - 2*x[0] - 8*x[1]

x = torch.tensor([0.0, 0.0], dtype=torch.float64)
for i in range(5):
    x = newton_step(quadratic, x)
    print(f"Step {i+1}: x = {x}, f = {quadratic(x).item():.6f}")

print(f"최적해: [1, 1] (이론값)")

# 4. Sharpness 측정
def measure_sharpness(f, x_star, rho=0.1, n_samples=100):
    """주변 영역에서 최대 증가량 측정"""
    f_star = f(x_star).item()
    max_increase = 0.0

    for _ in range(n_samples):
        epsilon = torch.randn_like(x_star)
        epsilon = rho * epsilon / epsilon.norm()
        f_perturbed = f(x_star + epsilon).item()
        max_increase = max(max_increase, f_perturbed - f_star)

    return max_increase

# Flat minimum
def flat_fn(x):
    return 0.1 * x[0]**2 + 0.1 * x[1]**2

# Sharp minimum
def sharp_fn(x):
    return 10 * x[0]**2 + 10 * x[1]**2

x_opt = torch.tensor([0.0, 0.0])
print(f"Flat sharpness: {measure_sharpness(flat_fn, x_opt):.6f}")
print(f"Sharp sharpness: {measure_sharpness(sharp_fn, x_opt):.6f}")
```

---

## 7. 연습 문제

1. **테일러 급수**
   $f(x) = \ln(1 + x)$를 $x = 0$에서 3차까지 전개하고, $\ln(1.1)$의 근사값을 구하라.

2. **다변수 근사**
   $f(x, y) = e^{x + y}$를 $(0, 0)$에서 2차까지 전개하고, $f(0.1, 0.2)$를 근사하라.

3. **임계점 분류**
   $f(x, y) = x^3 - 3xy + y^3$의 모든 임계점을 찾고, Hessian으로 분류하라.

4. **Newton vs Gradient Descent**
   $f(x, y) = 10x^2 + y^2$를 $(5, 5)$에서 시작하여 Newton과 GD(lr=0.01)로 최적화하고 수렴 속도를 비교하라.

5. **Sharpness 비교**
   동일한 최솟값을 갖지만 곡률이 다른 두 함수를 만들고, Hessian 고유값과 sharpness를 비교하라.

---

## 8. 참고 자료

- James Stewart, *Calculus*, Chapter 11
- Nocedal & Wright, *Numerical Optimization*, Chapter 2-4
- Keskar et al., "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima" (ICLR 2017)
- Foret et al., "Sharpness-Aware Minimization for Efficiently Improving Generalization" (ICLR 2021)

---

## 9. 다음 학습

- [[Math/Calculus/미분과 미분법|미적분 전체 개요]]
- [[Math/Optimization/최적화 기초|최적화 기초]]
- [[Math/Linear Algebra/고유값과 고유벡터|고유값과 고유벡터]] (Hessian 분석)
- [[ML Foundations/머신러닝 최적화|ML에서의 최적화]]
