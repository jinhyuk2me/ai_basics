## TL;DR
- 다변수 미분은 여러 입력에 대한 함수의 변화를 분석하는 도구로, 신경망의 gradient descent를 이해하는 핵심이다.
- 편미분, gradient, Jacobian, Hessian은 각각 "한 방향 변화율", "가장 가파른 방향", "벡터 함수 미분", "곡률 정보"를 제공한다.
- Chain rule의 다변수 버전이 backpropagation의 수학적 토대이며, 이를 확실히 이해해야 딥러닝 학습 과정을 제대로 파악할 수 있다.

---

## 1. 편미분 (Partial Derivative)

### 1.1 정의

**정의**: 다변수 함수 $f(x_1, \ldots, x_n)$의 $x_i$에 대한 편미분:
$$
\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i + h, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}
$$

**의미**: 다른 변수를 고정하고 $x_i$만 변할 때의 변화율.

**표기법**:
- $\frac{\partial f}{\partial x}$, $f_x$, $\partial_x f$, $D_x f$

### 1.2 계산 예제

**예제 1**: $f(x, y) = x^2 y + \sin(xy)$
$$
\frac{\partial f}{\partial x} = 2xy + y\cos(xy)
$$
$$
\frac{\partial f}{\partial y} = x^2 + x\cos(xy)
$$

**예제 2**: $f(x, y, z) = e^{xyz}$
$$
\frac{\partial f}{\partial x} = yze^{xyz}, \quad
\frac{\partial f}{\partial y} = xze^{xyz}, \quad
\frac{\partial f}{\partial z} = xye^{xyz}
$$

### 1.3 기하학적 해석

$z = f(x, y)$의 그래프에서:
- $\frac{\partial f}{\partial x}$: $y$를 고정하고 $x$ 방향으로 자른 곡선의 기울기
- $\frac{\partial f}{\partial y}$: $x$를 고정하고 $y$ 방향으로 자른 곡선의 기울기

---

## 2. Gradient (그래디언트)

### 2.1 정의

**정의**: 스칼라 함수 $f: \mathbb{R}^n \rightarrow \mathbb{R}$의 gradient:
$$
\nabla f = \begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix}
$$

**표기법**: $\nabla f$, $\text{grad } f$

### 2.2 기하학적 의미

**정리**: $\nabla f(\mathbf{x})$는 점 $\mathbf{x}$에서 $f$가 가장 빠르게 증가하는 방향이며, 그 크기는 최대 변화율이다.

**증명 스케치**:
방향 미분 $D_{\mathbf{u}} f = \nabla f \cdot \mathbf{u}$는 $\mathbf{u} = \frac{\nabla f}{\|\nabla f\|}$일 때 최대 ($= \|\nabla f\|$). ∎

### 2.3 예제

**예제**: $f(x, y) = x^2 + y^2$
$$
\nabla f = \begin{bmatrix} 2x \\ 2y \end{bmatrix}
$$

점 $(1, 2)$에서:
$$
\nabla f(1, 2) = \begin{bmatrix} 2 \\ 4 \end{bmatrix}
$$

이 방향으로 움직이면 $f$가 가장 빠르게 증가.

### 2.4 Gradient Descent

#### 2.4.1 개념: "내리막길 찾기"

함수 $L(\theta)$는 '손실(loss)'이다. 우리는 이 값이 가장 작아지는 $\theta^*$를 찾고 싶다. 그런데 $\theta$가 여러 개의 변수(가중치 벡터)일 때, 어디로 가야 줄어드는지 모른다.

그래서 gradient $\nabla L(\theta)$가 등장한다:
- $\nabla L(\theta)$는 "가장 빠르게 증가하는 방향"
- 따라서 감소시키려면 반대 방향으로 가야 함

즉,
$$
\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)
$$

여기서:
- $\eta$: 학습률(learning rate) — 한 번에 얼마나 움직일지
- $-\nabla L$: 손실이 줄어드는 가장 빠른 방향

#### 2.4.2 기하학적 직관

지형으로 비유하면:
- $L(\theta)$: 산의 높이
- $\theta$: 현재 위치 (x, y 좌표)
- $\nabla L$: 산이 가장 가파르게 올라가는 방향
- $-\nabla L$: 가장 가파르게 내려가는 방향

즉, gradient descent는 매번 현재 위치에서 "지형의 기울기"를 보고, 조금씩 아래쪽으로 걸어 내려가는 과정이다.

#### 2.4.3 수학적 연결

Gradient descent는 결국 다변수 테일러 전개에서 나온다.

함수 $L$을 $\theta_t$ 근처에서 1차 테일러 전개:
$$
L(\theta_t + \Delta\theta) \approx L(\theta_t) + \nabla L(\theta_t)^\top \Delta\theta
$$

손실을 줄이려면 $\Delta\theta$가 $-\nabla L(\theta_t)$ 방향으로 가야 $L$이 줄어든다.

$\Delta\theta = -\eta \nabla L(\theta_t)$를 선택하면:
$$
L(\theta_{t+1}) \approx L(\theta_t) - \eta \|\nabla L(\theta_t)\|^2 < L(\theta_t)
$$

이것이 바로 업데이트 법칙의 수학적 근거다.

#### 2.4.4 딥러닝과의 연결

딥러닝에서는:
- $\theta$: 신경망의 가중치(weight) 전체
- $L(\theta)$: 손실 (예: MSE, CrossEntropy 등)
- $\nabla L$: 각 가중치별 손실의 편미분(gradient)

이 gradient를 **역전파(backpropagation)**로 계산하고, 그 결과를 gradient descent 식에 넣어서 매 스텝마다 weight를 조정하는 것이다.

**모든 최적화 알고리즘의 기초!**

#### 2.4.5 시각적 요약

| 개념 | 수학적 표현 | 의미 |
|------|------------|------|
| 기울기(gradient) | $\nabla L(\theta)$ | 가장 빠르게 증가하는 방향 |
| 하강 방향 | $-\nabla L(\theta)$ | 손실이 줄어드는 방향 |
| 한 스텝 이동 | $\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)$ | 새로운 가중치 갱신 |
| 학습률($\eta$) | 양의 상수 | 너무 크면 overshoot, 너무 작으면 느림 |

---

## 3. 방향 미분 (Directional Derivative)

### 3.1 정의

**정의**: 단위 벡터 $\mathbf{u}$ 방향으로의 방향 미분:
$$
D_{\mathbf{u}} f(\mathbf{x}) = \lim_{h \to 0} \frac{f(\mathbf{x} + h\mathbf{u}) - f(\mathbf{x})}{h}
$$

### 3.2 Gradient와의 관계

**정리**:
$$
D_{\mathbf{u}} f = \nabla f \cdot \mathbf{u}
$$

**증명**: Chain rule 사용. $g(t) = f(\mathbf{x} + t\mathbf{u})$라 하면
$$
g'(0) = \nabla f(\mathbf{x}) \cdot \mathbf{u} \quad \text{∎}
$$

**예제**: $f(x, y) = x^2 + y^2$, 점 $(1, 2)$, 방향 $\mathbf{u} = \frac{1}{\sqrt{2}}(1, 1)$
$$
D_{\mathbf{u}} f(1, 2) = \begin{bmatrix} 2 \\ 4 \end{bmatrix} \cdot \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{6}{\sqrt{2}} = 3\sqrt{2}
$$

---

## 4. Chain Rule (연쇄 법칙)

### 4.1 단변수 → 다변수

$z = f(x, y)$이고 $x = x(t)$, $y = y(t)$이면:
$$
\frac{dz}{dt} = \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt}
$$

**예제**: $z = x^2 + y^2$, $x = \cos t$, $y = \sin t$
$$
\frac{dz}{dt} = 2x(-\sin t) + 2y(\cos t) = -2\cos t \sin t + 2\sin t \cos t = 0
$$

($z = \cos^2 t + \sin^2 t = 1$이므로 당연!)

### 4.2 다변수 → 다변수

$z = f(x, y)$이고 $x = x(s, t)$, $y = y(s, t)$이면:
$$
\frac{\partial z}{\partial s} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial s} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial s}
$$
$$
\frac{\partial z}{\partial t} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}
$$

**행렬 형태**:
$$
\begin{bmatrix}
\frac{\partial z}{\partial s} & \frac{\partial z}{\partial t}
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial f}{\partial x} & \frac{\partial f}{\partial y}
\end{bmatrix}
\begin{bmatrix}
\frac{\partial x}{\partial s} & \frac{\partial x}{\partial t} \\
\frac{\partial y}{\partial s} & \frac{\partial y}{\partial t}
\end{bmatrix}
$$

이것이 **Jacobian**!

### 4.3 일반 Chain Rule

$\mathbf{z} = f(\mathbf{y})$, $\mathbf{y} = g(\mathbf{x})$이면:
$$
\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial f}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
$$

**딥러닝 연결**: 신경망 $\mathbf{z} = f_L(f_{L-1}(\cdots f_1(\mathbf{x})))$의 gradient는 Jacobian의 연쇄 곱!

---

## 5. Jacobian (야코비안)

### 5.1 정의

**정의**: 벡터 함수 $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$의 Jacobian:
$$
J_{\mathbf{f}}(\mathbf{x}) = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix} \in \mathbb{R}^{m \times n}
$$

**표기**: $J$, $\frac{\partial \mathbf{f}}{\partial \mathbf{x}}$, $D\mathbf{f}$

**직관적 이해**:
- Jacobian은 "벡터 함수의 선형 근사"를 나타내는 행렬이다.
- 입력 $\mathbf{x}$가 조금 변할 때, 출력 $\mathbf{f}(\mathbf{x})$가 어떻게 변하는지를 포착한다.
- 각 행은 "출력의 한 성분이 입력 전체에 대해 어떻게 변하는가"를 나타낸다.

$$
\mathbf{f}(\mathbf{x} + \Delta\mathbf{x}) \approx \mathbf{f}(\mathbf{x}) + J_{\mathbf{f}}(\mathbf{x}) \Delta\mathbf{x}
$$

이것은 벡터 함수의 1차 테일러 근사다.

### 5.2 예제

**예제**: $\mathbf{f}(x, y, z) = (xy, yz, zx)$
$$
J = \begin{bmatrix}
y & x & 0 \\
0 & z & y \\
z & 0 & x
\end{bmatrix}
$$

점 $(1, 1, 1)$에서:
$$
J(1, 1, 1) = \begin{bmatrix}
1 & 1 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1
\end{bmatrix}
$$

### 5.3 Gradient는 Jacobian의 특수 케이스

$f: \mathbb{R}^n \rightarrow \mathbb{R}$ (스칼라 함수)이면:
$$
J_f = \begin{bmatrix}
\frac{\partial f}{\partial x_1} & \cdots & \frac{\partial f}{\partial x_n}
\end{bmatrix} = (\nabla f)^\top
$$

$1 \times n$ 행렬 (행 벡터).

### 5.4 Chain Rule in Jacobian Form

$\mathbf{h} = \mathbf{f} \circ \mathbf{g}$이면:
$$
J_{\mathbf{h}}(\mathbf{x}) = J_{\mathbf{f}}(\mathbf{g}(\mathbf{x})) \cdot J_{\mathbf{g}}(\mathbf{x})
$$

**딥러닝**: Backpropagation = Jacobian의 연쇄 곱!

### 5.5 기하학적 의미

Jacobian은 "국소적 선형 변환"을 나타낸다.

점 $\mathbf{x}$ 근처에서 함수 $\mathbf{f}$는 대략 선형 변환처럼 동작하고, 그 변환 행렬이 바로 $J_{\mathbf{f}}(\mathbf{x})$다.

**예시**: 극좌표 변환 $\mathbf{f}(r, \theta) = (r\cos\theta, r\sin\theta)$
$$
J = \begin{bmatrix}
\cos\theta & -r\sin\theta \\
\sin\theta & r\cos\theta
\end{bmatrix}
$$

행렬식 $\det(J) = r$는 "면적 스케일링 비율"을 나타낸다. 반지름 $r$인 원 위에서는 작은 영역이 $r$배로 확대된다.

### 5.6 딥러닝에서의 역할

**신경망의 각 레이어**를 벡터 함수로 보면:
- 입력: $\mathbf{x} \in \mathbb{R}^n$
- 출력: $\mathbf{y} = \mathbf{f}(\mathbf{x}) \in \mathbb{R}^m$
- Jacobian $J \in \mathbb{R}^{m \times n}$: 입력이 출력에 미치는 영향

**역전파 과정**:
$$
\frac{\partial L}{\partial \mathbf{x}} = J_{\mathbf{f}}(\mathbf{x})^\top \frac{\partial L}{\partial \mathbf{y}}
$$

즉, 손실 $L$의 출력에 대한 gradient를 입력에 대한 gradient로 변환할 때 Jacobian의 전치를 곱한다.

**다층 신경망**:
$$
\mathbf{y} = f_3(f_2(f_1(\mathbf{x})))
$$

전체 Jacobian:
$$
J_{\mathbf{y}}(\mathbf{x}) = J_{f_3} \cdot J_{f_2} \cdot J_{f_1}
$$

Gradient:
$$
\frac{\partial L}{\partial \mathbf{x}} = J_{f_1}^\top J_{f_2}^\top J_{f_3}^\top \frac{\partial L}{\partial \mathbf{y}}
$$

이것이 바로 **역전파(backpropagation)**의 수학적 본질이다.

---

## 6. Hessian (헤시안)

### 6.1 정의

**정의**: 스칼라 함수 $f: \mathbb{R}^n \rightarrow \mathbb{R}$의 2차 편미분 행렬:
$$
H_f(\mathbf{x}) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

**표기**: $H$, $\nabla^2 f$, $D^2 f$

**직관적 이해**:
- Hessian은 "함수의 곡률(curvature)"을 나타내는 행렬이다.
- Gradient가 "어느 방향으로 가야 하는가"를 알려준다면, Hessian은 "얼마나 가파르게 휘어지는가"를 알려준다.
- 2차 테일러 근사:
$$
f(\mathbf{x} + \Delta\mathbf{x}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\top \Delta\mathbf{x} + \frac{1}{2} \Delta\mathbf{x}^\top H_f(\mathbf{x}) \Delta\mathbf{x}
$$

마지막 항 $\frac{1}{2} \Delta\mathbf{x}^\top H \Delta\mathbf{x}$가 "곡률에 의한 2차 보정항"이다.

### 6.2 Schwarz의 정리

**정리**: $f$가 $C^2$ (2차 편미분 연속)이면:
$$
\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}
$$

따라서 **Hessian은 대칭 행렬**.

### 6.3 예제

**예제**: $f(x, y) = x^3 + xy^2 - y^3$
$$
\frac{\partial f}{\partial x} = 3x^2 + y^2, \quad
\frac{\partial f}{\partial y} = 2xy - 3y^2
$$
$$
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{bmatrix}
= \begin{bmatrix}
6x & 2y \\
2y & 2x - 6y
\end{bmatrix}
$$

### 6.4 2차 도함수 판정법

**핵심 아이디어**: "임계점 주변의 곡면 모양은 Hessian의 고유값 부호로 결정된다."

#### 6.4.1 단계별 이해

**1단계: 1차 미분 (Gradient)**

임계점 찾기:
$$
\nabla f(\mathbf{x}) = \mathbf{0}
$$

이것은 "기울기가 0"인 점, 즉 정점을 찾는 단계다.
언덕 꼭대기, 골짜기 바닥, 혹은 안장점 같은 곳이다.

**2단계: 2차 미분 (Hessian Matrix)**

곡면의 "굽힘(curvature)" 정보:
$$
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{bmatrix}
$$

각 방향으로 얼마나 휘었는지를 알려준다.

#### 6.4.2 고유값이 말해주는 것

Hessian의 고유값 $\lambda_1, \lambda_2, \ldots, \lambda_n$은 "함수가 그 방향(고유벡터 방향)으로 얼마나 휘었는가"를 나타낸다.

| 고유값 부호                       | 기하학적 의미           | 예시 모양 | 판정    |
| ---------------------------- | ----------------- | ----- | ----- |
| $\lambda_1, \lambda_2 > 0$   | 양쪽 다 위로 볼록        | 그릇    | 극소    |
| $\lambda_1, \lambda_2 < 0$   | 양쪽 다 아래로 볼록       | 산     | 극대    |
| $\lambda_1, \lambda_2$ 부호 다름 | 한쪽 위로, 한쪽 아래로 휘어짐 | 안장    | 안장점   |
| $\lambda$ 중 0 포함             | 평평한 방향 존재         | 평면 포함 | 판정 불가 |

#### 6.4.3 정리

**정리**: $\nabla f(\mathbf{x}^*) = \mathbf{0}$인 임계점에서:
- $H$가 양정부 (모든 고유값 > 0) → **극소**
- $H$가 음정부 (모든 고유값 < 0) → **극대**
- $H$가 부정 (양수/음수 고유값 혼재) → **안장점**
- $H$가 반정부 (고유값 중 0 포함) → **판정 불가**

#### 6.4.4 상세 예제: $f(x, y) = x^2 - y^2$ (안장점)

이것은 안장점의 대표적인 예다.

**Step 1: 임계점 찾기**
$$
\nabla f = \begin{bmatrix} 2x \\ -2y \end{bmatrix} = \mathbf{0}
$$

임계점: $(0, 0)$

**Step 2: Hessian 계산**
$$
H = \begin{bmatrix} 2 & 0 \\ 0 & -2 \end{bmatrix}
$$

**Step 3: 고유값 구하기**

고유값: $\lambda_1 = 2$, $\lambda_2 = -2$ (부호가 다르다!)

**Step 4: 기하학적 해석**
- x 방향 ($\lambda = 2 > 0$): 위로 볼록 (극소처럼)
- y 방향 ($\lambda = -2 < 0$): 아래로 볼록 (극대처럼)

**결론**: 안장점

**직관적 설명**:
- $f(x, y) = x^2 - y^2$는 가운데가 평평한 마루처럼 생겼다.
- x 방향으로는 위로, y 방향으로는 아래로 휘어 있다.
- 중심 $(0, 0)$에 공을 두면:
  - x축 방향으로는 공이 가운데로 굴러오고 (극소)
  - y축 방향으로는 공이 밖으로 굴러나간다 (극대)
- 즉, "어느 한 방향에서는 극소처럼 보이지만, 다른 방향에서는 극대처럼 보이는" 점이다.

#### 6.4.5 추가 예제

**예제 1**: $f(x, y) = x^2 + y^2$ (극소)
$$
\nabla f = (2x, 2y) = \mathbf{0} \quad \Rightarrow \quad (0, 0)
$$
$$
H = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}, \quad \lambda_1 = \lambda_2 = 2 > 0
$$
→ 양정부 → 극소 (그릇 모양)

**예제 2**: $f(x, y) = -x^2 - y^2$ (극대)
$$
H = \begin{bmatrix} -2 & 0 \\ 0 & -2 \end{bmatrix}, \quad \lambda_1 = \lambda_2 = -2 < 0
$$
→ 음정부 → 극대 (산 모양)

**예제 3**: $f(x, y) = x^3$ (판정 불가)
$$
\nabla f = 3x^2 = 0 \quad \Rightarrow \quad x = 0
$$
$$
H = \frac{\partial^2 f}{\partial x^2} = 6x = 0 \text{ at } x = 0
$$
→ Hessian이 0 → 판정 불가 (실제로는 변곡점)

### 6.5 기하학적 의미: 곡률과 방향별 민감도

Hessian은 "각 방향으로 얼마나 휘어져 있는가"를 나타낸다.

**방향별 곡률**:
단위 벡터 $\mathbf{u}$ 방향으로의 2차 방향 도함수:
$$
\frac{\partial^2 f}{\partial \mathbf{u}^2} = \mathbf{u}^\top H \mathbf{u}
$$

- $\mathbf{u}^\top H \mathbf{u} > 0$: 이 방향으로 위로 볼록 (아래로 오목)
- $\mathbf{u}^\top H \mathbf{u} < 0$: 이 방향으로 아래로 볼록 (위로 오목)
- $\mathbf{u}^\top H \mathbf{u} = 0$: 이 방향으로 평평

**고유벡터 방향의 의미**:
$H$의 고유벡터 방향은 "주요 곡률 방향"이다.
- 고유값이 큰 방향: 가파르게 휘어짐
- 고유값이 작은 방향: 완만하게 휘어짐
- 양의 고유값: 위로 볼록 (극소 방향)
- 음의 고유값: 아래로 볼록 (극대 방향)

### 6.6 조건수와 최적화

**조건수**: $\kappa(H) = \frac{\lambda_{\max}}{\lambda_{\min}}$

조건수는 "가장 가파른 방향과 가장 평평한 방향의 비율"이다.

**조건수가 큰 경우** ($\kappa \gg 1$):
- 한 방향은 매우 가파르고 다른 방향은 평평 (협곡 모양)
- Gradient descent가 지그재그로 진동
- 수렴 속도가 매우 느림 ($O(\kappa)$에 비례)

**조건수가 작은 경우** ($\kappa \approx 1$):
- 모든 방향에서 비슷한 곡률 (원형/구형)
- Gradient descent가 직선으로 수렴
- 빠른 수렴

**딥러닝 최적화**:
- SGD: Hessian을 전혀 고려하지 않음 → 조건수가 크면 느림
- Newton's method: $H^{-1}$을 사용 → 조건수 문제 해결, 하지만 계산 비용 큼
- Adam, RMSprop: Hessian의 대각 성분을 근사하여 precondition → 중간 해법

### 6.7 Hessian과 Gradient의 관계

| 개념 | Gradient $\nabla f$ | Hessian $H$ |
|------|-------------------|-------------|
| 차수 | 1차 도함수 | 2차 도함수 |
| 형태 | 벡터 ($n \times 1$) | 행렬 ($n \times n$) |
| 의미 | 방향 (어디로 가는가) | 곡률 (얼마나 휘어지는가) |
| 최적화 역할 | 하강 방향 결정 | 스텝 크기 조정 |
| 계산 비용 | $O(n)$ | $O(n^2)$ |
| 딥러닝 활용 | 모든 optimizer | 2차 방법 (L-BFGS 등) |

**결합된 정보**:
- Gradient만: 어느 방향으로 가야 하는지는 알지만, 얼마나 가야 하는지 모름
- Gradient + Hessian: 방향과 적절한 스텝 크기를 모두 알 수 있음

$$
\mathbf{x}_{t+1} = \mathbf{x}_t - H^{-1} \nabla f(\mathbf{x}_t) \quad \text{(Newton's method)}
$$

### 6.8 딥러닝에서의 Hessian

**문제점**:
- 파라미터 수가 $n$이면 Hessian은 $n \times n$ 행렬
- 현대 신경망은 $n = 10^6 \sim 10^9$ → Hessian 저장 불가능

**근사 방법**:
1. **대각 근사**: Hessian의 대각 성분만 사용 (Adam, RMSprop)
2. **Fisher Information Matrix**: Hessian의 기댓값으로 근사 (Natural Gradient)
3. **Low-rank 근사**: Hessian의 주요 고유벡터만 추정 (Shampoo)
4. **Hessian-vector product**: 전체 Hessian 대신 방향별 곱만 계산

**실제 활용 예시**:
- **PCA**: 공분산 행렬 (Hessian과 유사)의 고유벡터 = 주성분
- **Trust Region**: Hessian으로 안전한 업데이트 범위 결정
- **Saddle Point 탈출**: Hessian의 음수 고유값 방향으로 이동

---

## 7. 암묵적 함수 정리

### 7.1 2변수 케이스

**정리**: $F(x, y) = 0$이 $y = f(x)$를 정의한다면:
$$
\frac{dy}{dx} = -\frac{\partial F / \partial x}{\partial F / \partial y}
$$

**예제**: $x^2 + y^2 = 1$ (원)
- $F(x, y) = x^2 + y^2 - 1$
- $\frac{\partial F}{\partial x} = 2x$, $\frac{\partial F}{\partial y} = 2y$
$$
\frac{dy}{dx} = -\frac{2x}{2y} = -\frac{x}{y}
$$

### 7.2 일반 형태

**정리 (Implicit Function Theorem)**: $\mathbf{F}(\mathbf{x}, \mathbf{y}) = \mathbf{0}$이 $\mathbf{y} = \mathbf{g}(\mathbf{x})$를 정의하고, $\frac{\partial \mathbf{F}}{\partial \mathbf{y}}$가 가역이면:
$$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = -\left( \frac{\partial \mathbf{F}}{\partial \mathbf{y}} \right)^{-1} \frac{\partial \mathbf{F}}{\partial \mathbf{x}}
$$

**딥러닝 연결**: Implicit differentiation, Neural ODE에서 사용.

---

## 8. PyTorch 실습

```python
import torch

# 1. 편미분
x = torch.tensor([1.0, 2.0], requires_grad=True)
f = x[0]**2 * x[1] + torch.sin(x[0] * x[1])

f.backward()
print(f"∇f = {x.grad}")
# 이론: ∂f/∂x = 2xy + y*cos(xy) = 2*1*2 + 2*cos(2) = 4 + 2*cos(2)
#       ∂f/∂y = x^2 + x*cos(xy) = 1 + cos(2)

# 2. Gradient descent
def loss(theta):
    return (theta[0] - 3)**2 + (theta[1] + 2)**2

theta = torch.tensor([0.0, 0.0], requires_grad=True)
lr = 0.1

for step in range(50):
    L = loss(theta)
    L.backward()

    with torch.no_grad():
        theta -= lr * theta.grad
        theta.grad.zero_()

    if step % 10 == 0:
        print(f"Step {step}: θ = {theta.detach()}, L = {L.item():.4f}")

print(f"최종: θ = {theta.detach()}, 정답 = [3, -2]")

# 3. Jacobian
def f(x):
    return torch.stack([x[0] * x[1], x[1] * x[2], x[2] * x[0]])

x = torch.tensor([1.0, 1.0, 1.0], requires_grad=True)
J = torch.autograd.functional.jacobian(f, x)
print(f"Jacobian:\n{J}")
# 이론:
# [[y, x, 0],
#  [0, z, y],
#  [z, 0, x]] = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]

# 4. Hessian
def g(x):
    return x[0]**3 + x[0] * x[1]**2 - x[1]**3

x = torch.tensor([1.0, 2.0], requires_grad=True)
H = torch.autograd.functional.hessian(g, x)
print(f"Hessian:\n{H}")
# 이론:
# [[6x, 2y],
#  [2y, 2x - 6y]] = [[6, 4], [4, 2 - 12]] = [[6, 4], [4, -10]]

# 5. 2차 도함수 판정법
eigenvalues = torch.linalg.eigvalsh(H)
print(f"고유값: {eigenvalues}")
if torch.all(eigenvalues > 0):
    print("극소")
elif torch.all(eigenvalues < 0):
    print("극대")
else:
    print("안장점")

# 6. Chain rule 검증
x = torch.tensor(2.0, requires_grad=True)
y = x**2  # y = x^2
z = torch.sin(y)  # z = sin(y)

z.backward()
# dz/dx = dz/dy * dy/dx = cos(y) * 2x = cos(4) * 4
print(f"dz/dx = {x.grad.item()}")
print(f"이론값: {(torch.cos(torch.tensor(4.0)) * 4).item()}")
```

---

## 9. 연습 문제

1. **편미분 계산**
   $f(x, y, z) = x^2 yz + e^{xyz}$의 모든 1차 편미분을 구하라.

2. **Gradient**
   $f(x, y) = x^2 + 4xy + y^2$의 gradient를 구하고, 점 $(1, -1)$에서 가장 가파른 상승 방향을 구하라.

3. **방향 미분**
   $f(x, y) = xy$의 점 $(2, 3)$에서 방향 $\mathbf{u} = \frac{1}{\sqrt{5}}(2, 1)$로의 방향 미분을 계산하라.

4. **Chain Rule**
   $z = e^{x^2 + y^2}$이고 $x = r\cos\theta$, $y = r\sin\theta$일 때, $\frac{\partial z}{\partial r}$과 $\frac{\partial z}{\partial \theta}$를 구하라.

5. **Jacobian**
   극좌표 변환 $\mathbf{f}(r, \theta) = (r\cos\theta, r\sin\theta)$의 Jacobian을 계산하고, 그 행렬식이 $r$임을 보여라.

6. **Hessian과 극값**
   $f(x, y) = x^3 + y^3 - 3xy$의 임계점을 찾고, Hessian으로 각각의 성질(극소/극대/안장점)을 판별하라.
