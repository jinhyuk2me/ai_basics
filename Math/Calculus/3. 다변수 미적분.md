## TL;DR
- 다변수 미분은 여러 입력에 대한 함수의 변화를 분석하는 도구로, 신경망의 gradient descent를 이해하는 핵심이다.
- 편미분, gradient, Jacobian, Hessian은 각각 "한 방향 변화율", "가장 가파른 방향", "벡터 함수 미분", "곡률 정보"를 제공한다.
- Chain rule의 다변수 버전이 backpropagation의 수학적 토대이며, 이를 확실히 이해해야 딥러닝 학습 과정을 제대로 파악할 수 있다.

---

## 1. 편미분 (Partial Derivative)

### 1.1 정의

**정의**: 다변수 함수 $f(x_1, \ldots, x_n)$의 $x_i$에 대한 편미분:
$$
\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i + h, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}
$$

**의미**: 다른 변수를 고정하고 $x_i$만 변할 때의 변화율.

**표기법**:
- $\frac{\partial f}{\partial x}$, $f_x$, $\partial_x f$, $D_x f$

### 1.2 계산 예제

**예제 1**: $f(x, y) = x^2 y + \sin(xy)$
$$
\frac{\partial f}{\partial x} = 2xy + y\cos(xy)
$$
$$
\frac{\partial f}{\partial y} = x^2 + x\cos(xy)
$$

**예제 2**: $f(x, y, z) = e^{xyz}$
$$
\frac{\partial f}{\partial x} = yze^{xyz}, \quad
\frac{\partial f}{\partial y} = xze^{xyz}, \quad
\frac{\partial f}{\partial z} = xye^{xyz}
$$

### 1.3 기하학적 해석

$z = f(x, y)$의 그래프에서:
- $\frac{\partial f}{\partial x}$: $y$를 고정하고 $x$ 방향으로 자른 곡선의 기울기
- $\frac{\partial f}{\partial y}$: $x$를 고정하고 $y$ 방향으로 자른 곡선의 기울기

---

## 2. Gradient (그래디언트)

### 2.1 정의

**정의**: 스칼라 함수 $f: \mathbb{R}^n \rightarrow \mathbb{R}$의 gradient:
$$
\nabla f = \begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix}
$$

**표기법**: $\nabla f$, $\text{grad } f$

### 2.2 기하학적 의미

**정리**: $\nabla f(\mathbf{x})$는 점 $\mathbf{x}$에서 $f$가 가장 빠르게 증가하는 방향이며, 그 크기는 최대 변화율이다.

**증명 스케치**:
방향 미분 $D_{\mathbf{u}} f = \nabla f \cdot \mathbf{u}$는 $\mathbf{u} = \frac{\nabla f}{\|\nabla f\|}$일 때 최대 ($= \|\nabla f\|$). ∎

### 2.3 예제

**예제**: $f(x, y) = x^2 + y^2$
$$
\nabla f = \begin{bmatrix} 2x \\ 2y \end{bmatrix}
$$

점 $(1, 2)$에서:
$$
\nabla f(1, 2) = \begin{bmatrix} 2 \\ 4 \end{bmatrix}
$$

이 방향으로 움직이면 $f$가 가장 빠르게 증가.

### 2.4 Gradient Descent

**아이디어**: 손실 함수 $L(\theta)$를 최소화하려면 $-\nabla L$ 방향으로 이동:
$$
\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)
$$

여기서 $\eta$는 학습률(learning rate).

**딥러닝 연결**: 모든 최적화 알고리즘의 기초!

---

## 3. 방향 미분 (Directional Derivative)

### 3.1 정의

**정의**: 단위 벡터 $\mathbf{u}$ 방향으로의 방향 미분:
$$
D_{\mathbf{u}} f(\mathbf{x}) = \lim_{h \to 0} \frac{f(\mathbf{x} + h\mathbf{u}) - f(\mathbf{x})}{h}
$$

### 3.2 Gradient와의 관계

**정리**:
$$
D_{\mathbf{u}} f = \nabla f \cdot \mathbf{u}
$$

**증명**: Chain rule 사용. $g(t) = f(\mathbf{x} + t\mathbf{u})$라 하면
$$
g'(0) = \nabla f(\mathbf{x}) \cdot \mathbf{u} \quad \text{∎}
$$

**예제**: $f(x, y) = x^2 + y^2$, 점 $(1, 2)$, 방향 $\mathbf{u} = \frac{1}{\sqrt{2}}(1, 1)$
$$
D_{\mathbf{u}} f(1, 2) = \begin{bmatrix} 2 \\ 4 \end{bmatrix} \cdot \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{6}{\sqrt{2}} = 3\sqrt{2}
$$

---

## 4. Chain Rule (연쇄 법칙)

### 4.1 단변수 → 다변수

$z = f(x, y)$이고 $x = x(t)$, $y = y(t)$이면:
$$
\frac{dz}{dt} = \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt}
$$

**예제**: $z = x^2 + y^2$, $x = \cos t$, $y = \sin t$
$$
\frac{dz}{dt} = 2x(-\sin t) + 2y(\cos t) = -2\cos t \sin t + 2\sin t \cos t = 0
$$

($z = \cos^2 t + \sin^2 t = 1$이므로 당연!)

### 4.2 다변수 → 다변수

$z = f(x, y)$이고 $x = x(s, t)$, $y = y(s, t)$이면:
$$
\frac{\partial z}{\partial s} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial s} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial s}
$$
$$
\frac{\partial z}{\partial t} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}
$$

**행렬 형태**:
$$
\begin{bmatrix}
\frac{\partial z}{\partial s} & \frac{\partial z}{\partial t}
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial f}{\partial x} & \frac{\partial f}{\partial y}
\end{bmatrix}
\begin{bmatrix}
\frac{\partial x}{\partial s} & \frac{\partial x}{\partial t} \\
\frac{\partial y}{\partial s} & \frac{\partial y}{\partial t}
\end{bmatrix}
$$

이것이 **Jacobian**!

### 4.3 일반 Chain Rule

$\mathbf{z} = f(\mathbf{y})$, $\mathbf{y} = g(\mathbf{x})$이면:
$$
\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial f}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
$$

**딥러닝 연결**: 신경망 $\mathbf{z} = f_L(f_{L-1}(\cdots f_1(\mathbf{x})))$의 gradient는 Jacobian의 연쇄 곱!

---

## 5. Jacobian (야코비안)

### 5.1 정의

**정의**: 벡터 함수 $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$의 Jacobian:
$$
J_{\mathbf{f}}(\mathbf{x}) = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix} \in \mathbb{R}^{m \times n}
$$

**표기**: $J$, $\frac{\partial \mathbf{f}}{\partial \mathbf{x}}$, $D\mathbf{f}$

### 5.2 예제

**예제**: $\mathbf{f}(x, y, z) = (xy, yz, zx)$
$$
J = \begin{bmatrix}
y & x & 0 \\
0 & z & y \\
z & 0 & x
\end{bmatrix}
$$

점 $(1, 1, 1)$에서:
$$
J(1, 1, 1) = \begin{bmatrix}
1 & 1 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1
\end{bmatrix}
$$

### 5.3 Gradient는 Jacobian의 특수 케이스

$f: \mathbb{R}^n \rightarrow \mathbb{R}$ (스칼라 함수)이면:
$$
J_f = \begin{bmatrix}
\frac{\partial f}{\partial x_1} & \cdots & \frac{\partial f}{\partial x_n}
\end{bmatrix} = (\nabla f)^\top
$$

$1 \times n$ 행렬 (행 벡터).

### 5.4 Chain Rule in Jacobian Form

$\mathbf{h} = \mathbf{f} \circ \mathbf{g}$이면:
$$
J_{\mathbf{h}}(\mathbf{x}) = J_{\mathbf{f}}(\mathbf{g}(\mathbf{x})) \cdot J_{\mathbf{g}}(\mathbf{x})
$$

**딥러닝**: Backpropagation = Jacobian의 연쇄 곱!

---

## 6. Hessian (헤시안)

### 6.1 정의

**정의**: 스칼라 함수 $f: \mathbb{R}^n \rightarrow \mathbb{R}$의 2차 편미분 행렬:
$$
H_f(\mathbf{x}) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

**표기**: $H$, $\nabla^2 f$, $D^2 f$

### 6.2 Schwarz의 정리

**정리**: $f$가 $C^2$ (2차 편미분 연속)이면:
$$
\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}
$$

따라서 **Hessian은 대칭 행렬**.

### 6.3 예제

**예제**: $f(x, y) = x^3 + xy^2 - y^3$
$$
\frac{\partial f}{\partial x} = 3x^2 + y^2, \quad
\frac{\partial f}{\partial y} = 2xy - 3y^2
$$
$$
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{bmatrix}
= \begin{bmatrix}
6x & 2y \\
2y & 2x - 6y
\end{bmatrix}
$$

### 6.4 2차 도함수 판정법

**정리**: $\nabla f(\mathbf{x}^*) = \mathbf{0}$인 임계점에서:
- $H$가 양정부 (모든 고유값 > 0) → 극소
- $H$가 음정부 (모든 고유값 < 0) → 극대
- $H$가 부정 (양수/음수 고유값 혼재) → 안장점
- $H$가 반정부 → 판정 불가

**예제**: $f(x, y) = x^2 - y^2$
- $\nabla f = (2x, -2y) = (0, 0)$ at $(0, 0)$
- $H = \begin{bmatrix} 2 & 0 \\ 0 & -2 \end{bmatrix}$
- 고유값: $2, -2$ → 안장점!

### 6.5 곡률과 조건수

**조건수**: $\kappa(H) = \frac{\lambda_{\max}}{\lambda_{\min}}$

- $\kappa$가 크면 한 방향은 가파르고 다른 방향은 평평 → gradient descent 느림
- Adam, RMSprop 등은 Hessian을 근사하여 precondition

---

## 7. 암묵적 함수 정리

### 7.1 2변수 케이스

**정리**: $F(x, y) = 0$이 $y = f(x)$를 정의한다면:
$$
\frac{dy}{dx} = -\frac{\partial F / \partial x}{\partial F / \partial y}
$$

**예제**: $x^2 + y^2 = 1$ (원)
- $F(x, y) = x^2 + y^2 - 1$
- $\frac{\partial F}{\partial x} = 2x$, $\frac{\partial F}{\partial y} = 2y$
$$
\frac{dy}{dx} = -\frac{2x}{2y} = -\frac{x}{y}
$$

### 7.2 일반 형태

**정리 (Implicit Function Theorem)**: $\mathbf{F}(\mathbf{x}, \mathbf{y}) = \mathbf{0}$이 $\mathbf{y} = \mathbf{g}(\mathbf{x})$를 정의하고, $\frac{\partial \mathbf{F}}{\partial \mathbf{y}}$가 가역이면:
$$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = -\left( \frac{\partial \mathbf{F}}{\partial \mathbf{y}} \right)^{-1} \frac{\partial \mathbf{F}}{\partial \mathbf{x}}
$$

**딥러닝 연결**: Implicit differentiation, Neural ODE에서 사용.

---

## 8. PyTorch 실습

```python
import torch

# 1. 편미분
x = torch.tensor([1.0, 2.0], requires_grad=True)
f = x[0]**2 * x[1] + torch.sin(x[0] * x[1])

f.backward()
print(f"∇f = {x.grad}")
# 이론: ∂f/∂x = 2xy + y*cos(xy) = 2*1*2 + 2*cos(2) = 4 + 2*cos(2)
#       ∂f/∂y = x^2 + x*cos(xy) = 1 + cos(2)

# 2. Gradient descent
def loss(theta):
    return (theta[0] - 3)**2 + (theta[1] + 2)**2

theta = torch.tensor([0.0, 0.0], requires_grad=True)
lr = 0.1

for step in range(50):
    L = loss(theta)
    L.backward()

    with torch.no_grad():
        theta -= lr * theta.grad
        theta.grad.zero_()

    if step % 10 == 0:
        print(f"Step {step}: θ = {theta.detach()}, L = {L.item():.4f}")

print(f"최종: θ = {theta.detach()}, 정답 = [3, -2]")

# 3. Jacobian
def f(x):
    return torch.stack([x[0] * x[1], x[1] * x[2], x[2] * x[0]])

x = torch.tensor([1.0, 1.0, 1.0], requires_grad=True)
J = torch.autograd.functional.jacobian(f, x)
print(f"Jacobian:\n{J}")
# 이론:
# [[y, x, 0],
#  [0, z, y],
#  [z, 0, x]] = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]

# 4. Hessian
def g(x):
    return x[0]**3 + x[0] * x[1]**2 - x[1]**3

x = torch.tensor([1.0, 2.0], requires_grad=True)
H = torch.autograd.functional.hessian(g, x)
print(f"Hessian:\n{H}")
# 이론:
# [[6x, 2y],
#  [2y, 2x - 6y]] = [[6, 4], [4, 2 - 12]] = [[6, 4], [4, -10]]

# 5. 2차 도함수 판정법
eigenvalues = torch.linalg.eigvalsh(H)
print(f"고유값: {eigenvalues}")
if torch.all(eigenvalues > 0):
    print("극소")
elif torch.all(eigenvalues < 0):
    print("극대")
else:
    print("안장점")

# 6. Chain rule 검증
x = torch.tensor(2.0, requires_grad=True)
y = x**2  # y = x^2
z = torch.sin(y)  # z = sin(y)

z.backward()
# dz/dx = dz/dy * dy/dx = cos(y) * 2x = cos(4) * 4
print(f"dz/dx = {x.grad.item()}")
print(f"이론값: {(torch.cos(torch.tensor(4.0)) * 4).item()}")
```

---

## 9. 연습 문제

1. **편미분 계산**
   $f(x, y, z) = x^2 yz + e^{xyz}$의 모든 1차 편미분을 구하라.

2. **Gradient**
   $f(x, y) = x^2 + 4xy + y^2$의 gradient를 구하고, 점 $(1, -1)$에서 가장 가파른 상승 방향을 구하라.

3. **방향 미분**
   $f(x, y) = xy$의 점 $(2, 3)$에서 방향 $\mathbf{u} = \frac{1}{\sqrt{5}}(2, 1)$로의 방향 미분을 계산하라.

4. **Chain Rule**
   $z = e^{x^2 + y^2}$이고 $x = r\cos\theta$, $y = r\sin\theta$일 때, $\frac{\partial z}{\partial r}$과 $\frac{\partial z}{\partial \theta}$를 구하라.

5. **Jacobian**
   극좌표 변환 $\mathbf{f}(r, \theta) = (r\cos\theta, r\sin\theta)$의 Jacobian을 계산하고, 그 행렬식이 $r$임을 보여라.

6. **Hessian과 극값**
   $f(x, y) = x^3 + y^3 - 3xy$의 임계점을 찾고, Hessian으로 각각의 성질(극소/극대/안장점)을 판별하라.

---

## 10. 참고 자료

- James Stewart, *Calculus*, Chapter 14-15
- Tom M. Apostol, *Calculus*, Volume 2
- MIT OCW 18.02 Multivariable Calculus
- 3Blue1Brown, *Essence of Calculus*, Episode 7-10

---

## 11. 다음 학습

- [[Math/Calculus/자동 미분|자동 미분과 Backpropagation]]
- [[Math/Calculus/테일러 급수와 최적화|테일러 전개와 최적화]]
- [[Math/Linear Algebra/고유값과 고유벡터|고유값과 고유벡터]] (Hessian 분석)
- [[Math/Optimization/최적화 기초|최적화 기초]]
