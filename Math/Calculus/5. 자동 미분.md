## TL;DR
- 자동 미분(Automatic Differentiation, AutoDiff)은 chain rule을 체계적으로 적용하여 복잡한 합성 함수의 미분을 자동 계산하는 기법이다.
- Forward mode와 Reverse mode(backpropagation)가 있으며, 딥러닝에서는 출력이 스칼라인 경우가 많아 reverse mode가 효율적이다.
- PyTorch, TensorFlow의 `autograd` 시스템은 계산 그래프를 구축하고 역전파로 gradient를 계산한다.

---

## 1. 미분 계산의 세 가지 방법

### 1.1 수치 미분 (Numerical Differentiation)

**유한 차분**:
$$
f'(x) \approx \frac{f(x + h) - f(x)}{h}
$$

**문제점**:
- $h$가 너무 크면 부정확
- $h$가 너무 작으면 부동소수점 오차
- $n$차원에서 $O(n)$번 함수 평가 필요

**딥러닝에서**: 학습에 사용 불가 (너무 느림), gradient 검증용으로만 사용.

### 1.2 기호 미분 (Symbolic Differentiation)

**방법**: 수학적 규칙을 적용하여 도함수 수식 유도.

**예**: SymPy, Mathematica

**문제점**:
- 표현식이 기하급수적으로 커짐 ("expression swell")
- $f(x) = \prod_{i=1}^n x_i$의 미분은 $n$개 항으로 폭발

### 1.3 자동 미분 (Automatic Differentiation)

**핵심 아이디어**:
- 모든 함수는 기본 연산($+, -, \times, /, \sin, \exp$ 등)의 합성
- 각 기본 연산의 미분 규칙은 알려져 있음
- Chain rule을 체계적으로 적용

**장점**:
- 정확 (기계 정밀도까지)
- 효율적 (최대 $O(n)$ 또는 $O(1)$ 함수 평가)
- 표현식 폭발 없음

---

## 2. 계산 그래프 (Computational Graph)

### 2.1 정의

**계산 그래프**: 연산을 노드, 데이터 흐름을 엣지로 표현한 DAG (Directed Acyclic Graph).

**예제**: $f(x_1, x_2) = (x_1 x_2) \sin(x_1)$

```
x_1 ───┬───> v_1 = x_1 ───> v_3 = sin(v_1) ───┐
       │                                        ├─> v_4 = v_2 * v_3 = f
x_2 ───┴───> v_2 = x_1 * x_2 ──────────────────┘
```

### 2.2 Forward Pass (순전파)

입력부터 출력까지 계산:

| 노드 | 연산 | 값 (x₁=2, x₂=3) |
|------|------|------------------|
| $v_1$ | $x_1$ | 2 |
| $v_2$ | $x_1 \cdot x_2$ | 6 |
| $v_3$ | $\sin(v_1)$ | $\sin(2) \approx 0.909$ |
| $v_4$ | $v_2 \cdot v_3$ | $6 \times 0.909 = 5.454$ |

### 2.3 Backward Pass (역전파)

출력부터 입력까지 gradient 계산 (chain rule):

$$
\frac{\partial f}{\partial x_i} = \sum_{\text{parents } j} \frac{\partial f}{\partial v_j} \frac{\partial v_j}{\partial x_i}
$$

---

## 3. Forward Mode

### 3.1 원리

**아이디어**: Dual number $x + \dot{x}\epsilon$ (단, $\epsilon^2 = 0$) 사용.

함수 $f$에 대입:
$$
f(x + \dot{x}\epsilon) = f(x) + f'(x)\dot{x}\epsilon
$$

**예제**: $f(x) = x^2$
$$
f(x + \epsilon) = (x + \epsilon)^2 = x^2 + 2x\epsilon + \epsilon^2 = x^2 + 2x\epsilon
$$

계수 $2x$가 도함수!

### 3.2 다변수

**Jacobian-vector product**:
$$
J \mathbf{v}
$$

$\mathbf{v}$ 방향으로 한 번에 계산.

### 3.3 복잡도

- 한 번의 forward pass로 $\frac{\partial f}{\partial x_i}$ 하나 계산
- $n$개 입력이면 $n$번 필요
- **$n \gg m$ (입력이 많고 출력이 적으면) 비효율적**

---

## 4. Reverse Mode (Backpropagation)

### 4.1 원리

**아이디어**: 출력에서 시작하여 거꾸로 gradient 전파.

**Adjoint (adjoint variable)**:
$$
\bar{v}_i = \frac{\partial f}{\partial v_i}
$$

**Chain rule 적용**:
$$
\bar{v}_i = \sum_{j \in \text{children}(i)} \bar{v}_j \frac{\partial v_j}{\partial v_i}
$$

### 4.2 알고리즘

1. **Forward pass**: 모든 중간값 $v_i$ 저장
2. **Backward pass**: $\bar{v}_{\text{output}} = 1$부터 시작하여 역순으로

**예제**: $f(x_1, x_2) = (x_1 + x_2) \cdot x_1$

Forward:
- $v_1 = x_1$
- $v_2 = x_2$
- $v_3 = v_1 + v_2$
- $v_4 = v_3 \cdot v_1$ (= f)

Backward:
- $\bar{v}_4 = 1$
- $\bar{v}_3 = \bar{v}_4 \frac{\partial v_4}{\partial v_3} = 1 \cdot v_1 = x_1$
- $\bar{v}_1 = \bar{v}_4 \frac{\partial v_4}{\partial v_1} + \bar{v}_3 \frac{\partial v_3}{\partial v_1} = 1 \cdot v_3 + \bar{v}_3 \cdot 1 = v_3 + x_1 = 2x_1 + x_2$
- $\bar{v}_2 = \bar{v}_3 \frac{\partial v_3}{\partial v_2} = x_1 \cdot 1 = x_1$

결과: $\frac{\partial f}{\partial x_1} = 2x_1 + x_2$, $\frac{\partial f}{\partial x_2} = x_1$

검증: $f = (x_1 + x_2)x_1 = x_1^2 + x_1 x_2$
- $\frac{\partial f}{\partial x_1} = 2x_1 + x_2$ ✓
- $\frac{\partial f}{\partial x_2} = x_1$ ✓

### 4.3 복잡도

- 한 번의 backward pass로 **모든** 입력에 대한 gradient 계산
- **$m \ll n$ (출력이 적고 입력이 많으면) 매우 효율적**
- 딥러닝: $m = 1$ (손실 함수 스칼라) → reverse mode 최적!

---

## 5. Backpropagation in Neural Networks

### 5.1 신경망의 계산 그래프

**Layer-wise**:
$$
\mathbf{h}_1 = \sigma(W_1 \mathbf{x} + \mathbf{b}_1)
$$
$$
\mathbf{h}_2 = \sigma(W_2 \mathbf{h}_1 + \mathbf{b}_2)
$$
$$
\mathbf{y} = W_3 \mathbf{h}_2 + \mathbf{b}_3
$$
$$
L = \| \mathbf{y} - \mathbf{y}_{\text{true}} \|^2
$$

### 5.2 Gradient 계산

**Chain rule**:
$$
\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{h}_2} \frac{\partial \mathbf{h}_2}{\partial \mathbf{h}_1} \frac{\partial \mathbf{h}_1}{\partial W_1}
$$

**Backward pass**:
1. $\frac{\partial L}{\partial \mathbf{y}}$ (output layer)
2. $\frac{\partial L}{\partial \mathbf{h}_2} = \frac{\partial L}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{h}_2}$
3. $\frac{\partial L}{\partial \mathbf{h}_1} = \frac{\partial L}{\partial \mathbf{h}_2} \frac{\partial \mathbf{h}_2}{\partial \mathbf{h}_1}$
4. $\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial \mathbf{h}_1} \frac{\partial \mathbf{h}_1}{\partial W_1}$

### 5.3 벡터화

**행렬 미분**:
- $\mathbf{z} = W\mathbf{x}$이면 $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \mathbf{z}} \mathbf{x}^\top$
- $\mathbf{z} = \sigma(\mathbf{x})$이면 $\frac{\partial L}{\partial \mathbf{x}} = \frac{\partial L}{\partial \mathbf{z}} \odot \sigma'(\mathbf{x})$ (element-wise)

---

## 6. PyTorch Autograd

### 6.1 기본 사용법

```python
import torch

# requires_grad=True로 추적 시작
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = x**2
z = y.sum()

# backward로 gradient 계산
z.backward()

print(x.grad)  # tensor([2., 4., 6.])
# dz/dx_i = d(sum x_i^2)/dx_i = 2x_i
```

### 6.2 계산 그래프 구축

```python
x = torch.tensor(2.0, requires_grad=True)
a = x * 3
b = a + 2
c = b ** 2

# 그래프: x -> a -> b -> c
# c = ((x * 3) + 2)^2 = (3x + 2)^2

c.backward()
print(x.grad)  # dc/dx = 2(3x+2)*3 = 6(3x+2) = 6*8 = 48
```

### 6.3 중간 gradient 접근

```python
x = torch.tensor(2.0, requires_grad=True)
y = x ** 2
z = y ** 3

# y의 gradient도 필요하면 retain_graph=True
y.retain_grad()

z.backward()
print(f"dz/dx = {x.grad}")  # 2x * 3y^2 = 2x * 3(x^2)^2 = 6x^5
print(f"dz/dy = {y.grad}")  # 3y^2
```

### 6.4 다중 출력

```python
x = torch.tensor([1.0, 2.0], requires_grad=True)
y = x ** 2
z = y.sum()

# 스칼라 출력이 아니면 gradient 벡터 명시
y.backward(torch.tensor([1.0, 1.0]))  # sum과 동일
print(x.grad)
```

---

## 7. Gradient Checking

### 7.1 수치 미분으로 검증

```python
def numerical_grad(f, x, eps=1e-5):
    grad = torch.zeros_like(x)
    for i in range(x.numel()):
        x_plus = x.clone()
        x_plus.view(-1)[i] += eps
        x_minus = x.clone()
        x_minus.view(-1)[i] -= eps

        grad.view(-1)[i] = (f(x_plus) - f(x_minus)) / (2 * eps)
    return grad

def f(x):
    return (x ** 2).sum()

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = f(x)
y.backward()

autograd_grad = x.grad.clone()
numerical = numerical_grad(f, x.detach())

print(f"AutoGrad: {autograd_grad}")
print(f"Numerical: {numerical}")
print(f"Difference: {(autograd_grad - numerical).abs().max().item()}")
```

### 7.2 상대 오차

$$
\text{relative error} = \frac{\| \nabla_{\text{auto}} - \nabla_{\text{num}} \|}{\| \nabla_{\text{auto}} \| + \| \nabla_{\text{num}} \|}
$$

보통 $10^{-5}$ 이하면 정상.

---

## 8. 고급 주제

### 8.1 Higher-order Derivatives

```python
x = torch.tensor(2.0, requires_grad=True)
y = x ** 3

# 1차 도함수
dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]
print(f"dy/dx = {dy_dx.item()}")  # 3x^2 = 12

# 2차 도함수
d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]
print(f"d²y/dx² = {d2y_dx2.item()}")  # 6x = 12
```

### 8.2 Custom Autograd Function

```python
class MyReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input

x = torch.tensor([-1.0, 2.0], requires_grad=True)
y = MyReLU.apply(x)
y.sum().backward()
print(x.grad)  # [0, 1]
```

---

## 9. 연습 문제

1. **계산 그래프**
   $f(x, y, z) = (x + y) \cdot z$의 계산 그래프를 그리고, reverse mode로 모든 편미분을 계산하라.

2. **Backpropagation 손계산**
   2-layer 신경망 ($\mathbf{h} = \sigma(W_1\mathbf{x})$, $\mathbf{y} = W_2\mathbf{h}$, $L = \|\mathbf{y} - \mathbf{t}\|^2$)에서 $\frac{\partial L}{\partial W_1}$을 유도하라.

3. **Gradient Checking**
   임의의 3-layer MLP를 만들고, autograd와 수치 미분 결과를 비교하라.

4. **Custom Function**
   Leaky ReLU ($f(x) = \max(0.01x, x)$)를 `torch.autograd.Function`으로 구현하라.

5. **Higher-order**
   $f(x) = e^{-x^2}$의 2차 도함수를 PyTorch로 계산하고, 이론값 $f''(x) = (4x^2 - 2)e^{-x^2}$와 비교하라.

---

## 10. 참고 자료

- Griewank & Walther, *Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation*
- Baydin et al., "Automatic Differentiation in Machine Learning: a Survey" (JMLR 2018)
- PyTorch Autograd 문서: https://pytorch.org/docs/stable/autograd.html
- CS231n Lecture Notes on Backpropagation

---

## 11. 다음 학습

- [[Math/Calculus/4. 테일러 급수와 최적화|테일러 전개와 최적화]]
- [[Math/Calculus/3. 다변수 미적분|다변수 미분]] (Jacobian, Hessian 복습)
- [[ML Foundations/3. 머신러닝 최적화|ML에서의 최적화]]
- [[Math/Optimization/1. 최적화 기초|최적화 기초]]
