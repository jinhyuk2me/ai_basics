## TL;DR
- 단변수 미분은 "함수값이 입력의 작은 변화에 얼마나 민감한가"를 정량화하는 도구로, 딥러닝의 gradient descent 이해를 위한 기초다.
- 극한의 엄밀한 정의(ε-δ)부터 도함수 규칙, 극값 판별, 평균값 정리까지 대학 미적분 1의 핵심 내용을 다룬다.
- 1차원에서의 직관을 확실히 다진 후 다변수 미분으로 확장하면 Jacobian, Hessian 등 복잡한 개념도 자연스럽게 이해된다.

---

## 1. 극한의 정의

### 1.1 직관적 정의

$x$가 $a$에 가까워질 때 $f(x)$가 $L$에 가까워진다면:
$$
\lim_{x \to a} f(x) = L
$$

**예제**:
$$
\lim_{x \to 2} (3x + 1) = 7
$$

### 1.2 엄밀한 정의 (ε-δ 정의)

**정의**: 다음 조건이 성립하면 $\lim_{x \to a} f(x) = L$이라 한다:

모든 $\epsilon > 0$에 대해, 어떤 $\delta > 0$가 존재하여
$$
0 < |x - a| < \delta \Rightarrow |f(x) - L| < \epsilon
$$

**의미**: $x$를 $a$에 충분히 가까이 ($\delta$ 이내) 하면, $f(x)$를 $L$에 원하는 만큼 가깝게 ($\epsilon$ 이내) 만들 수 있다.

**예제 (증명)**:
$\lim_{x \to 2} (3x + 1) = 7$임을 증명하자.

주어진 $\epsilon > 0$에 대해:
$$
|(3x + 1) - 7| = |3x - 6| = 3|x - 2| < \epsilon
$$
$$
\Leftrightarrow |x - 2| < \frac{\epsilon}{3}
$$

따라서 $\delta = \epsilon/3$로 선택하면 조건 만족. ∎

### 1.3 극한의 성질

**정리**:
1. $\lim_{x \to a} [f(x) + g(x)] = \lim_{x \to a} f(x) + \lim_{x \to a} g(x)$
2. $\lim_{x \to a} [f(x) \cdot g(x)] = \lim_{x \to a} f(x) \cdot \lim_{x \to a} g(x)$
3. $\lim_{x \to a} \frac{f(x)}{g(x)} = \frac{\lim_{x \to a} f(x)}{\lim_{x \to a} g(x)}$ (단, 분모 $\neq 0$)

### 1.4 연속성

**정의**: 함수 $f$가 점 $a$에서 연속 $\Leftrightarrow$ $\lim_{x \to a} f(x) = f(a)$

세 조건:
1. $f(a)$ 정의됨
2. $\lim_{x \to a} f(x)$ 존재
3. 둘이 같음

**딥러닝 연결**: 활성화 함수(ReLU, sigmoid 등)의 연속성이 학습 안정성에 영향.

---

## 2. 도함수의 정의

### 2.1 미분계수

**정의**: 점 $a$에서의 미분계수(derivative):
$$
f'(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}
$$

**기하학적 의미**: 점 $(a, f(a))$에서 접선의 기울기.

**물리적 의미**: 순간 변화율 (속도 = 위치의 미분).

### 2.2 도함수 함수

모든 점에서 미분 가능하면:
$$
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
$$

**표기법**:
- $f'(x)$ (Lagrange)
- $\frac{df}{dx}$ (Leibniz) - 딥러닝에서 주로 사용
- $Df(x)$ (Euler)
- $\dot{f}$ (Newton) - 시간 미분

### 2.3 첫 원리로 미분 계산

**예제 1**: $f(x) = x^2$
$$
f'(x) = \lim_{h \to 0} \frac{(x+h)^2 - x^2}{h} = \lim_{h \to 0} \frac{2xh + h^2}{h} = \lim_{h \to 0} (2x + h) = 2x
$$

**예제 2**: $f(x) = \sin x$
$$
f'(x) = \lim_{h \to 0} \frac{\sin(x+h) - \sin x}{h}
$$
삼각함수 공식 사용:
$$
= \lim_{h \to 0} \frac{\sin x \cos h + \cos x \sin h - \sin x}{h}
$$
$$
= \lim_{h \to 0} \left[ \sin x \frac{\cos h - 1}{h} + \cos x \frac{\sin h}{h} \right] = \cos x
$$

(사용한 극한: $\lim_{h \to 0} \frac{\sin h}{h} = 1$, $\lim_{h \to 0} \frac{\cos h - 1}{h} = 0$)

---

## 3. 미분 규칙

### 3.1 기본 도함수

| 함수 | 도함수 |
|------|--------|
| $c$ (상수) | $0$ |
| $x^n$ | $nx^{n-1}$ |
| $e^x$ | $e^x$ |
| $\ln x$ | $\frac{1}{x}$ |
| $\sin x$ | $\cos x$ |
| $\cos x$ | $-\sin x$ |
| $\tan x$ | $\sec^2 x$ |

### 3.2 연산 규칙

**합/차**:
$$
(f \pm g)' = f' \pm g'
$$

**곱셈 규칙 (Product Rule)**:
$$
(fg)' = f'g + fg'
$$

**증명**:
$$
\begin{aligned}
(fg)'(x) &= \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x)g(x)}{h} \\
&= \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x)g(x+h) + f(x)g(x+h) - f(x)g(x)}{h} \\
&= \lim_{h \to 0} \left[ \frac{f(x+h) - f(x)}{h} g(x+h) + f(x) \frac{g(x+h) - g(x)}{h} \right] \\
&= f'(x)g(x) + f(x)g'(x) \quad \text{∎}
\end{aligned}
$$

**몫의 규칙 (Quotient Rule)**:
$$
\left( \frac{f}{g} \right)' = \frac{f'g - fg'}{g^2}
$$

**예제**: $f(x) = \frac{x^2}{\sin x}$
$$
f'(x) = \frac{2x \sin x - x^2 \cos x}{\sin^2 x}
$$

### 3.3 합성함수 미분 (Chain Rule)

**정리**: $h(x) = f(g(x))$이면
$$
h'(x) = f'(g(x)) \cdot g'(x)
$$

**Leibniz 표기**:
$$
\frac{dh}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}
$$

**예제**: $h(x) = \sin(x^2)$
- $g(x) = x^2$, $f(u) = \sin u$
- $g'(x) = 2x$, $f'(u) = \cos u$
$$
h'(x) = \cos(x^2) \cdot 2x = 2x\cos(x^2)
$$

**딥러닝 연결**: Backpropagation은 chain rule의 반복 적용!

---

## 4. 극값과 최적화

### 4.1 극값의 정의

**정의**:
- **극댓값**: $f(c) \geq f(x)$ for all $x$ in some neighborhood of $c$
- **극솟값**: $f(c) \leq f(x)$ for all $x$ in some neighborhood of $c$

**전역 vs 지역**:
- 전역 최댓값/최솟값 (global)
- 지역 최댓값/최솟값 (local)

### 4.2 페르마 정리 (Fermat's Theorem)

**정리**: $f$가 $c$에서 극값을 가지고 $f'(c)$가 존재하면, $f'(c) = 0$.

**역은 성립 안 함**: $f'(c) = 0$이어도 극값이 아닐 수 있음 (안장점).

**예제**: $f(x) = x^3$에서 $f'(0) = 0$이지만 $x=0$은 극값이 아니다.

### 4.3 1차 도함수 판정법

**정리**:
- $f'(x) > 0$ on $(a, b)$ → $f$는 증가
- $f'(x) < 0$ on $(a, b)$ → $f$는 감소
- $f'$이 $c$에서 양에서 음으로 → $c$는 극대
- $f'$이 $c$에서 음에서 양으로 → $c$는 극소

### 4.4 2차 도함수 판정법

**정리**: $f'(c) = 0$이고,
- $f''(c) > 0$ → $c$는 극소
- $f''(c) < 0$ → $c$는 극대
- $f''(c) = 0$ → 판정 불가 (고차 도함수 필요)

**예제**: $f(x) = x^4 - 4x^3$
1. $f'(x) = 4x^3 - 12x^2 = 4x^2(x - 3)$
2. 임계점: $x = 0, 3$
3. $f''(x) = 12x^2 - 24x$
   - $f''(0) = 0$ (판정 불가)
   - $f''(3) = 36 > 0$ (극소)

---

## 5. 평균값 정리

### 5.1 롤의 정리 (Rolle's Theorem)

**정리**: $f$가 $[a, b]$에서 연속, $(a, b)$에서 미분 가능, $f(a) = f(b)$이면,
어떤 $c \in (a, b)$에서 $f'(c) = 0$.

**기하학적 의미**: 양 끝이 같은 높이면 어딘가 수평 접선 존재.

### 5.2 평균값 정리 (Mean Value Theorem)

**정리**: $f$가 $[a, b]$에서 연속, $(a, b)$에서 미분 가능이면,
$$
f'(c) = \frac{f(b) - f(a)}{b - a}
$$
인 $c \in (a, b)$ 존재.

**의미**: 평균 변화율 = 어떤 점의 순간 변화율.

**증명**: $g(x) = f(x) - \frac{f(b) - f(a)}{b - a}(x - a)$에 롤의 정리 적용. ∎

**응용**: $f'(x) = 0$ for all $x$ → $f$는 상수함수.

---

## 6. 로피탈의 정리

### 6.1 부정형 $\frac{0}{0}$

**정리**: $\lim_{x \to a} f(x) = \lim_{x \to a} g(x) = 0$이고, $g'(x) \neq 0$이면,
$$
\lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}
$$
(우변이 존재한다면)

**예제**: $\lim_{x \to 0} \frac{\sin x}{x}$
$$
= \lim_{x \to 0} \frac{\cos x}{1} = 1
$$

### 6.2 부정형 $\frac{\infty}{\infty}$

같은 정리 적용.

**예제**: $\lim_{x \to \infty} \frac{e^x}{x^2}$
$$
= \lim_{x \to \infty} \frac{e^x}{2x} = \lim_{x \to \infty} \frac{e^x}{2} = \infty
$$

---

## 7. PyTorch 실습

```python
import torch
import matplotlib.pyplot as plt

# 1. 도함수 자동 계산
x = torch.tensor(2.0, requires_grad=True)
f = x**2

f.backward()
print(f"f(x) = x^2, f'(2) = {x.grad.item()}")  # 4.0

# 2. 합성함수 미분 (chain rule)
x = torch.tensor(3.0, requires_grad=True)
u = x**2       # u = x^2
f = torch.sin(u)  # f = sin(u)

f.backward()
# df/dx = df/du * du/dx = cos(u) * 2x = cos(9) * 6
print(f"f(x) = sin(x^2), f'(3) = {x.grad.item()}")
print(f"이론값: {(torch.cos(torch.tensor(9.0)) * 6).item()}")

# 3. 극값 찾기
def f(x):
    return x**4 - 4*x**3 + 4

x_vals = torch.linspace(-1, 4, 1000, requires_grad=True)
y_vals = f(x_vals)

# 수치 미분으로 임계점 근사
dx = x_vals[1] - x_vals[0]
dy = torch.diff(y_vals) / dx
critical_approx = x_vals[:-1][torch.abs(dy) < 0.1]
print(f"임계점 근사: {critical_approx}")

# 4. 평균값 정리 시각화
a, b = 0.0, 2.0
x_a = torch.tensor(a, requires_grad=True)
x_b = torch.tensor(b, requires_grad=True)

f_a = torch.sin(x_a)
f_b = torch.sin(x_b)

avg_slope = (f_b - f_a) / (b - a)
print(f"평균 기울기: {avg_slope.item()}")

# 어떤 c에서 f'(c) = avg_slope 찾기
c = torch.tensor(1.0, requires_grad=True)
f_c = torch.sin(c)
f_c.backward()
print(f"f'(c) at c=1: {c.grad.item()}")
print(f"cos(1) = {torch.cos(torch.tensor(1.0)).item()}")

# 5. 로피탈 정리 검증
def lhopital_example(x):
    # lim (sin x) / x as x -> 0
    if x.abs() < 1e-10:
        return torch.tensor(1.0)  # 이론값
    return torch.sin(x) / x

x_small = torch.tensor(0.01, requires_grad=True)
result = lhopital_example(x_small)
print(f"sin(0.01)/0.01 = {result.item()}")
print(f"이론값 (로피탈): 1.0")
```

---

## 8. 연습 문제

1. **극한 증명**
   ε-δ 정의를 사용하여 $\lim_{x \to 3} (2x - 1) = 5$임을 증명하라.

2. **첫 원리 미분**
   정의를 사용하여 $f(x) = \frac{1}{x}$의 도함수가 $f'(x) = -\frac{1}{x^2}$임을 보여라.

3. **Chain Rule**
   $h(x) = e^{\sin(x^2)}$의 도함수를 구하라.

4. **극값 문제**
   $f(x) = x^3 - 6x^2 + 9x + 1$의 모든 극값을 찾고, 극댓값인지 극솟값인지 판별하라.

5. **평균값 정리 응용**
   $f(x) = x^3$에 대해 $[1, 3]$에서 평균값 정리를 만족하는 $c$를 구하라.

6. **로피탈 정리**
   다음 극한을 계산하라:
   - $\lim_{x \to 0} \frac{e^x - 1 - x}{x^2}$
   - $\lim_{x \to \infty} \frac{\ln x}{x}$

---

## 9. 참고 자료

- James Stewart, *Calculus*, 8th ed., Chapter 2-4
- Michael Spivak, *Calculus*, Chapter 5-11
- MIT OCW 18.01 Single Variable Calculus
- 3Blue1Brown, *Essence of Calculus*, Episode 1-6

---

## 10. 다음 학습

- [[Math/Calculus/다변수 미적분|다변수 미분과 편미분]]
- [[Math/Calculus/테일러 급수와 최적화|테일러 전개와 최적화]]
- [[Math/Calculus/자동 미분|자동 미분과 Backpropagation]]
- [[Math/Calculus/미분과 미분법|미적분 전체 개요]]
