## TL;DR
- 머신러닝 최적화는 미분 가능한 손실 함수를 최소화하기 위해 파라미터를 업데이트하는 과정이며, 최적화 알고리즘과 하이퍼파라미터 선택이 모델 성능을 크게 좌우한다.
- 확률적 gradient, 모멘텀, 적응형 learning rate, 학습률 스케줄, gradient clipping 등 실전에서 자주 쓰이는 기법을 이해하면 트러블슈팅이 쉬워진다.
- 일반화와 수렴 속도 사이의 균형을 맞추기 위해 배치 크기, 정규화, 옵티마이저 설정을 함께 고려해야 한다.

## 핵심 요소
- **배치 크기 (Batch Size)**
  - 작은 배치 → gradient 추정에 노이즈가 많아 일반화에 유리하지만 수렴이 느릴 수 있음.
  - 큰 배치 → 안정적인 gradient, 빠른 수렴 가능하지만 일반화 성능이 떨어질 수 있음. (Linear scaling rule)
- **Momentum 계열**
  - SGD+Momentum: `v_t = β v_{t-1} + (1-β)∇f`, `θ_{t+1} = θ_t - η v_t`
  - Nesterov Momentum: look-ahead 위치에서 gradient 계산 → overshoot 감소.
- **Adaptive Optimizers**
  - Adagrad: 학습률을 parameter-wise로 감소, 희소 데이터에 유리.
  - RMSProp: 지수 이동 평균 기반, 최근 gradient에 더 집중.
  - Adam: Momentum + RMSProp 결합, `m_t`, `v_t`로 1차/2차 모멘트를 추적.
  - AdamW: L2 regularization을 decoupled weight decay로 처리해 일반화 개선.
- **학습률 스케줄**
  - Step decay, Exponential decay, Cosine annealing, OneCycle.
  - Warmup: 초기에 작은 lr로 안정화 후 점차 증가.
- **Gradient Clipping & Scaling**
  - Clipping: gradient norm이 임계값 이상이면 비율 조정 → 폭주 방지.
  - AMP(Automatic Mixed Precision)와 grad scaling은 underflow 방지.
- **2차 정보 활용**
  - Newton, Natural Gradient, Fisher 정보 행렬. 대규모 모델에서는 근사(Adafactor, Shampoo 등)로 사용.

## 실습 예제
```python
import torch
from torch.optim import SGD, AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR

model = torch.nn.Linear(100, 10)
criterion = torch.nn.CrossEntropyLoss()

optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)
scheduler = CosineAnnealingLR(optimizer, T_max=50)

for epoch in range(50):
    for x, y in dataloader:
        optimizer.zero_grad()
        logits = model(x)
        loss = criterion(logits, y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
    scheduler.step()
```
- AdamW + cosine annealing + gradient clipping 조합은 Transformer류 모델에서 널리 사용되는 패턴이다.

## 실험 포인트
- 동일 모델에 대해 SGD, SGD+Momentum, AdamW를 적용해 학습 곡선과 최종 정확도 비교.
- Learning rate finder(주석 달린 lr range test)로 적절한 초기 학습률을 찾기.
- 배치 크기를 32/128/512로 바꾸어 loss 변동성과 generalization 측정.
- Gradient clipping 적용 시/미적용 시 gradient norm의 변화를 로깅.

## 일반화 관점에서의 최적화
- 큰 학습률과 노이즈는 flat minimum(평평한 지역)에 도달하도록 도와 일반화에 유리할 수 있다.
- Adaptive optimizer는 빠르게 수렴하지만 overfitting 위험이 있으므로 weight decay나 learning rate decay로 보완.
- Sharpness-Aware Minimization(SAM), stochastic weight averaging(SWA) 등 최근 기법은 손실 landscape의 평평함을 직접 제어한다.

## 추천 참고 자료
- Sebastian Ruder, *An overview of gradient descent optimization algorithms*
- Loshchilov & Hutter, *Decoupled Weight Decay Regularization (AdamW)*
- Smith, *Cyclical Learning Rates for Training Neural Networks*

## 다음 학습
- [[ML Foundations/정규화와 일반화]]
- [[Training/Optimizers and Schedulers]]
- [[Advanced/AMP and Quantization]]
