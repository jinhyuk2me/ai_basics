## TL;DR
- RNN, LSTM, GRU 등 순환 구조와 어텐션 기반 시퀀스 모델 비교 관점을 제시합니다.
- 시계열·자연어·강화학습 등 도메인별 시퀀스 처리 요구사항을 분류합니다.

## 모델 계보
- RNN 기본 구조와 시간에 따른 기울기 흐름
- LSTM/GRU 게이트 설계 의도와 장단점
- 어텐션 메커니즘과 Transformer 블록 핵심 구성

## 데이터 처리 전략
- 시퀀스 길이, 패딩, 마스킹 처리 체크리스트
- 트렁케이트 BPTT, Teacher Forcing, Scheduled Sampling 개요
- 임베딩 층 설계 및 사전학습 벡터 활용 계획

## 실습 로드맵
- 텍스트 분류용 RNN vs Transformer 비교 실험 플랜
- 시계열 예측용 윈도우 생성, 평가 지표 설정
- 학습 로그 모니터링(Perplexity, BLEU, MSE 등) 항목

## 관련 노트
- [[DL Foundations/3. 역전파와 최적화 전략]]
- [[DL Foundations/5. 합성곱 신경망 기초]]
- [[Programming/2. 자료구조와 알고리즘]]
