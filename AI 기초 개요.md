## TL;DR
- AI 기초 학습은 수학 이론 → 머신러닝/딥러닝 실전 순으로 진행합니다.
- 8단계 로드맵을 따라 필수 개념을 익히고, 각 단계마다 실습을 병행합니다.
- 막히는 부분이 있으면 이전 단계로 돌아가 복습하는 Layered Approach를 활용하세요.
- Python 기초는 외부 리소스([Real Python](https://realpython.com/))로 먼저 학습하세요.
- AI 전체 분류 체계를 먼저 파악하려면 [[0. AI 분류 체계]]를 참고하세요.

## 학습 로드맵

| 단계 | 주제 | 핵심 질문 | 시작 노트 | 필수 개념 |
| --- | --- | --- | --- | --- |
| 1 | 수학 기초 | 선형대수·미적분·확률이 왜 필요한가? | [[Math/Linear Algebra/1. 선형대수 핵심 개요]] | 벡터, 행렬, 고유값, 미분, 그래디언트 |
| 2 | 확률/통계·최적화 | 불확실성과 손실 최소화를 어떻게 다루나? | [[Math/Probability/1. 확률과 통계 입문]] | 확률분포, 기댓값, 경사하강법, 볼록 최적화 |
| 3 | ML 기초 개념 | 지도/비지도/평가 지표는 무엇인가? | [[MLDL Basic/1. 학습 패러다임]] | 훈련/검증/테스트, 손실 함수, 평가 지표 |
| 4 | 최적화/정규화 | 학습 절차를 어떻게 안정화하나? | [[MLDL Basic/3. 최적화]] | SGD, Adam, 배치 정규화, 드롭아웃 |
| 5 | 신경망 기초 | 심층 신경망은 어떻게 동작하나? | [[MLDL Basic/5. 신경망 기본 구조]] | MLP, 활성화 함수, 역전파 |
| 6 | 신경망 아키텍처 | CNN, RNN은 어떻게 동작하나? | [[MLDL Basic/8. 합성곱 신경망 기초]] | 합성곱, 풀링, 시퀀스 모델 |
| 7 | 고급 수학 | 행렬 분해와 합성곱 이론은? | [[Math/Linear Algebra/8. 선형 연산자]] | 선형 변환, 행렬 분해, 합성곱과 FFT |
| 8 | 분석/수치 심화 | 라플라스·푸리에·측도가 DL과 어떻게 연결되나? | [[Math/Analysis/1. 라플라스 변환과 미분방정식]] | Neural ODE, Bayesian DL, Spectral CNN |

---

## 단계별 상세 가이드

### 1단계: 수학 기초

**학습 목표**: 신경망의 수학적 기반 이해

**필수 노트**:
- [[Math/Linear Algebra/1. 선형대수 핵심 개요]]: 벡터, 행렬, 선형 변환
- [[Math/Linear Algebra/2. 고유값과 고유벡터]]: 고유값 분해, 대각화
- [[Math/Calculus/1. 미분과 미분법]]: 도함수, 편미분, 연쇄 법칙
- [[Math/Calculus/2. 그래디언트와 방향 도함수]]: 다변수 함수의 미분

**왜 필요한가?**
- 신경망의 forward/backward pass는 행렬 연산
- 역전파는 연쇄 법칙의 반복 적용
- 최적화는 그래디언트를 따라 파라미터 업데이트

**실습 포인트**:
- NumPy로 벡터/행렬 연산 구현
- 간단한 함수의 그래디언트 손으로 계산
- PyTorch autograd로 자동 미분 실험

**다음으로**: 수학 기초가 갖춰지면 2단계(확률/최적화)로 이동

---

### 2단계: 확률/통계·최적화

**학습 목표**: 불확실성 모델링과 손실 최소화 기법 이해

**필수 노트**:
- [[Math/Probability/1. 확률과 통계 입문]]: 확률분포, 기댓값, 분산
- [[Math/Probability/2. 주요 확률분포]]: 정규분포, 베르누이, 다항분포
- [[Math/Optimization/1. 최적화 기초]]: 볼록 함수, 최적성 조건
- [[Math/Optimization/2. 경사하강법과 변형]]: SGD, Momentum, Adam

**왜 필요한가?**
- 머신러닝 모델은 확률적 예측을 수행
- 손실 함수 최소화는 최적화 문제
- SGD 등 최적화 알고리즘은 학습의 핵심

**실습 포인트**:
- 다양한 확률분포를 NumPy로 샘플링
- 간단한 함수에 경사하강법 적용
- 배치 크기에 따른 수렴 속도 비교

**다음으로**: 이론 준비가 끝나면 3단계(ML 기초)로 이동

---

### 3단계: ML 기초 개념

**학습 목표**: 머신러닝의 핵심 개념과 평가 방법 이해

**필수 노트**:
- [[MLDL Basic/1. 학습 패러다임]]: 지도/비지도/강화 학습
- [[MLDL Basic/2. 손실 함수와 평가 지표]]: MSE, Cross Entropy, Accuracy, F1

**왜 필요한가?**
- 문제 유형에 맞는 학습 방법 선택
- 모델 성능을 정량적으로 평가
- 과적합/과소적합 진단

**실습 포인트**:
- Scikit-learn으로 간단한 분류기 학습
- 훈련/검증/테스트 세트 분리
- 다양한 평가 지표 계산 및 비교

**다음으로**: ML 개념이 잡히면 4단계(최적화/정규화)로 이동

---

### 4단계: 최적화/정규화

**학습 목표**: 안정적이고 일반화된 모델 학습 방법

**필수 노트**:
- [[MLDL Basic/3. 최적화]]: SGD, Momentum, Adam, 배치 크기
- [[MLDL Basic/4. 정규화와 일반화]]: L2 정규화, 드롭아웃, 조기 종료

**왜 필요한가?**
- 학습 속도와 안정성 향상
- 과적합 방지로 일반화 성능 개선
- 실무에서 가장 중요한 기법들

**실습 포인트**:
- PyTorch로 다양한 옵티마이저 비교
- 드롭아웃/배치 정규화 적용 전후 비교
- Learning rate 스케줄링 실험

**다음으로**: 최적화 기법을 익히면 5단계(신경망 기초)로 이동

---

### 5단계: 신경망 기초

**학습 목표**: 다층 퍼셉트론과 역전파 이해

**필수 노트**:
- [[MLDL Basic/5. 신경망 기본 구조]]: MLP, Forward/Backward Pass
- [[MLDL Basic/6. 활성화 함수와 출력층 설계]]: ReLU, Sigmoid, Softmax
- [[MLDL Basic/7. 역전파와 최적화 전략]]: 연쇄 법칙, 자동 미분
- [[PyTorch Tensor]]: Tensor 기본 연산
- [[TensorFlow/TensorFlow Overview]]: TensorFlow 학습 로드맵

**왜 필요한가?**
- 딥러닝의 기본 구성 요소
- 역전파는 모든 신경망의 학습 원리
- PyTorch API 이해의 출발점

**실습 포인트**:
- MLP를 PyTorch로 처음부터 구현
- 역전파를 손으로 계산 후 autograd와 비교
- MNIST 분류 문제 해결

**다음으로**: 기본 신경망을 이해하면 6단계(CNN/RNN)로 이동

---

### 6단계: 신경망 아키텍처

**학습 목표**: CNN과 RNN의 동작 원리 및 응용

**필수 노트**:
- [[MLDL Basic/8. 합성곱 신경망 기초]]: Conv, Pooling, 출력 크기 계산
- [[MLDL Basic/9. 시퀀스 모델 기초]]: RNN, LSTM, Attention

**왜 필요한가?**
- 이미지 처리는 CNN, 시퀀스 처리는 RNN
- 현대 딥러닝 모델의 핵심 구성 요소
- 전이 학습의 기반 (ResNet, BERT 등)

**실습 포인트**:
- CIFAR-10 이미지 분류 (CNN)
- 특징 맵 시각화로 CNN이 학습한 패턴 확인
- 간단한 시계열 데이터 예측 (RNN/LSTM)

**다음으로**: 주요 아키텍처를 익히면 7단계(고급 수학)로 이동

---

### 7단계: 고급 수학 (선택)

**학습 목표**: 딥러닝 연산의 수학적 배경 이해

**필수 노트**:
- [[Math/Linear Algebra/8. 선형 연산자]]: 선형 변환, 내적, 노름
- [[Math/Linear Algebra/9. 행렬 분해]]: LU, QR, Cholesky, SVD
- [[Math/Analysis/4. 합성곱과 푸리에 변환]]: 합성곱 이론, FFT

**왜 필요한가?**
- PyTorch 연산의 수학적 배경 이해
- 효율적인 연산 선택 (메모리/속도)
- 고급 아키텍처 설계 및 논문 이해

**실습 포인트**:
- SVD로 이미지 압축 구현
- FFT 기반 합성곱과 직접 합성곱 속도 비교
- Cholesky 분해로 가우시안 샘플링

**다음으로**: 기초 과정 완료. 8단계(심화) 또는 프로젝트로 이동

---

### 8단계: 분석/수치 심화 (선택)

**학습 목표**: 최신 딥러닝 연구와 연결되는 고급 수학

**필수 노트**:
- [[Math/Analysis/1. 라플라스 변환과 미분방정식]]: Neural ODE
- [[Math/Analysis/2. 푸리에 변환]]: Spectral CNN, Frequency Domain
- [[Math/Probability/5. 베이지안 추론]]: Bayesian Deep Learning
- [[Math/Numerical/1. 수치 선형대수]]: 반복법, 조건수, DEQ

**왜 필요한가?**
- 최신 연구 논문 이해 (Neural ODE, PINN)
- Bayesian Deep Learning으로 불확실성 정량화
- 대규모 모델의 수치 안정성 이해

**실습 포인트**:
- Neural ODE로 시계열 데이터 모델링
- Laplace Approximation으로 불확실성 추정
- FFT를 활용한 고속 합성곱

**다음으로**: 연구 논문 구현, 대회 참가, 실무 프로젝트

---

## 학습 전략

### Layered Approach (계층적 학습)

**원칙**: 모르는 개념이 나오면 즉시 관련 노트로 이동

**예시**:
- CNN 학습 중 "합성곱"이 이해되지 않으면 → [[Operators/3. 합성곱 연산자]]
- 역전파 중 "연쇄 법칙"이 막히면 → [[Math/Calculus/1. 미분과 미분법]]
- Adam 옵티마이저가 궁금하면 → [[Foundations/3. 최적화]]

**도구**:
- Obsidian 그래프 뷰로 개념 간 연결 확인
- 각 노트 말미의 "다음 학습 경로" 섹션 활용

---

### 실습 병행 (Hands-On Practice)

**원칙**: 모든 개념은 코드로 직접 구현

**단계별 실습**:
1. **수학 노트**: NumPy로 개념 구현 (예: 행렬 곱셈, 미분 계산)
2. **ML 노트**: Scikit-learn으로 간단한 모델 학습
3. **DL 노트**: PyTorch로 신경망 처음부터 구현
4. **프로젝트**: Kaggle 데이터셋으로 종합 실습

**추천 환경**:
- Jupyter Notebook 또는 Google Colab
- PyTorch, NumPy, Matplotlib 설치

---

### 용어 정리 (Glossary)

**원칙**: 헷갈리는 용어는 즉시 정리

**활용 방법**:
- [[Math/Reference/개념 용어집]]: 수학 용어 빠른 조회
- 각 노트의 TL;DR: 핵심 개념 3-5줄 요약
- 노트 말미의 참고 자료: 외부 링크로 심화 학습

---

### 반복 학습 (Spaced Repetition)

**원칙**: 한 번에 완벽히 이해하려 하지 말고 여러 번 반복

**방법**:
1. **1차**: TL;DR + 핵심 개념만 빠르게 읽기
2. **2차**: 예제 직접 풀어보기
3. **3차**: 실습 코드 실행 및 변형
4. **4차**: 연습 문제 도전

---

## 학습 체크리스트

각 단계 완료 후 다음을 확인하세요:

### 1단계 완료 시
- [ ] 벡터와 행렬의 기본 연산을 NumPy로 구현할 수 있다
- [ ] 고유값과 고유벡터의 의미를 설명할 수 있다
- [ ] 편미분과 그래디언트를 손으로 계산할 수 있다

### 2단계 완료 시
- [ ] 주요 확률분포의 특징을 설명할 수 있다
- [ ] 경사하강법의 동작 원리를 이해했다
- [ ] Adam과 SGD의 차이를 설명할 수 있다

### 3-5단계 완료 시
- [ ] PyTorch로 MLP를 처음부터 구현할 수 있다
- [ ] 역전파의 수학적 원리를 설명할 수 있다
- [ ] 과적합 방지 기법 3가지 이상을 적용할 수 있다
- [ ] MNIST에서 98% 이상 정확도를 달성했다

### 6단계 완료 시
- [ ] CNN으로 CIFAR-10 분류 모델을 구현했다
- [ ] RNN/LSTM의 차이를 설명할 수 있다
- [ ] 전이 학습으로 작은 데이터셋에서 좋은 성능을 냈다

---

## 추천 학습 경로

### 최소 경로 (빠른 시작)
1단계 → 3단계 → 4단계 → 5단계 → 프로젝트

**특징**: 수학을 최소화하고 실습 위주로 빠르게 진행
**적합한 사람**: Python 경험이 있고 빠르게 모델을 만들어보고 싶은 사람

### 정석 경로 (탄탄한 기초)
1단계 → 2단계 → 3단계 → 4단계 → 5단계 → 6단계

**특징**: 수학 기초부터 주요 아키텍처까지 완주
**적합한 사람**: 체계적으로 기초부터 쌓고 싶은 사람

### 연구자 경로 (깊이 있는 이해)
1단계 → 2단계 → 3단계 → 4단계 → 5단계 → 6단계 → 7단계 → 8단계

**특징**: 최신 연구와 연결되는 고급 수학까지 학습
**적합한 사람**: 논문을 읽고 구현하거나 연구를 목표로 하는 사람

---

## 다음으로

### AI 전체 지형도 파악
- [[0. AI 분류 체계]]: AI의 전체 분류 체계와 실무 활용법

### 학습 시작
- 1단계부터 시작: [[Math/Linear Algebra/1. 선형대수 핵심 개요]]
- 빠른 시작: [[MLDL Basic/1. 학습 패러다임]]

### 참고 자료
- 노트 작성 가이드: [[CLAUDE]]
- 용어 사전: [[Math/Reference/개념 용어집]]
