## TL;DR
- tf.distribute 전략(Mirrored, MultiWorker, TPUStrategy 등)을 한눈에 비교할 예정입니다.

## 언제 쓰나
- 멀티 GPU/노드/TPU 환경에서 학습을 확장하려고 할 때.

## 주요 포인트
- 전략 선택 기준
- 전략별 코드 스켈레톤
- 데이터/optimizer 동작 주의

## 실습 예제
```python
# TODO
```

## 실수 주의
- TODO

## 관련 노트
- TODO
